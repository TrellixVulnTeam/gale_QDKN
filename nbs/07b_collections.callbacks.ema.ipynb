{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp collections.callbacks.ema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# hide\\n%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\\n%matplotlib inline\";\n",
       "                var nbb_formatted_code = \"# hide\\n%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\\n%matplotlib inline\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "%load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"#hide\\nimport warnings\\n\\nfrom nbdev.export import *\\nfrom nbdev.export import Config\\nfrom nbdev.showdoc import *\\nfrom timm.utils import *\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\nsetup_default_logging()\";\n",
       "                var nbb_formatted_code = \"# hide\\nimport warnings\\n\\nfrom nbdev.export import *\\nfrom nbdev.export import Config\\nfrom nbdev.showdoc import *\\nfrom timm.utils import *\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\nsetup_default_logging()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "import warnings\n",
    "\n",
    "from nbdev.export import *\n",
    "from nbdev.export import Config\n",
    "from nbdev.showdoc import *\n",
    "from timm.utils import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "setup_default_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Exponential Moving Average Callback for PyTorch Lightning\n",
    "> This is intended to allow functionality like https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# export\\nimport logging\\n\\nimport pytorch_lightning as pl\\nimport torch\\nfrom pytorch_lightning.callbacks import Callback\\nfrom pytorch_lightning.utilities import rank_zero_only\\nfrom timm.utils.model import get_state_dict, unwrap_model\\nfrom timm.utils.model_ema import ModelEmaV2\\n\\nfrom gale.utils.logger import log_main_process\\n\\n_logger = logging.getLogger(__name__)\";\n",
       "                var nbb_formatted_code = \"# export\\nimport logging\\n\\nimport pytorch_lightning as pl\\nimport torch\\nfrom pytorch_lightning.callbacks import Callback\\nfrom pytorch_lightning.utilities import rank_zero_only\\nfrom timm.utils.model import get_state_dict, unwrap_model\\nfrom timm.utils.model_ema import ModelEmaV2\\n\\nfrom gale.utils.logger import log_main_process\\n\\n_logger = logging.getLogger(__name__)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "import logging\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "from timm.utils.model import get_state_dict, unwrap_model\n",
    "from timm.utils.model_ema import ModelEmaV2\n",
    "\n",
    "from gale.utils.logger import log_main_process\n",
    "\n",
    "_logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# export\\nclass EMACallback(Callback):\\n    \\\"\\\"\\\"\\n    Model Exponential Moving Average. Empirically it has been found that using the moving average \\n    of the trained parameters of a deep network is better than using its trained parameters directly.\\n    \\n    If `use_ema_weights`, then the ema parameters of the network is set after training end.\\n    \\\"\\\"\\\"\\n    def __init__(self, decay=0.9999, use_ema_weights: bool = True):\\n        self.decay = decay\\n        self.ema = None\\n        self.use_ema_weights = use_ema_weights\\n\\n    def on_fit_start(self, trainer, pl_module):\\n        \\\"Initialize `ModelEmaV2` from timm to keep a copy of the moving average of the weights\\\"\\n        self.ema = ModelEmaV2(pl_module, decay=self.decay, device=None)\\n\\n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\\n        \\\"Update the stored parameters using a moving average\\\"\\n        # Update currently maintained parameters.\\n        self.ema.update(pl_module)\\n\\n    def on_validation_epoch_start(self, trainer, pl_module):\\n        \\\"do validation using the stored parameters\\\"\\n        # save original parameters before replacing with EMA version\\n        self.store(pl_module.parameters())\\n\\n        # update the LightningModule with the EMA weights\\n        # ~ Copy EMA parameters to LightningModule\\n        self.copy_to(self.ema.module.parameters(), pl_module.parameters())\\n\\n    def on_validation_end(self, trainer, pl_module):\\n        \\\"Restore original parameters to resume training later\\\"\\n        self.restore(pl_module.parameters())\\n\\n    def on_train_end(self, trainer, pl_module):\\n        # update the LightningModule with the EMA weights\\n        if self.use_ema_weights:\\n            self.copy_to(self.ema.module.parameters(), pl_module.parameters())\\n            msg = \\\"Model weights replaced with the EMA version.\\\"\\n            log_main_process(_logger, logging.INFO, msg)\\n            \\n    def on_save_checkpoint(self, trainer, pl_module, checkpoint):\\n        if self.ema is not None:\\n            return {\\\"state_dict_ema\\\": get_state_dict(self.ema, unwrap_model)}\\n    \\n    def on_load_checkpoint(self, callback_state):\\n        if self.ema is not None:\\n            self.ema.module.load_state_dict(callback_state[\\\"state_dict_ema\\\"])\\n\\n    def store(self, parameters):\\n        \\\"Save the current parameters for restoring later.\\\"\\n        self.collected_params = [param.clone() for param in parameters]       \\n\\n    def restore(self, parameters):\\n        \\\"\\\"\\\"\\n        Restore the parameters stored with the `store` method.\\n        Useful to validate the model with EMA parameters without affecting the\\n        original optimization process.\\n        \\\"\\\"\\\"\\n        for c_param, param in zip(self.collected_params, parameters):\\n            param.data.copy_(c_param.data)\\n\\n    def copy_to(self, shadow_parameters, parameters):\\n        \\\"Copy current parameters into given collection of parameters.\\\"\\n        for s_param, param in zip(shadow_parameters, parameters):\\n            if param.requires_grad:\\n                param.data.copy_(s_param.data)\";\n",
       "                var nbb_formatted_code = \"# export\\nclass EMACallback(Callback):\\n    \\\"\\\"\\\"\\n    Model Exponential Moving Average. Empirically it has been found that using the moving average\\n    of the trained parameters of a deep network is better than using its trained parameters directly.\\n\\n    If `use_ema_weights`, then the ema parameters of the network is set after training end.\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, decay=0.9999, use_ema_weights: bool = True):\\n        self.decay = decay\\n        self.ema = None\\n        self.use_ema_weights = use_ema_weights\\n\\n    def on_fit_start(self, trainer, pl_module):\\n        \\\"Initialize `ModelEmaV2` from timm to keep a copy of the moving average of the weights\\\"\\n        self.ema = ModelEmaV2(pl_module, decay=self.decay, device=None)\\n\\n    def on_train_batch_end(\\n        self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx\\n    ):\\n        \\\"Update the stored parameters using a moving average\\\"\\n        # Update currently maintained parameters.\\n        self.ema.update(pl_module)\\n\\n    def on_validation_epoch_start(self, trainer, pl_module):\\n        \\\"do validation using the stored parameters\\\"\\n        # save original parameters before replacing with EMA version\\n        self.store(pl_module.parameters())\\n\\n        # update the LightningModule with the EMA weights\\n        # ~ Copy EMA parameters to LightningModule\\n        self.copy_to(self.ema.module.parameters(), pl_module.parameters())\\n\\n    def on_validation_end(self, trainer, pl_module):\\n        \\\"Restore original parameters to resume training later\\\"\\n        self.restore(pl_module.parameters())\\n\\n    def on_train_end(self, trainer, pl_module):\\n        # update the LightningModule with the EMA weights\\n        if self.use_ema_weights:\\n            self.copy_to(self.ema.module.parameters(), pl_module.parameters())\\n            msg = \\\"Model weights replaced with the EMA version.\\\"\\n            log_main_process(_logger, logging.INFO, msg)\\n\\n    def on_save_checkpoint(self, trainer, pl_module, checkpoint):\\n        if self.ema is not None:\\n            return {\\\"state_dict_ema\\\": get_state_dict(self.ema, unwrap_model)}\\n\\n    def on_load_checkpoint(self, callback_state):\\n        if self.ema is not None:\\n            self.ema.module.load_state_dict(callback_state[\\\"state_dict_ema\\\"])\\n\\n    def store(self, parameters):\\n        \\\"Save the current parameters for restoring later.\\\"\\n        self.collected_params = [param.clone() for param in parameters]\\n\\n    def restore(self, parameters):\\n        \\\"\\\"\\\"\\n        Restore the parameters stored with the `store` method.\\n        Useful to validate the model with EMA parameters without affecting the\\n        original optimization process.\\n        \\\"\\\"\\\"\\n        for c_param, param in zip(self.collected_params, parameters):\\n            param.data.copy_(c_param.data)\\n\\n    def copy_to(self, shadow_parameters, parameters):\\n        \\\"Copy current parameters into given collection of parameters.\\\"\\n        for s_param, param in zip(shadow_parameters, parameters):\\n            if param.requires_grad:\\n                param.data.copy_(s_param.data)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class EMACallback(Callback):\n",
    "    \"\"\"\n",
    "    Model Exponential Moving Average. Empirically it has been found that using the moving average\n",
    "    of the trained parameters of a deep network is better than using its trained parameters directly.\n",
    "\n",
    "    If `use_ema_weights`, then the ema parameters of the network is set after training end.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, decay=0.9999, use_ema_weights: bool = True):\n",
    "        self.decay = decay\n",
    "        self.ema = None\n",
    "        self.use_ema_weights = use_ema_weights\n",
    "\n",
    "    def on_fit_start(self, trainer, pl_module):\n",
    "        \"Initialize `ModelEmaV2` from timm to keep a copy of the moving average of the weights\"\n",
    "        self.ema = ModelEmaV2(pl_module, decay=self.decay, device=None)\n",
    "\n",
    "    def on_train_batch_end(\n",
    "        self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx\n",
    "    ):\n",
    "        \"Update the stored parameters using a moving average\"\n",
    "        # Update currently maintained parameters.\n",
    "        self.ema.update(pl_module)\n",
    "\n",
    "    def on_validation_epoch_start(self, trainer, pl_module):\n",
    "        \"do validation using the stored parameters\"\n",
    "        # save original parameters before replacing with EMA version\n",
    "        self.store(pl_module.parameters())\n",
    "\n",
    "        # update the LightningModule with the EMA weights\n",
    "        # ~ Copy EMA parameters to LightningModule\n",
    "        self.copy_to(self.ema.module.parameters(), pl_module.parameters())\n",
    "\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        \"Restore original parameters to resume training later\"\n",
    "        self.restore(pl_module.parameters())\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        # update the LightningModule with the EMA weights\n",
    "        if self.use_ema_weights:\n",
    "            self.copy_to(self.ema.module.parameters(), pl_module.parameters())\n",
    "            msg = \"Model weights replaced with the EMA version.\"\n",
    "            log_main_process(_logger, logging.INFO, msg)\n",
    "\n",
    "    def on_save_checkpoint(self, trainer, pl_module, checkpoint):\n",
    "        if self.ema is not None:\n",
    "            return {\"state_dict_ema\": get_state_dict(self.ema, unwrap_model)}\n",
    "\n",
    "    def on_load_checkpoint(self, callback_state):\n",
    "        if self.ema is not None:\n",
    "            self.ema.module.load_state_dict(callback_state[\"state_dict_ema\"])\n",
    "\n",
    "    def store(self, parameters):\n",
    "        \"Save the current parameters for restoring later.\"\n",
    "        self.collected_params = [param.clone() for param in parameters]\n",
    "\n",
    "    def restore(self, parameters):\n",
    "        \"\"\"\n",
    "        Restore the parameters stored with the `store` method.\n",
    "        Useful to validate the model with EMA parameters without affecting the\n",
    "        original optimization process.\n",
    "        \"\"\"\n",
    "        for c_param, param in zip(self.collected_params, parameters):\n",
    "            param.data.copy_(c_param.data)\n",
    "\n",
    "    def copy_to(self, shadow_parameters, parameters):\n",
    "        \"Copy current parameters into given collection of parameters.\"\n",
    "        for s_param, param in zip(shadow_parameters, parameters):\n",
    "            if param.requires_grad:\n",
    "                param.data.copy_(s_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"EMACallback\" class=\"doc_header\"><code>class</code> <code>EMACallback</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>EMACallback</code>(**`decay`**=*`0.9999`*, **`use_ema_weights`**:`bool`=*`True`*) :: `Callback`\n",
       "\n",
       "Model Exponential Moving Average. Empirically it has been found that using the moving average \n",
       "of the trained parameters of a deep network is better than using its trained parameters directly.\n",
       "\n",
       "If `use_ema_weights`, then the ema parameters of the network is set after training end."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"show_doc(EMACallback)\";\n",
       "                var nbb_formatted_code = \"show_doc(EMACallback)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(EMACallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.logger.ipynb.\n",
      "Converted 00a_utils.display.ipynb.\n",
      "Converted 00b_utils.structures.ipynb.\n",
      "Converted 01_torch_utils.ipynb.\n",
      "Converted 01a_losses.ipynb.\n",
      "Converted 02_optimizer.ipynb.\n",
      "Converted 02a_schedules.ipynb.\n",
      "Converted 03_core-classes.ipynb.\n",
      "Converted 04_classification.models.backbones.ipynb.\n",
      "Converted 04a_classification.models.heads.ipynb.\n",
      "Converted 04b_classification.model.meta_arch.common.ipynb.\n",
      "Converted 04b_classification.model.meta_arch.vit.ipynb.\n",
      "Converted 05_classification.core.ipynb.\n",
      "Converted 05a_classification.augment.ipynb.\n",
      "Converted 05b_classification.data.ipynb.\n",
      "Converted 06_classification.task.ipynb.\n",
      "Converted 07_collections.pandas.ipynb.\n",
      "Converted 07a_collections.callbacks.notebook.ipynb.\n",
      "Converted 07b_collections.callbacks.ema.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"# hide\\nnotebook2script()\";\n",
       "                var nbb_formatted_code = \"# hide\\nnotebook2script()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gale_dev",
   "language": "python",
   "name": "gale_dev"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
