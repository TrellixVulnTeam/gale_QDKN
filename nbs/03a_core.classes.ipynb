{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# hide\\n%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\";\n",
       "                var nbb_formatted_code = \"# hide\\n%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "%load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# hide\\nimport warnings\\n\\nfrom nbdev.export import *\\nfrom nbdev.showdoc import *\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\";\n",
       "                var nbb_formatted_code = \"# hide\\nimport warnings\\n\\nfrom nbdev.export import *\\nfrom nbdev.showdoc import *\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "import warnings\n",
    "\n",
    "from nbdev.export import *\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes\n",
    "> Interfaces common to all `Modules` and `Models` in Gale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# export\\nimport copy\\nimport logging\\nimport math\\nfrom abc import ABC, ABCMeta, abstractmethod\\nfrom contextlib import contextmanager\\nfrom typing import *\\n\\nimport hydra\\nimport pytorch_lightning as pl\\nimport torch\\nimport torchmetrics\\nfrom fastcore.all import L, noop, patch\\nfrom omegaconf import DictConfig, OmegaConf\\nfrom torch.nn import Module\\n\\nfrom gale.core.nn.optim import OPTIM_REGISTRY, SCHEDULER_REGISTRY\\nfrom gale.core.nn.utils import params, trainable_params\\nfrom gale.core.utils.logger import log_main_process\\n\\n_logger = logging.getLogger(__name__)\";\n",
       "                var nbb_formatted_code = \"# export\\nimport copy\\nimport logging\\nimport math\\nfrom abc import ABC, ABCMeta, abstractmethod\\nfrom contextlib import contextmanager\\nfrom typing import *\\n\\nimport hydra\\nimport pytorch_lightning as pl\\nimport torch\\nimport torchmetrics\\nfrom fastcore.all import L, noop, patch\\nfrom omegaconf import DictConfig, OmegaConf\\nfrom torch.nn import Module\\n\\nfrom gale.core.nn.optim import OPTIM_REGISTRY, SCHEDULER_REGISTRY\\nfrom gale.core.nn.utils import params, trainable_params\\nfrom gale.core.utils.logger import log_main_process\\n\\n_logger = logging.getLogger(__name__)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "import copy\n",
    "import logging\n",
    "import math\n",
    "from abc import ABC, ABCMeta, abstractmethod\n",
    "from contextlib import contextmanager\n",
    "from typing import *\n",
    "\n",
    "import hydra\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchmetrics\n",
    "from fastcore.all import L, noop, patch\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from torch.nn import Module\n",
    "\n",
    "from gale.core.nn.optim import OPTIM_REGISTRY, SCHEDULER_REGISTRY\n",
    "from gale.core.nn.utils import params, trainable_params\n",
    "from gale.core.utils.logger import log_main_process\n",
    "\n",
    "_logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"from gale.core.utils.logger import setup_logger\\n\\n# setup logging\\nsetup_logger()\\n_logger = logging.getLogger(\\\"gale.core.classes\\\")\";\n",
       "                var nbb_formatted_code = \"from gale.core.utils.logger import setup_logger\\n\\n# setup logging\\nsetup_logger()\\n_logger = logging.getLogger(\\\"gale.core.classes\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gale.core.utils.logger import setup_logger\n",
    "\n",
    "# setup logging\n",
    "setup_logger()\n",
    "_logger = logging.getLogger(\"gale.core.classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# export\\nclass Configurable(ABC):\\n    \\\"\\\"\\\"\\n    Helper Class to instantiate obj from config\\n    \\\"\\\"\\\"\\n\\n    @classmethod\\n    def from_config_dict(cls, config: DictConfig, **kwargs):\\n        \\\"\\\"\\\"\\n        Instantiates object using `DictConfig-based` configuration. You can optionally\\n        pass in extra `kwargs`\\n        \\\"\\\"\\\"\\n        # Resolve the config dict\\n        if isinstance(config, DictConfig):\\n            config = OmegaConf.to_container(config, resolve=True)\\n            config = OmegaConf.create(config)\\n\\n        if \\\"_target_\\\" in config:\\n            # regular hydra-based instantiation\\n            instance = hydra.utils.instantiate(config=config, **kwargs)\\n        else:\\n            # instantiate directly using kwargs\\n            try:\\n                instance = cls(cfg=config, **kwargs)\\n            except:\\n                cfg = OmegaConf.to_container(config, resolve=True)\\n                instance = cls(**config, **kwargs)\\n\\n        if not hasattr(instance, \\\"_cfg\\\"):\\n            instance._cfg = config\\n        return instance\\n\\n    def to_config_dict(self) -> DictConfig:\\n        # fmt: off\\n        \\\"\\\"\\\"Returns object's configuration to config dictionary\\\"\\\"\\\"\\n        if hasattr(self, \\\"_cfg\\\") and self._cfg is not None and isinstance(self._cfg, DictConfig):\\n            # Resolve the config dict\\n            config = OmegaConf.to_container(self._cfg, resolve=True)\\n            config = OmegaConf.create(config)\\n            OmegaConf.set_struct(config, True)\\n            self._cfg = config\\n\\n            return self._cfg\\n        else:\\n            raise NotImplementedError(\\\"to_config_dict() can currently only return object._cfg but current object does not have it.\\\")\\n        # fmt: on\";\n",
       "                var nbb_formatted_code = \"# export\\nclass Configurable(ABC):\\n    \\\"\\\"\\\"\\n    Helper Class to instantiate obj from config\\n    \\\"\\\"\\\"\\n\\n    @classmethod\\n    def from_config_dict(cls, config: DictConfig, **kwargs):\\n        \\\"\\\"\\\"\\n        Instantiates object using `DictConfig-based` configuration. You can optionally\\n        pass in extra `kwargs`\\n        \\\"\\\"\\\"\\n        # Resolve the config dict\\n        if isinstance(config, DictConfig):\\n            config = OmegaConf.to_container(config, resolve=True)\\n            config = OmegaConf.create(config)\\n\\n        if \\\"_target_\\\" in config:\\n            # regular hydra-based instantiation\\n            instance = hydra.utils.instantiate(config=config, **kwargs)\\n        else:\\n            # instantiate directly using kwargs\\n            try:\\n                instance = cls(cfg=config, **kwargs)\\n            except:\\n                cfg = OmegaConf.to_container(config, resolve=True)\\n                instance = cls(**config, **kwargs)\\n\\n        if not hasattr(instance, \\\"_cfg\\\"):\\n            instance._cfg = config\\n        return instance\\n\\n    def to_config_dict(self) -> DictConfig:\\n        # fmt: off\\n        \\\"\\\"\\\"Returns object's configuration to config dictionary\\\"\\\"\\\"\\n        if hasattr(self, \\\"_cfg\\\") and self._cfg is not None and isinstance(self._cfg, DictConfig):\\n            # Resolve the config dict\\n            config = OmegaConf.to_container(self._cfg, resolve=True)\\n            config = OmegaConf.create(config)\\n            OmegaConf.set_struct(config, True)\\n            self._cfg = config\\n\\n            return self._cfg\\n        else:\\n            raise NotImplementedError(\\\"to_config_dict() can currently only return object._cfg but current object does not have it.\\\")\\n        # fmt: on\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class Configurable(ABC):\n",
    "    \"\"\"\n",
    "    Helper Class to instantiate obj from config\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_config_dict(cls, config: DictConfig, **kwargs):\n",
    "        \"\"\"\n",
    "        Instantiates object using `DictConfig-based` configuration. You can optionally\n",
    "        pass in extra `kwargs`\n",
    "        \"\"\"\n",
    "        # Resolve the config dict\n",
    "        if isinstance(config, DictConfig):\n",
    "            config = OmegaConf.to_container(config, resolve=True)\n",
    "            config = OmegaConf.create(config)\n",
    "\n",
    "        if \"_target_\" in config:\n",
    "            # regular hydra-based instantiation\n",
    "            instance = hydra.utils.instantiate(config=config, **kwargs)\n",
    "        else:\n",
    "            # instantiate directly using kwargs\n",
    "            try:\n",
    "                instance = cls(cfg=config, **kwargs)\n",
    "            except:\n",
    "                cfg = OmegaConf.to_container(config, resolve=True)\n",
    "                instance = cls(**config, **kwargs)\n",
    "\n",
    "        if not hasattr(instance, \"_cfg\"):\n",
    "            instance._cfg = config\n",
    "        return instance\n",
    "\n",
    "    def to_config_dict(self) -> DictConfig:\n",
    "        # fmt: off\n",
    "        \"\"\"Returns object's configuration to config dictionary\"\"\"\n",
    "        if hasattr(self, \"_cfg\") and self._cfg is not None and isinstance(self._cfg, DictConfig):\n",
    "            # Resolve the config dict\n",
    "            config = OmegaConf.to_container(self._cfg, resolve=True)\n",
    "            config = OmegaConf.create(config)\n",
    "            OmegaConf.set_struct(config, True)\n",
    "            self._cfg = config\n",
    "\n",
    "            return self._cfg\n",
    "        else:\n",
    "            raise NotImplementedError(\"to_config_dict() can currently only return object._cfg but current object does not have it.\")\n",
    "        # fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Configurable.from_config_dict\" class=\"doc_header\"><code>Configurable.from_config_dict</code><a href=\"__main__.py#L7\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Configurable.from_config_dict</code>(**`config`**:`DictConfig`, **\\*\\*`kwargs`**)\n",
       "\n",
       "Instantiates object using `DictConfig-based` configuration. You can optionally\n",
       "pass in extra `kwargs`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Configurable.to_config_dict\" class=\"doc_header\"><code>Configurable.to_config_dict</code><a href=\"__main__.py#L33\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Configurable.to_config_dict</code>()\n",
       "\n",
       "Returns object's configuration to config dictionary"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"show_doc(Configurable.from_config_dict)\\nshow_doc(Configurable.to_config_dict)\";\n",
       "                var nbb_formatted_code = \"show_doc(Configurable.from_config_dict)\\nshow_doc(Configurable.to_config_dict)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Configurable.from_config_dict)\n",
    "show_doc(Configurable.to_config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"# export\\nclass GaleModule(Module, Configurable, metaclass=ABCMeta):\\n    \\\"\\\"\\\"\\n    Abstract class offering interface which should be implemented by all `Backbones`,\\n    `Heads` and `Meta Archs` in gale.\\n    \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def forward(self) -> Any:\\n        \\\"\\\"\\\"\\n        The main logic for the model lives here. Can return either features, logits\\n        or loss.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def build_param_dicts(self) -> Union[Iterable, List[Dict], Dict, List]:\\n        \\\"\\\"\\\"\\n        Should return the iterable of parameters to optimize or dicts defining parameter groups\\n        for the Module.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @property\\n    def param_lists(self):\\n        \\\"Returns the list of paramters in the module\\\"\\n        return [p for p in self.parameters()]\\n\\n    def all_params(self, n=slice(None), with_grad=False):\\n        \\\"List of `param_groups` upto n\\\"\\n        res = L(p for p in self.param_lists[n])\\n        # fmt: off\\n        return L(o for o in res if hasattr(o, \\\"grad\\\") and o.grad is not None) if with_grad else res\\n        # fmt: on\\n\\n    def _set_require_grad(self, rg, p):\\n        p.requires_grad_(rg)\\n\\n    def unfreeze(self) -> None:\\n        \\\"\\\"\\\"\\n        Unfreeze all parameters for training.\\n        \\\"\\\"\\\"\\n        for param in self.parameters():\\n            param.requires_grad = True\\n\\n        self.train()\\n\\n    def freeze(self) -> None:\\n        \\\"\\\"\\\"\\n        Freeze all params for inference & set model to eval\\n        \\\"\\\"\\\"\\n        for param in self.parameters():\\n            param.requires_grad = False\\n        self.eval()\\n\\n    def freeze_to(self, n) -> None:\\n        \\\"Freeze parameter groups up to `n`\\\"\\n        self.frozen_idx = n if n >= 0 else len(self.param_lists) + n\\n        if self.frozen_idx >= len(self.param_lists):\\n            # fmt: off\\n            _logger.warning(f\\\"Freezing {self.frozen_idx} groups; model has {len(self.param_lists)}; whole model is frozen.\\\")\\n            # fmt: on\\n\\n        for o in self.all_params(slice(n, None)):\\n            self._set_require_grad(True, o)\\n\\n        for o in self.all_params(slice(None, n)):\\n            self._set_require_grad(False, o)\\n\\n    @contextmanager\\n    def as_frozen(self):\\n        \\\"\\\"\\\"\\n        Context manager which temporarily freezes a module, yields control\\n        and finally unfreezes the module.\\n        \\\"\\\"\\\"\\n        self.freeze()\\n\\n        try:\\n            yield\\n        finally:\\n            self.unfreeze()\";\n",
       "                var nbb_formatted_code = \"# export\\nclass GaleModule(Module, Configurable, metaclass=ABCMeta):\\n    \\\"\\\"\\\"\\n    Abstract class offering interface which should be implemented by all `Backbones`,\\n    `Heads` and `Meta Archs` in gale.\\n    \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def forward(self) -> Any:\\n        \\\"\\\"\\\"\\n        The main logic for the model lives here. Can return either features, logits\\n        or loss.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def build_param_dicts(self) -> Union[Iterable, List[Dict], Dict, List]:\\n        \\\"\\\"\\\"\\n        Should return the iterable of parameters to optimize or dicts defining parameter groups\\n        for the Module.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @property\\n    def param_lists(self):\\n        \\\"Returns the list of paramters in the module\\\"\\n        return [p for p in self.parameters()]\\n\\n    def all_params(self, n=slice(None), with_grad=False):\\n        \\\"List of `param_groups` upto n\\\"\\n        res = L(p for p in self.param_lists[n])\\n        # fmt: off\\n        return L(o for o in res if hasattr(o, \\\"grad\\\") and o.grad is not None) if with_grad else res\\n        # fmt: on\\n\\n    def _set_require_grad(self, rg, p):\\n        p.requires_grad_(rg)\\n\\n    def unfreeze(self) -> None:\\n        \\\"\\\"\\\"\\n        Unfreeze all parameters for training.\\n        \\\"\\\"\\\"\\n        for param in self.parameters():\\n            param.requires_grad = True\\n\\n        self.train()\\n\\n    def freeze(self) -> None:\\n        \\\"\\\"\\\"\\n        Freeze all params for inference & set model to eval\\n        \\\"\\\"\\\"\\n        for param in self.parameters():\\n            param.requires_grad = False\\n        self.eval()\\n\\n    def freeze_to(self, n) -> None:\\n        \\\"Freeze parameter groups up to `n`\\\"\\n        self.frozen_idx = n if n >= 0 else len(self.param_lists) + n\\n        if self.frozen_idx >= len(self.param_lists):\\n            # fmt: off\\n            _logger.warning(f\\\"Freezing {self.frozen_idx} groups; model has {len(self.param_lists)}; whole model is frozen.\\\")\\n            # fmt: on\\n\\n        for o in self.all_params(slice(n, None)):\\n            self._set_require_grad(True, o)\\n\\n        for o in self.all_params(slice(None, n)):\\n            self._set_require_grad(False, o)\\n\\n    @contextmanager\\n    def as_frozen(self):\\n        \\\"\\\"\\\"\\n        Context manager which temporarily freezes a module, yields control\\n        and finally unfreezes the module.\\n        \\\"\\\"\\\"\\n        self.freeze()\\n\\n        try:\\n            yield\\n        finally:\\n            self.unfreeze()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class GaleModule(Module, Configurable, metaclass=ABCMeta):\n",
    "    \"\"\"\n",
    "    Abstract class offering interface which should be implemented by all `Backbones`,\n",
    "    `Heads` and `Meta Archs` in gale.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self) -> Any:\n",
    "        \"\"\"\n",
    "        The main logic for the model lives here. Can return either features, logits\n",
    "        or loss.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def build_param_dicts(self) -> Union[Iterable, List[Dict], Dict, List]:\n",
    "        \"\"\"\n",
    "        Should return the iterable of parameters to optimize or dicts defining parameter groups\n",
    "        for the Module.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def param_lists(self):\n",
    "        \"Returns the list of paramters in the module\"\n",
    "        return [p for p in self.parameters()]\n",
    "\n",
    "    def all_params(self, n=slice(None), with_grad=False):\n",
    "        \"List of `param_groups` upto n\"\n",
    "        res = L(p for p in self.param_lists[n])\n",
    "        # fmt: off\n",
    "        return L(o for o in res if hasattr(o, \"grad\") and o.grad is not None) if with_grad else res\n",
    "        # fmt: on\n",
    "\n",
    "    def _set_require_grad(self, rg, p):\n",
    "        p.requires_grad_(rg)\n",
    "\n",
    "    def unfreeze(self) -> None:\n",
    "        \"\"\"\n",
    "        Unfreeze all parameters for training.\n",
    "        \"\"\"\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.train()\n",
    "\n",
    "    def freeze(self) -> None:\n",
    "        \"\"\"\n",
    "        Freeze all params for inference & set model to eval\n",
    "        \"\"\"\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.eval()\n",
    "\n",
    "    def freeze_to(self, n) -> None:\n",
    "        \"Freeze parameter groups up to `n`\"\n",
    "        self.frozen_idx = n if n >= 0 else len(self.param_lists) + n\n",
    "        if self.frozen_idx >= len(self.param_lists):\n",
    "            # fmt: off\n",
    "            _logger.warning(f\"Freezing {self.frozen_idx} groups; model has {len(self.param_lists)}; whole model is frozen.\")\n",
    "            # fmt: on\n",
    "\n",
    "        for o in self.all_params(slice(n, None)):\n",
    "            self._set_require_grad(True, o)\n",
    "\n",
    "        for o in self.all_params(slice(None, n)):\n",
    "            self._set_require_grad(False, o)\n",
    "\n",
    "    @contextmanager\n",
    "    def as_frozen(self):\n",
    "        \"\"\"\n",
    "        Context manager which temporarily freezes a module, yields control\n",
    "        and finally unfreezes the module.\n",
    "        \"\"\"\n",
    "        self.freeze()\n",
    "\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            self.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"GaleModule\" class=\"doc_header\"><code>class</code> <code>GaleModule</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>GaleModule</code>() :: `Module`\n",
       "\n",
       "Abstract class offering interface which should be implemented by all `Backbones`,\n",
       "`Heads` and `Meta Archs` in gale."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleModule)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleModule)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleModule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Required Methods -*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.forward\" class=\"doc_header\"><code>GaleModule.forward</code><a href=\"__main__.py#L8\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleModule.forward</code>()\n",
       "\n",
       "The main logic for the model lives here. Can return either features, logits\n",
       "or loss."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.build_param_dicts\" class=\"doc_header\"><code>GaleModule.build_param_dicts</code><a href=\"__main__.py#L16\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleModule.build_param_dicts</code>()\n",
       "\n",
       "Should return the iterable of parameters to optimize or dicts defining parameter groups\n",
       "for the Module."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleModule.forward)\\nshow_doc(GaleModule.build_param_dicts)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleModule.forward)\\nshow_doc(GaleModule.build_param_dicts)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleModule.forward)\n",
    "show_doc(GaleModule.build_param_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Extra functionality -*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Configurable.from_config_dict\" class=\"doc_header\"><code>Configurable.from_config_dict</code><a href=\"__main__.py#L7\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Configurable.from_config_dict</code>(**`config`**:`DictConfig`, **\\*\\*`kwargs`**)\n",
       "\n",
       "Instantiates object using `DictConfig-based` configuration. You can optionally\n",
       "pass in extra `kwargs`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.param_lists\" class=\"doc_header\"><code>GaleModule.param_lists</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "Returns the list of paramters in the module"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.all_params\" class=\"doc_header\"><code>GaleModule.all_params</code><a href=\"__main__.py#L29\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleModule.all_params</code>(**`n`**=*`slice(None, None, None)`*, **`with_grad`**=*`False`*)\n",
       "\n",
       "List of `param_groups` upto n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.freeze\" class=\"doc_header\"><code>GaleModule.freeze</code><a href=\"__main__.py#L48\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleModule.freeze</code>()\n",
       "\n",
       "Freeze all params for inference & set model to eval"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.freeze_to\" class=\"doc_header\"><code>GaleModule.freeze_to</code><a href=\"__main__.py#L56\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleModule.freeze_to</code>(**`n`**)\n",
       "\n",
       "Freeze parameter groups up to `n`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.unfreeze\" class=\"doc_header\"><code>GaleModule.unfreeze</code><a href=\"__main__.py#L39\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleModule.unfreeze</code>()\n",
       "\n",
       "Unfreeze all parameters for training."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.as_frozen\" class=\"doc_header\"><code>GaleModule.as_frozen</code><a href=\"__main__.py#L70\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleModule.as_frozen</code>()\n",
       "\n",
       "Context manager which temporarily freezes a module, yields control\n",
       "and finally unfreezes the module."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleModule.from_config_dict)\\nshow_doc(GaleModule.param_lists)\\nshow_doc(GaleModule.all_params)\\nshow_doc(GaleModule.freeze)\\nshow_doc(GaleModule.freeze_to)\\nshow_doc(GaleModule.unfreeze)\\nshow_doc(GaleModule.as_frozen)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleModule.from_config_dict)\\nshow_doc(GaleModule.param_lists)\\nshow_doc(GaleModule.all_params)\\nshow_doc(GaleModule.freeze)\\nshow_doc(GaleModule.freeze_to)\\nshow_doc(GaleModule.unfreeze)\\nshow_doc(GaleModule.as_frozen)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleModule.from_config_dict)\n",
    "show_doc(GaleModule.param_lists)\n",
    "show_doc(GaleModule.all_params)\n",
    "show_doc(GaleModule.freeze)\n",
    "show_doc(GaleModule.freeze_to)\n",
    "show_doc(GaleModule.unfreeze)\n",
    "show_doc(GaleModule.as_frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"# export\\nclass OptimSchedBuilder:\\n    \\\"\\\"\\\"\\n    Interface that constructs an Optimizer and Scheduler for `GaleTask` from config.\\n    \\\"\\\"\\\"\\n\\n    _train_dl: Callable\\n    _trainer: pl.Trainer\\n    optimization_cfg: DictConfig\";\n",
       "                var nbb_formatted_code = \"# export\\nclass OptimSchedBuilder:\\n    \\\"\\\"\\\"\\n    Interface that constructs an Optimizer and Scheduler for `GaleTask` from config.\\n    \\\"\\\"\\\"\\n\\n    _train_dl: Callable\\n    _trainer: pl.Trainer\\n    optimization_cfg: DictConfig\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class OptimSchedBuilder:\n",
    "    \"\"\"\n",
    "    Interface that constructs an Optimizer and Scheduler for `GaleTask` from config.\n",
    "    \"\"\"\n",
    "\n",
    "    _train_dl: Callable\n",
    "    _trainer: pl.Trainer\n",
    "    optimization_cfg: DictConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"from dataclasses import dataclass, field\\n\\nfrom fastcore.all import Path\\nfrom nbdev.export import Config\\nfrom torch.utils.data import DataLoader\\nfrom torchvision.datasets import FashionMNIST\\nfrom gale.config import get_config\\n\\ndata_path = Path(Config().path(\\\"nbs_path\\\")) / \\\"data\\\"\\ndset = FashionMNIST(root=data_path, download=True)\\n\\ncfg = get_config()\\n\\nbuilder = OptimSchedBuilder()\\n# create mock dataloaders and trainer for builder\\nbuilder._train_dl = DataLoader(dset, batch_size=32)\\nbuilder._trainer = pl.Trainer(max_epochs=10, accumulate_grad_batches=1)\\n\\n# print(f\\\"Original Config: \\\\n{OmegaConf.to_yaml(cfg.OPTIMIZATION)}\\\")\";\n",
       "                var nbb_formatted_code = \"from dataclasses import dataclass, field\\n\\nfrom fastcore.all import Path\\nfrom nbdev.export import Config\\nfrom torch.utils.data import DataLoader\\nfrom torchvision.datasets import FashionMNIST\\nfrom gale.config import get_config\\n\\ndata_path = Path(Config().path(\\\"nbs_path\\\")) / \\\"data\\\"\\ndset = FashionMNIST(root=data_path, download=True)\\n\\ncfg = get_config()\\n\\nbuilder = OptimSchedBuilder()\\n# create mock dataloaders and trainer for builder\\nbuilder._train_dl = DataLoader(dset, batch_size=32)\\nbuilder._trainer = pl.Trainer(max_epochs=10, accumulate_grad_batches=1)\\n\\n# print(f\\\"Original Config: \\\\n{OmegaConf.to_yaml(cfg.OPTIMIZATION)}\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "from fastcore.all import Path\n",
    "from nbdev.export import Config\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from gale.config import get_config\n",
    "\n",
    "data_path = Path(Config().path(\"nbs_path\")) / \"data\"\n",
    "dset = FashionMNIST(root=data_path, download=True)\n",
    "\n",
    "cfg = get_config()\n",
    "\n",
    "builder = OptimSchedBuilder()\n",
    "# create mock dataloaders and trainer for builder\n",
    "builder._train_dl = DataLoader(dset, batch_size=32)\n",
    "builder._trainer = pl.Trainer(max_epochs=10, accumulate_grad_batches=1)\n",
    "\n",
    "# print(f\"Original Config: \\n{OmegaConf.to_yaml(cfg.OPTIMIZATION)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"# export\\n@patch\\ndef prepare_optimization_config(self: OptimSchedBuilder, config: DictConfig):\\n    \\\"\\\"\\\"\\n    Prepares `OptimizationConfig` config and adds some interval\\n    values and infers values like max_steps, max_epochs, etc. This method\\n    takes in a config that is inherited from `BaseGaleConfig`. This method also fills in\\n    the values for `max_iters` & `epochs`, `steps_per_epoch` which are required by some of\\n    the LearningRate Schedulers.\\n    \\\"\\\"\\\"\\n    opt_config = copy.deepcopy(config)\\n    self.optimization_cfg = opt_config\\n\\n    self.optimization_cfg[\\\"steps_per_epoch\\\"] = len(self._train_dl)\\n\\n    if self._trainer.max_epochs is None and self._trainer.max_steps is None:\\n        log_main_process(\\n            _logger,\\n            logging.ERROR,\\n            \\\"Either one of max_epochs or max_epochs must be provided in Trainer\\\",\\n        )\\n        raise ValueError\\n\\n    # compute effective num training steps\\n    if (\\n        isinstance(self._trainer.limit_train_batches, int)\\n        and self._trainer.limit_train_batches != 0\\n    ):\\n        dataset_size = self.trainer.limit_train_batches\\n    elif isinstance(self._trainer.limit_train_batches, float):\\n        # limit_train_batches is a percentage of batches\\n        dataset_size = len(self._train_dl)\\n        dataset_size = int(dataset_size * self._trainer.limit_train_batches)\\n    else:\\n        dataset_size = len(self._train_dl)\\n\\n    num_devices = max(1, self._trainer.num_gpus, self._trainer.num_processes)\\n\\n    if self._trainer.tpu_cores:\\n        num_devices = max(num_devices, self._trainer.tpu_cores)\\n\\n    effective_batch_size = self._trainer.accumulate_grad_batches * num_devices\\n    max_steps = (dataset_size // effective_batch_size) * self._trainer.max_epochs\\n\\n    if self._trainer.max_steps is None:\\n        self.optimization_cfg[\\\"max_epochs\\\"] = self._trainer.max_epochs\\n        self.optimization_cfg[\\\"max_steps\\\"] = max_steps\\n\\n    else:\\n        self.optimization_cfg[\\\"max_steps\\\"] = self._trainer.max_steps\\n        self.optimization_cfg[\\\"max_epochs\\\"] = self._trainer.max_steps * len(\\n            self._train_dl\\n        )\\n\\n    # covert config to Dictionary\\n    sched_config = OmegaConf.to_container(\\n        self.optimization_cfg.scheduler.init_args, resolve=True\\n    )\\n\\n    # populate values in learning rate schedulers\\n\\n    if \\\"max_iters\\\" in sched_config:\\n        if sched_config[\\\"max_iters\\\"] is None:\\n            OmegaConf.update(\\n                self.optimization_cfg,\\n                \\\"scheduler.init_args.max_iters\\\",\\n                self.optimization_cfg[\\\"max_steps\\\"],\\n            )\\n            log_main_process(\\n                _logger,\\n                logging.INFO,\\n                f\\\"Set the value of 'max_iters' to be {self.optimization_cfg['max_steps']}\\\",\\n            )\\n\\n    if \\\"epochs\\\" in sched_config:\\n        if sched_config[\\\"epochs\\\"] is None:\\n            OmegaConf.update(\\n                self.optimization_cfg,\\n                \\\"scheduler.init_args.epochs\\\",\\n                self.optimization_cfg[\\\"max_epochs\\\"],\\n            )\\n            log_main_process(\\n                _logger,\\n                logging.INFO,\\n                f\\\"Set the value of 'epochs' to be {self.optimization_cfg['max_epochs']}\\\",\\n            )\\n\\n    if \\\"steps_per_epoch\\\" in sched_config:\\n        if sched_config[\\\"steps_per_epoch\\\"] is None:\\n            OmegaConf.update(\\n                self.optimization_cfg,\\n                \\\"scheduler.init_args.steps_per_epoch\\\",\\n                self.optimization_cfg[\\\"steps_per_epoch\\\"],\\n            )\\n            log_main_process(\\n                _logger,\\n                logging.INFO,\\n                f\\\"Set the value of 'steps_per_epoch' to be {self.optimization_cfg['steps_per_epoch']}\\\",\\n            )\";\n",
       "                var nbb_formatted_code = \"# export\\n@patch\\ndef prepare_optimization_config(self: OptimSchedBuilder, config: DictConfig):\\n    \\\"\\\"\\\"\\n    Prepares `OptimizationConfig` config and adds some interval\\n    values and infers values like max_steps, max_epochs, etc. This method\\n    takes in a config that is inherited from `BaseGaleConfig`. This method also fills in\\n    the values for `max_iters` & `epochs`, `steps_per_epoch` which are required by some of\\n    the LearningRate Schedulers.\\n    \\\"\\\"\\\"\\n    opt_config = copy.deepcopy(config)\\n    self.optimization_cfg = opt_config\\n\\n    self.optimization_cfg[\\\"steps_per_epoch\\\"] = len(self._train_dl)\\n\\n    if self._trainer.max_epochs is None and self._trainer.max_steps is None:\\n        log_main_process(\\n            _logger,\\n            logging.ERROR,\\n            \\\"Either one of max_epochs or max_epochs must be provided in Trainer\\\",\\n        )\\n        raise ValueError\\n\\n    # compute effective num training steps\\n    if (\\n        isinstance(self._trainer.limit_train_batches, int)\\n        and self._trainer.limit_train_batches != 0\\n    ):\\n        dataset_size = self.trainer.limit_train_batches\\n    elif isinstance(self._trainer.limit_train_batches, float):\\n        # limit_train_batches is a percentage of batches\\n        dataset_size = len(self._train_dl)\\n        dataset_size = int(dataset_size * self._trainer.limit_train_batches)\\n    else:\\n        dataset_size = len(self._train_dl)\\n\\n    num_devices = max(1, self._trainer.num_gpus, self._trainer.num_processes)\\n\\n    if self._trainer.tpu_cores:\\n        num_devices = max(num_devices, self._trainer.tpu_cores)\\n\\n    effective_batch_size = self._trainer.accumulate_grad_batches * num_devices\\n    max_steps = (dataset_size // effective_batch_size) * self._trainer.max_epochs\\n\\n    if self._trainer.max_steps is None:\\n        self.optimization_cfg[\\\"max_epochs\\\"] = self._trainer.max_epochs\\n        self.optimization_cfg[\\\"max_steps\\\"] = max_steps\\n\\n    else:\\n        self.optimization_cfg[\\\"max_steps\\\"] = self._trainer.max_steps\\n        self.optimization_cfg[\\\"max_epochs\\\"] = self._trainer.max_steps * len(\\n            self._train_dl\\n        )\\n\\n    # covert config to Dictionary\\n    sched_config = OmegaConf.to_container(\\n        self.optimization_cfg.scheduler.init_args, resolve=True\\n    )\\n\\n    # populate values in learning rate schedulers\\n\\n    if \\\"max_iters\\\" in sched_config:\\n        if sched_config[\\\"max_iters\\\"] is None:\\n            OmegaConf.update(\\n                self.optimization_cfg,\\n                \\\"scheduler.init_args.max_iters\\\",\\n                self.optimization_cfg[\\\"max_steps\\\"],\\n            )\\n            log_main_process(\\n                _logger,\\n                logging.INFO,\\n                f\\\"Set the value of 'max_iters' to be {self.optimization_cfg['max_steps']}\\\",\\n            )\\n\\n    if \\\"epochs\\\" in sched_config:\\n        if sched_config[\\\"epochs\\\"] is None:\\n            OmegaConf.update(\\n                self.optimization_cfg,\\n                \\\"scheduler.init_args.epochs\\\",\\n                self.optimization_cfg[\\\"max_epochs\\\"],\\n            )\\n            log_main_process(\\n                _logger,\\n                logging.INFO,\\n                f\\\"Set the value of 'epochs' to be {self.optimization_cfg['max_epochs']}\\\",\\n            )\\n\\n    if \\\"steps_per_epoch\\\" in sched_config:\\n        if sched_config[\\\"steps_per_epoch\\\"] is None:\\n            OmegaConf.update(\\n                self.optimization_cfg,\\n                \\\"scheduler.init_args.steps_per_epoch\\\",\\n                self.optimization_cfg[\\\"steps_per_epoch\\\"],\\n            )\\n            log_main_process(\\n                _logger,\\n                logging.INFO,\\n                f\\\"Set the value of 'steps_per_epoch' to be {self.optimization_cfg['steps_per_epoch']}\\\",\\n            )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@patch\n",
    "def prepare_optimization_config(self: OptimSchedBuilder, config: DictConfig):\n",
    "    \"\"\"\n",
    "    Prepares `OptimizationConfig` config and adds some interval\n",
    "    values and infers values like max_steps, max_epochs, etc. This method\n",
    "    takes in a config that is inherited from `BaseGaleConfig`. This method also fills in\n",
    "    the values for `max_iters` & `epochs`, `steps_per_epoch` which are required by some of\n",
    "    the LearningRate Schedulers.\n",
    "    \"\"\"\n",
    "    opt_config = copy.deepcopy(config)\n",
    "    self.optimization_cfg = opt_config\n",
    "\n",
    "    self.optimization_cfg[\"steps_per_epoch\"] = len(self._train_dl)\n",
    "\n",
    "    if self._trainer.max_epochs is None and self._trainer.max_steps is None:\n",
    "        log_main_process(\n",
    "            _logger,\n",
    "            logging.ERROR,\n",
    "            \"Either one of max_epochs or max_epochs must be provided in Trainer\",\n",
    "        )\n",
    "        raise ValueError\n",
    "\n",
    "    # compute effective num training steps\n",
    "    if (\n",
    "        isinstance(self._trainer.limit_train_batches, int)\n",
    "        and self._trainer.limit_train_batches != 0\n",
    "    ):\n",
    "        dataset_size = self.trainer.limit_train_batches\n",
    "    elif isinstance(self._trainer.limit_train_batches, float):\n",
    "        # limit_train_batches is a percentage of batches\n",
    "        dataset_size = len(self._train_dl)\n",
    "        dataset_size = int(dataset_size * self._trainer.limit_train_batches)\n",
    "    else:\n",
    "        dataset_size = len(self._train_dl)\n",
    "\n",
    "    num_devices = max(1, self._trainer.num_gpus, self._trainer.num_processes)\n",
    "\n",
    "    if self._trainer.tpu_cores:\n",
    "        num_devices = max(num_devices, self._trainer.tpu_cores)\n",
    "\n",
    "    effective_batch_size = self._trainer.accumulate_grad_batches * num_devices\n",
    "    max_steps = (dataset_size // effective_batch_size) * self._trainer.max_epochs\n",
    "\n",
    "    if self._trainer.max_steps is None:\n",
    "        self.optimization_cfg[\"max_epochs\"] = self._trainer.max_epochs\n",
    "        self.optimization_cfg[\"max_steps\"] = max_steps\n",
    "\n",
    "    else:\n",
    "        self.optimization_cfg[\"max_steps\"] = self._trainer.max_steps\n",
    "        self.optimization_cfg[\"max_epochs\"] = self._trainer.max_steps * len(\n",
    "            self._train_dl\n",
    "        )\n",
    "\n",
    "    # covert config to Dictionary\n",
    "    sched_config = OmegaConf.to_container(\n",
    "        self.optimization_cfg.scheduler.init_args, resolve=True\n",
    "    )\n",
    "\n",
    "    # populate values in learning rate schedulers\n",
    "\n",
    "    if \"max_iters\" in sched_config:\n",
    "        if sched_config[\"max_iters\"] is None:\n",
    "            OmegaConf.update(\n",
    "                self.optimization_cfg,\n",
    "                \"scheduler.init_args.max_iters\",\n",
    "                self.optimization_cfg[\"max_steps\"],\n",
    "            )\n",
    "            log_main_process(\n",
    "                _logger,\n",
    "                logging.INFO,\n",
    "                f\"Set the value of 'max_iters' to be {self.optimization_cfg['max_steps']}\",\n",
    "            )\n",
    "\n",
    "    if \"epochs\" in sched_config:\n",
    "        if sched_config[\"epochs\"] is None:\n",
    "            OmegaConf.update(\n",
    "                self.optimization_cfg,\n",
    "                \"scheduler.init_args.epochs\",\n",
    "                self.optimization_cfg[\"max_epochs\"],\n",
    "            )\n",
    "            log_main_process(\n",
    "                _logger,\n",
    "                logging.INFO,\n",
    "                f\"Set the value of 'epochs' to be {self.optimization_cfg['max_epochs']}\",\n",
    "            )\n",
    "\n",
    "    if \"steps_per_epoch\" in sched_config:\n",
    "        if sched_config[\"steps_per_epoch\"] is None:\n",
    "            OmegaConf.update(\n",
    "                self.optimization_cfg,\n",
    "                \"scheduler.init_args.steps_per_epoch\",\n",
    "                self.optimization_cfg[\"steps_per_epoch\"],\n",
    "            )\n",
    "            log_main_process(\n",
    "                _logger,\n",
    "                logging.INFO,\n",
    "                f\"Set the value of 'steps_per_epoch' to be {self.optimization_cfg['steps_per_epoch']}\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/23 12:09:35 gale.core.classes]: \u001b[0mSet the value of 'epochs' to be 10\n",
      "\u001b[32m[04/23 12:09:35 gale.core.classes]: \u001b[0mSet the value of 'steps_per_epoch' to be 1875\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"builder.prepare_optimization_config(config=cfg.OPTIMIZATION)\\n# print(f\\\"Modified Optimization Config: \\\\n{OmegaConf.to_yaml(builder.optimization_cfg)}\\\")\";\n",
       "                var nbb_formatted_code = \"builder.prepare_optimization_config(config=cfg.OPTIMIZATION)\\n# print(f\\\"Modified Optimization Config: \\\\n{OmegaConf.to_yaml(builder.optimization_cfg)}\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "builder.prepare_optimization_config(config=cfg.OPTIMIZATION)\n",
    "# print(f\"Modified Optimization Config: \\n{OmegaConf.to_yaml(builder.optimization_cfg)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 38;\n",
       "                var nbb_unformatted_code = \"# export\\n@patch\\ndef build_optimizer(self: OptimSchedBuilder, params: Any) -> torch.optim.Optimizer:\\n    \\\"\\\"\\\"\\n    Builds a single optimizer from `OptimizationConfig`. `params` are the parameter\\n    dict with the weights for the optimizer to optimizer.\\n\\n    Note this method must be called after `prepare_optimization_config()`\\n    \\\"\\\"\\\"\\n    if not isinstance(self.optimization_cfg, DictConfig):\\n        msg = \\\"optimization_cfg not found, did you call `prepare_optimization_config`.\\\"\\n        log_main_process(_logger, logging.WARNING, msg)\\n        raise NameError\\n    else:\\n        if self.optimization_cfg.optimizer.name is None:\\n            msg = \\\"Optimizer is None, so no optimizer will be created.\\\"\\n            log_main_process(_logger, logging.WARNING, msg)\\n            opt = None\\n        else:\\n            opt = self.optimization_cfg.optimizer\\n            opt = OPTIM_REGISTRY.get(opt.name)(params=params, **opt.init_args)\\n            msg = f\\\"[OptimSchedBuilder] Built optimizer, {opt.__class__.__name__} with {len(opt.param_groups)} param groups.\\\"\\n            log_main_process(_logger, logging.INFO, msg)\\n        return opt\";\n",
       "                var nbb_formatted_code = \"# export\\n@patch\\ndef build_optimizer(self: OptimSchedBuilder, params: Any) -> torch.optim.Optimizer:\\n    \\\"\\\"\\\"\\n    Builds a single optimizer from `OptimizationConfig`. `params` are the parameter\\n    dict with the weights for the optimizer to optimizer.\\n\\n    Note this method must be called after `prepare_optimization_config()`\\n    \\\"\\\"\\\"\\n    if not isinstance(self.optimization_cfg, DictConfig):\\n        msg = \\\"optimization_cfg not found, did you call `prepare_optimization_config`.\\\"\\n        log_main_process(_logger, logging.WARNING, msg)\\n        raise NameError\\n    else:\\n        if self.optimization_cfg.optimizer.name is None:\\n            msg = \\\"Optimizer is None, so no optimizer will be created.\\\"\\n            log_main_process(_logger, logging.WARNING, msg)\\n            opt = None\\n        else:\\n            opt = self.optimization_cfg.optimizer\\n            opt = OPTIM_REGISTRY.get(opt.name)(params=params, **opt.init_args)\\n            msg = f\\\"[OptimSchedBuilder] Built optimizer, {opt.__class__.__name__} with {len(opt.param_groups)} param groups.\\\"\\n            log_main_process(_logger, logging.INFO, msg)\\n        return opt\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@patch\n",
    "def build_optimizer(self: OptimSchedBuilder, params: Any) -> torch.optim.Optimizer:\n",
    "    \"\"\"\n",
    "    Builds a single optimizer from `OptimizationConfig`. `params` are the parameter\n",
    "    dict with the weights for the optimizer to optimizer.\n",
    "\n",
    "    Note this method must be called after `prepare_optimization_config()`\n",
    "    \"\"\"\n",
    "    if not isinstance(self.optimization_cfg, DictConfig):\n",
    "        msg = \"optimization_cfg not found, did you call `prepare_optimization_config`.\"\n",
    "        log_main_process(_logger, logging.WARNING, msg)\n",
    "        raise NameError\n",
    "    else:\n",
    "        if self.optimization_cfg.optimizer.name is None:\n",
    "            msg = \"Optimizer is None, so no optimizer will be created.\"\n",
    "            log_main_process(_logger, logging.WARNING, msg)\n",
    "            opt = None\n",
    "        else:\n",
    "            opt = self.optimization_cfg.optimizer\n",
    "            opt = OPTIM_REGISTRY.get(opt.name)(params=params, **opt.init_args)\n",
    "            msg = f\"[OptimSchedBuilder] Built optimizer, {opt.__class__.__name__} with {len(opt.param_groups)} param groups.\"\n",
    "            log_main_process(_logger, logging.INFO, msg)\n",
    "        return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/23 12:19:40 gale.core.classes]: \u001b[0m[OptimSchedBuilder] Built optimizer, AdamW with 1 param groups.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 39;\n",
       "                var nbb_unformatted_code = \"params = [torch.nn.Parameter(torch.randn(1, 2))]\\noptimizer = builder.build_optimizer(params)\\nassert isinstance(optimizer, torch.optim.AdamW)\";\n",
       "                var nbb_formatted_code = \"params = [torch.nn.Parameter(torch.randn(1, 2))]\\noptimizer = builder.build_optimizer(params)\\nassert isinstance(optimizer, torch.optim.AdamW)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = [torch.nn.Parameter(torch.randn(1, 2))]\n",
    "optimizer = builder.build_optimizer(params)\n",
    "assert isinstance(optimizer, torch.optim.AdamW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 44;\n",
       "                var nbb_unformatted_code = \"# export\\n@patch\\ndef build_lr_scheduler(\\n    self: OptimSchedBuilder, optimizer: torch.optim.Optimizer\\n) -> Any:\\n    \\\"\\\"\\\"\\n    Builds a LearningRate scheduler from `OptimizationConfig`. Returns an LRScheduler dict\\n    that is required by PyTorch Lightning for LRSchedulers.\\n    Note this method must be called after `prepare_optimization_config()`\\n    \\\"\\\"\\\"\\n    if not isinstance(self.optimization_cfg, DictConfig):\\n        msg = \\\"optimization_cfg not found, did you call `prepare_optimization_config`.\\\"\\n        log_main_process(_logger, logging.WARNING, msg)\\n        raise NameError\\n    else:\\n        if self.optimization_cfg.scheduler.name is None:\\n            msg = \\\"scheduler is None, so no scheduler will be created.\\\"\\n            log_main_process(_logger, logging.WARNING, msg)\\n            sched = None\\n        else:\\n            _c = self.optimization_cfg.scheduler.init_args\\n            _temp = OmegaConf.to_container(_c, resolve=True)\\n            kwds = {}\\n\\n            # if a key value is ListConfig then we convert it to simple list\\n            for key, value in _temp.items():\\n                if isinstance(value, list):\\n                    kwds[key] = list(value)\\n                else:\\n                    kwds[key] = value\\n\\n            instance = SCHEDULER_REGISTRY.get(self.optimization_cfg.scheduler.name)\\n            sch = instance(optimizer=optimizer, **kwds)\\n\\n            # convert the lr_scheduler to pytorch-lightning LRScheduler dictionary format\\n            msg = f\\\"[OptimSchedBuilder] LRScheduler : {sch.__class__.__name__}.\\\"\\n            log_main_process(_logger, logging.INFO, msg)\\n            sched = {\\n                \\\"scheduler\\\": sch,\\n                \\\"interval\\\": self.optimization_cfg.scheduler.interval,\\n                \\\"monitor\\\": self.optimization_cfg.scheduler.monitor,\\n            }\\n            return sched\";\n",
       "                var nbb_formatted_code = \"# export\\n@patch\\ndef build_lr_scheduler(\\n    self: OptimSchedBuilder, optimizer: torch.optim.Optimizer\\n) -> Any:\\n    \\\"\\\"\\\"\\n    Builds a LearningRate scheduler from `OptimizationConfig`. Returns an LRScheduler dict\\n    that is required by PyTorch Lightning for LRSchedulers.\\n    Note this method must be called after `prepare_optimization_config()`\\n    \\\"\\\"\\\"\\n    if not isinstance(self.optimization_cfg, DictConfig):\\n        msg = \\\"optimization_cfg not found, did you call `prepare_optimization_config`.\\\"\\n        log_main_process(_logger, logging.WARNING, msg)\\n        raise NameError\\n    else:\\n        if self.optimization_cfg.scheduler.name is None:\\n            msg = \\\"scheduler is None, so no scheduler will be created.\\\"\\n            log_main_process(_logger, logging.WARNING, msg)\\n            sched = None\\n        else:\\n            _c = self.optimization_cfg.scheduler.init_args\\n            _temp = OmegaConf.to_container(_c, resolve=True)\\n            kwds = {}\\n\\n            # if a key value is ListConfig then we convert it to simple list\\n            for key, value in _temp.items():\\n                if isinstance(value, list):\\n                    kwds[key] = list(value)\\n                else:\\n                    kwds[key] = value\\n\\n            instance = SCHEDULER_REGISTRY.get(self.optimization_cfg.scheduler.name)\\n            sch = instance(optimizer=optimizer, **kwds)\\n\\n            # convert the lr_scheduler to pytorch-lightning LRScheduler dictionary format\\n            msg = f\\\"[OptimSchedBuilder] LRScheduler : {sch.__class__.__name__}.\\\"\\n            log_main_process(_logger, logging.INFO, msg)\\n            sched = {\\n                \\\"scheduler\\\": sch,\\n                \\\"interval\\\": self.optimization_cfg.scheduler.interval,\\n                \\\"monitor\\\": self.optimization_cfg.scheduler.monitor,\\n            }\\n            return sched\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@patch\n",
    "def build_lr_scheduler(\n",
    "    self: OptimSchedBuilder, optimizer: torch.optim.Optimizer\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Builds a LearningRate scheduler from `OptimizationConfig`. Returns an LRScheduler dict\n",
    "    that is required by PyTorch Lightning for LRSchedulers.\n",
    "    Note this method must be called after `prepare_optimization_config()`\n",
    "    \"\"\"\n",
    "    if not isinstance(self.optimization_cfg, DictConfig):\n",
    "        msg = \"optimization_cfg not found, did you call `prepare_optimization_config`.\"\n",
    "        log_main_process(_logger, logging.WARNING, msg)\n",
    "        raise NameError\n",
    "    else:\n",
    "        if self.optimization_cfg.scheduler.name is None:\n",
    "            msg = \"scheduler is None, so no scheduler will be created.\"\n",
    "            log_main_process(_logger, logging.WARNING, msg)\n",
    "            sched = None\n",
    "        else:\n",
    "            _c = self.optimization_cfg.scheduler.init_args\n",
    "            _temp = OmegaConf.to_container(_c, resolve=True)\n",
    "            kwds = {}\n",
    "\n",
    "            # if a key value is ListConfig then we convert it to simple list\n",
    "            for key, value in _temp.items():\n",
    "                if isinstance(value, list):\n",
    "                    kwds[key] = list(value)\n",
    "                else:\n",
    "                    kwds[key] = value\n",
    "\n",
    "            instance = SCHEDULER_REGISTRY.get(self.optimization_cfg.scheduler.name)\n",
    "            sch = instance(optimizer=optimizer, **kwds)\n",
    "\n",
    "            # convert the lr_scheduler to pytorch-lightning LRScheduler dictionary format\n",
    "            msg = f\"[OptimSchedBuilder] LRScheduler : {sch.__class__.__name__}.\"\n",
    "            log_main_process(_logger, logging.INFO, msg)\n",
    "            sched = {\n",
    "                \"scheduler\": sch,\n",
    "                \"interval\": self.optimization_cfg.scheduler.interval,\n",
    "                \"monitor\": self.optimization_cfg.scheduler.monitor,\n",
    "            }\n",
    "            return sched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/23 12:20:36 gale.core.classes]: \u001b[0m[OptimSchedBuilder] LRScheduler : OneCycleLR.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 45;\n",
       "                var nbb_unformatted_code = \"builder.optimization_cfg.scheduler.init_args.max_lr = [1e-03]\\nscheduler = builder.build_lr_scheduler(optimizer)\\nassert isinstance(scheduler, Dict)\\nassert isinstance(scheduler[\\\"scheduler\\\"], torch.optim.lr_scheduler.OneCycleLR)\";\n",
       "                var nbb_formatted_code = \"builder.optimization_cfg.scheduler.init_args.max_lr = [1e-03]\\nscheduler = builder.build_lr_scheduler(optimizer)\\nassert isinstance(scheduler, Dict)\\nassert isinstance(scheduler[\\\"scheduler\\\"], torch.optim.lr_scheduler.OneCycleLR)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "builder.optimization_cfg.scheduler.init_args.max_lr = [1e-03]\n",
    "scheduler = builder.build_lr_scheduler(optimizer)\n",
    "assert isinstance(scheduler, Dict)\n",
    "assert isinstance(scheduler[\"scheduler\"], torch.optim.lr_scheduler.OneCycleLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 46;\n",
       "                var nbb_unformatted_code = \"# export\\nclass GaleTask(pl.LightningModule, OptimSchedBuilder, metaclass=ABCMeta):\\n    \\\"\\\"\\\"\\n    Interface for Pytorch-lightning based Gale modules\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        cfg: DictConfig,\\n        trainer: Optional[pl.Trainer] = None,\\n        metrics: Union[torchmetrics.Metric, Mapping, Sequence, None] = None,\\n    ):\\n        \\\"\\\"\\\"\\n        Base class from which all NeMo models should inherit.\\n\\n        Arguments:\\n        1. `cfg` `(DictConfig)`:  configuration object. cfg object should be inherited from `BaseGaleConfig`.\\n        2. `trainer` `(Optional, pl.Trainer)`: Pytorch Lightning Trainer instance\\n        3. `metrics` `(Optional)`: Metrics to compute for training and evaluation.\\n        \\\"\\\"\\\"\\n        super().__init__()\\n        self._cfg = OmegaConf.structured(cfg)\\n\\n        self.save_hyperparameters(self._cfg)\\n        self._train_dl = noop\\n        self._validation_dl = noop\\n        self._test_dl = noop\\n        self._optimizer = noop\\n        self._scheduler = noop\\n        self._trainer = trainer\\n        self.metrics = metrics\\n\\n    def train_dataloader(self) -> torch.utils.data.DataLoader:\\n        \\\"Returns the Dataloader used for Training\\\"\\n        if self._train_dl is not None and self._train_dl is not noop:\\n            return self._train_dl\\n\\n    def val_dataloader(self) -> Any:\\n        \\\"Returns the List of Dataloaders or Dataloader used for Validation\\\"\\n        if self._validation_dl is not None and self._validation_dl is not noop:\\n            return self._validation_dl\\n\\n    def test_dataloader(self) -> Any:\\n        \\\"Returns the List of Dataloaders or Dataloader used for Testing\\\"\\n        if self._test_dl is not None and self._test_dl is not noop:\\n            return self._test_dl\\n\\n    @abstractmethod\\n    def forward(self, x: torch.Tensor):\\n        \\\"\\\"\\\"\\n        The Forward method for LightningModule, users should modify this method.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def setup_training_data(self, train_data_config: Union[DictConfig, Dict]):\\n        \\\"\\\"\\\"\\n        Setups data loader to be used in training\\n\\n        Arguments:\\n        1. `train_data_config`: training data loader parameters.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def setup_validation_data(self, val_data_config: Union[DictConfig, Dict]):\\n        \\\"\\\"\\\"\\n        Setups data loader to be used in validation\\n\\n        Arguments:\\n        1. `val_data_config`: validation data loader parameters.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    def setup_test_data(\\n        self, test_data_config: Optional[Union[DictConfig, Dict]] = None\\n    ):\\n        \\\"\\\"\\\"\\n        (Optionally) Setups data loader to be used in test\\n\\n        Arguments:\\n        1. `test_data_config`: test data loader parameters.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @property\\n    def param_dicts(self) -> Union[Iterator, List[Dict]]:\\n        \\\"\\\"\\\"\\n        Property that returns the param dicts for optimization.\\n        Override for custom training behaviour. Currently returns all the trainable paramters.\\n        \\\"\\\"\\\"\\n        return L(self).map(trainable_params)\";\n",
       "                var nbb_formatted_code = \"# export\\nclass GaleTask(pl.LightningModule, OptimSchedBuilder, metaclass=ABCMeta):\\n    \\\"\\\"\\\"\\n    Interface for Pytorch-lightning based Gale modules\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        cfg: DictConfig,\\n        trainer: Optional[pl.Trainer] = None,\\n        metrics: Union[torchmetrics.Metric, Mapping, Sequence, None] = None,\\n    ):\\n        \\\"\\\"\\\"\\n        Base class from which all NeMo models should inherit.\\n\\n        Arguments:\\n        1. `cfg` `(DictConfig)`:  configuration object. cfg object should be inherited from `BaseGaleConfig`.\\n        2. `trainer` `(Optional, pl.Trainer)`: Pytorch Lightning Trainer instance\\n        3. `metrics` `(Optional)`: Metrics to compute for training and evaluation.\\n        \\\"\\\"\\\"\\n        super().__init__()\\n        self._cfg = OmegaConf.structured(cfg)\\n\\n        self.save_hyperparameters(self._cfg)\\n        self._train_dl = noop\\n        self._validation_dl = noop\\n        self._test_dl = noop\\n        self._optimizer = noop\\n        self._scheduler = noop\\n        self._trainer = trainer\\n        self.metrics = metrics\\n\\n    def train_dataloader(self) -> torch.utils.data.DataLoader:\\n        \\\"Returns the Dataloader used for Training\\\"\\n        if self._train_dl is not None and self._train_dl is not noop:\\n            return self._train_dl\\n\\n    def val_dataloader(self) -> Any:\\n        \\\"Returns the List of Dataloaders or Dataloader used for Validation\\\"\\n        if self._validation_dl is not None and self._validation_dl is not noop:\\n            return self._validation_dl\\n\\n    def test_dataloader(self) -> Any:\\n        \\\"Returns the List of Dataloaders or Dataloader used for Testing\\\"\\n        if self._test_dl is not None and self._test_dl is not noop:\\n            return self._test_dl\\n\\n    @abstractmethod\\n    def forward(self, x: torch.Tensor):\\n        \\\"\\\"\\\"\\n        The Forward method for LightningModule, users should modify this method.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def setup_training_data(self, train_data_config: Union[DictConfig, Dict]):\\n        \\\"\\\"\\\"\\n        Setups data loader to be used in training\\n\\n        Arguments:\\n        1. `train_data_config`: training data loader parameters.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def setup_validation_data(self, val_data_config: Union[DictConfig, Dict]):\\n        \\\"\\\"\\\"\\n        Setups data loader to be used in validation\\n\\n        Arguments:\\n        1. `val_data_config`: validation data loader parameters.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    def setup_test_data(\\n        self, test_data_config: Optional[Union[DictConfig, Dict]] = None\\n    ):\\n        \\\"\\\"\\\"\\n        (Optionally) Setups data loader to be used in test\\n\\n        Arguments:\\n        1. `test_data_config`: test data loader parameters.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @property\\n    def param_dicts(self) -> Union[Iterator, List[Dict]]:\\n        \\\"\\\"\\\"\\n        Property that returns the param dicts for optimization.\\n        Override for custom training behaviour. Currently returns all the trainable paramters.\\n        \\\"\\\"\\\"\\n        return L(self).map(trainable_params)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class GaleTask(pl.LightningModule, OptimSchedBuilder, metaclass=ABCMeta):\n",
    "    \"\"\"\n",
    "    Interface for Pytorch-lightning based Gale modules\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: DictConfig,\n",
    "        trainer: Optional[pl.Trainer] = None,\n",
    "        metrics: Union[torchmetrics.Metric, Mapping, Sequence, None] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Base class from which all NeMo models should inherit.\n",
    "\n",
    "        Arguments:\n",
    "        1. `cfg` `(DictConfig)`:  configuration object. cfg object should be inherited from `BaseGaleConfig`.\n",
    "        2. `trainer` `(Optional, pl.Trainer)`: Pytorch Lightning Trainer instance\n",
    "        3. `metrics` `(Optional)`: Metrics to compute for training and evaluation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._cfg = OmegaConf.structured(cfg)\n",
    "\n",
    "        self.save_hyperparameters(self._cfg)\n",
    "        self._train_dl = noop\n",
    "        self._validation_dl = noop\n",
    "        self._test_dl = noop\n",
    "        self._optimizer = noop\n",
    "        self._scheduler = noop\n",
    "        self._trainer = trainer\n",
    "        self.metrics = metrics\n",
    "\n",
    "    def train_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        \"Returns the Dataloader used for Training\"\n",
    "        if self._train_dl is not None and self._train_dl is not noop:\n",
    "            return self._train_dl\n",
    "\n",
    "    def val_dataloader(self) -> Any:\n",
    "        \"Returns the List of Dataloaders or Dataloader used for Validation\"\n",
    "        if self._validation_dl is not None and self._validation_dl is not noop:\n",
    "            return self._validation_dl\n",
    "\n",
    "    def test_dataloader(self) -> Any:\n",
    "        \"Returns the List of Dataloaders or Dataloader used for Testing\"\n",
    "        if self._test_dl is not None and self._test_dl is not noop:\n",
    "            return self._test_dl\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        The Forward method for LightningModule, users should modify this method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def setup_training_data(self, train_data_config: Union[DictConfig, Dict]):\n",
    "        \"\"\"\n",
    "        Setups data loader to be used in training\n",
    "\n",
    "        Arguments:\n",
    "        1. `train_data_config`: training data loader parameters.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def setup_validation_data(self, val_data_config: Union[DictConfig, Dict]):\n",
    "        \"\"\"\n",
    "        Setups data loader to be used in validation\n",
    "\n",
    "        Arguments:\n",
    "        1. `val_data_config`: validation data loader parameters.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def setup_test_data(\n",
    "        self, test_data_config: Optional[Union[DictConfig, Dict]] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        (Optionally) Setups data loader to be used in test\n",
    "\n",
    "        Arguments:\n",
    "        1. `test_data_config`: test data loader parameters.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def param_dicts(self) -> Union[Iterator, List[Dict]]:\n",
    "        \"\"\"\n",
    "        Property that returns the param dicts for optimization.\n",
    "        Override for custom training behaviour. Currently returns all the trainable paramters.\n",
    "        \"\"\"\n",
    "        return L(self).map(trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"GaleTask\" class=\"doc_header\"><code>class</code> <code>GaleTask</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>GaleTask</code>(**`cfg`**:`DictConfig`, **`trainer`**:`Optional`\\[`Trainer`\\]=*`None`*, **`metrics`**:`Union`\\[`Metric`, `Mapping`, `Sequence`, `NoneType`\\]=*`None`*) :: `LightningModule`\n",
       "\n",
       "Interface for Pytorch-lightning based Gale modules"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.train_dataloader\" class=\"doc_header\"><code>GaleTask.train_dataloader</code><a href=\"__main__.py#L33\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.train_dataloader</code>()\n",
       "\n",
       "Returns the Dataloader used for Training"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.setup_training_data\" class=\"doc_header\"><code>GaleTask.setup_training_data</code><a href=\"__main__.py#L55\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.setup_training_data</code>(**`train_data_config`**:`Union`\\[`DictConfig`, `Dict`\\])\n",
       "\n",
       "Setups data loader to be used in training\n",
       "\n",
       "Arguments:\n",
       "1. `train_data_config`: training data loader parameters."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.val_dataloader\" class=\"doc_header\"><code>GaleTask.val_dataloader</code><a href=\"__main__.py#L38\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.val_dataloader</code>()\n",
       "\n",
       "Returns the List of Dataloaders or Dataloader used for Validation"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.setup_validation_data\" class=\"doc_header\"><code>GaleTask.setup_validation_data</code><a href=\"__main__.py#L65\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.setup_validation_data</code>(**`val_data_config`**:`Union`\\[`DictConfig`, `Dict`\\])\n",
       "\n",
       "Setups data loader to be used in validation\n",
       "\n",
       "Arguments:\n",
       "1. `val_data_config`: validation data loader parameters."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.test_dataloader\" class=\"doc_header\"><code>GaleTask.test_dataloader</code><a href=\"__main__.py#L43\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.test_dataloader</code>()\n",
       "\n",
       "Returns the List of Dataloaders or Dataloader used for Testing"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.setup_test_data\" class=\"doc_header\"><code>GaleTask.setup_test_data</code><a href=\"__main__.py#L75\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.setup_test_data</code>(**`test_data_config`**:`Union`\\[`DictConfig`, `Dict`, `NoneType`\\]=*`None`*)\n",
       "\n",
       "(Optionally) Setups data loader to be used in test\n",
       "\n",
       "Arguments:\n",
       "1. `test_data_config`: test data loader parameters."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 47;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask)\\n\\nshow_doc(GaleTask.train_dataloader)\\nshow_doc(GaleTask.setup_training_data)\\n\\nshow_doc(GaleTask.val_dataloader)\\nshow_doc(GaleTask.setup_validation_data)\\n\\nshow_doc(GaleTask.test_dataloader)\\nshow_doc(GaleTask.setup_test_data)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask)\\n\\nshow_doc(GaleTask.train_dataloader)\\nshow_doc(GaleTask.setup_training_data)\\n\\nshow_doc(GaleTask.val_dataloader)\\nshow_doc(GaleTask.setup_validation_data)\\n\\nshow_doc(GaleTask.test_dataloader)\\nshow_doc(GaleTask.setup_test_data)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask)\n",
    "\n",
    "show_doc(GaleTask.train_dataloader)\n",
    "show_doc(GaleTask.setup_training_data)\n",
    "\n",
    "show_doc(GaleTask.val_dataloader)\n",
    "show_doc(GaleTask.setup_validation_data)\n",
    "\n",
    "show_doc(GaleTask.test_dataloader)\n",
    "show_doc(GaleTask.setup_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.forward\" class=\"doc_header\"><code>GaleTask.forward</code><a href=\"__main__.py#L48\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.forward</code>(**`x`**:`Tensor`)\n",
       "\n",
       "The Forward method for LightningModule, users should modify this method."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.param_dicts\" class=\"doc_header\"><code>GaleTask.param_dicts</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "Property that returns the param dicts for optimization.\n",
       "Override for custom training behaviour. Currently returns all the trainable paramters."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 48;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.forward)\\n\\nshow_doc(GaleTask.param_dicts)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.forward)\\n\\nshow_doc(GaleTask.param_dicts)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.forward)\n",
    "\n",
    "show_doc(GaleTask.param_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 49;\n",
       "                var nbb_unformatted_code = \"# export\\n@patch\\ndef shared_step(self: GaleTask, batch: Any, batch_idx: int, stage: str) -> Any:\\n    \\\"\\\"\\\"\\n    The common training/validation/test step. Override for custom behavior. This step\\n    is shared between training/validation/test step. For training/validation/test steps\\n    `stage` is train/val/test respectively. You training logic should go here avoid directly overriding\\n    training/validation/test step methods.\\n    \\\"\\\"\\\"\\n    raise NotImplementedError\";\n",
       "                var nbb_formatted_code = \"# export\\n@patch\\ndef shared_step(self: GaleTask, batch: Any, batch_idx: int, stage: str) -> Any:\\n    \\\"\\\"\\\"\\n    The common training/validation/test step. Override for custom behavior. This step\\n    is shared between training/validation/test step. For training/validation/test steps\\n    `stage` is train/val/test respectively. You training logic should go here avoid directly overriding\\n    training/validation/test step methods.\\n    \\\"\\\"\\\"\\n    raise NotImplementedError\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@patch\n",
    "def shared_step(self: GaleTask, batch: Any, batch_idx: int, stage: str) -> Any:\n",
    "    \"\"\"\n",
    "    The common training/validation/test step. Override for custom behavior. This step\n",
    "    is shared between training/validation/test step. For training/validation/test steps\n",
    "    `stage` is train/val/test respectively. You training logic should go here avoid directly overriding\n",
    "    training/validation/test step methods.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 50;\n",
       "                var nbb_unformatted_code = \"# export\\n@patch\\ndef training_step(self: GaleTask, batch: Any, batch_idx: int) -> Any:\\n    \\\"\\\"\\\"\\n    The training step of the LightningModule. For common use cases you need\\n    not need to override this method. See `GaleTask.shared_step()`\\n    \\\"\\\"\\\"\\n    return self.shared_step(batch, batch_idx, stage=\\\"train\\\")\";\n",
       "                var nbb_formatted_code = \"# export\\n@patch\\ndef training_step(self: GaleTask, batch: Any, batch_idx: int) -> Any:\\n    \\\"\\\"\\\"\\n    The training step of the LightningModule. For common use cases you need\\n    not need to override this method. See `GaleTask.shared_step()`\\n    \\\"\\\"\\\"\\n    return self.shared_step(batch, batch_idx, stage=\\\"train\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@patch\n",
    "def training_step(self: GaleTask, batch: Any, batch_idx: int) -> Any:\n",
    "    \"\"\"\n",
    "    The training step of the LightningModule. For common use cases you need\n",
    "    not need to override this method. See `GaleTask.shared_step()`\n",
    "    \"\"\"\n",
    "    return self.shared_step(batch, batch_idx, stage=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 51;\n",
       "                var nbb_unformatted_code = \"# export\\n@patch\\ndef validation_step(self: GaleTask, batch: Any, batch_idx: int) -> None:\\n    \\\"\\\"\\\"\\n    The validation step of the LightningModule. For common use cases you need\\n    not need to override this method. See `GaleTask.shared_step()`\\n    \\\"\\\"\\\"\\n    return self.shared_step(batch, batch_idx, stage=\\\"val\\\")\";\n",
       "                var nbb_formatted_code = \"# export\\n@patch\\ndef validation_step(self: GaleTask, batch: Any, batch_idx: int) -> None:\\n    \\\"\\\"\\\"\\n    The validation step of the LightningModule. For common use cases you need\\n    not need to override this method. See `GaleTask.shared_step()`\\n    \\\"\\\"\\\"\\n    return self.shared_step(batch, batch_idx, stage=\\\"val\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@patch\n",
    "def validation_step(self: GaleTask, batch: Any, batch_idx: int) -> None:\n",
    "    \"\"\"\n",
    "    The validation step of the LightningModule. For common use cases you need\n",
    "    not need to override this method. See `GaleTask.shared_step()`\n",
    "    \"\"\"\n",
    "    return self.shared_step(batch, batch_idx, stage=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 52;\n",
       "                var nbb_unformatted_code = \"# export\\n@patch\\ndef test_step(self: GaleTask, batch: Any, batch_idx: int) -> None:\\n    \\\"\\\"\\\"\\n    The test step of the LightningModule. For common use cases you need\\n    not need to override this method. See `GaleTask.shared_step()`\\n    \\\"\\\"\\\"\\n    return self.shared_step(batch, batch_idx, stage=\\\"test\\\")\";\n",
       "                var nbb_formatted_code = \"# export\\n@patch\\ndef test_step(self: GaleTask, batch: Any, batch_idx: int) -> None:\\n    \\\"\\\"\\\"\\n    The test step of the LightningModule. For common use cases you need\\n    not need to override this method. See `GaleTask.shared_step()`\\n    \\\"\\\"\\\"\\n    return self.shared_step(batch, batch_idx, stage=\\\"test\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@patch\n",
    "def test_step(self: GaleTask, batch: Any, batch_idx: int) -> None:\n",
    "    \"\"\"\n",
    "    The test step of the LightningModule. For common use cases you need\n",
    "    not need to override this method. See `GaleTask.shared_step()`\n",
    "    \"\"\"\n",
    "    return self.shared_step(batch, batch_idx, stage=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.shared_step\" class=\"doc_header\"><code>GaleTask.shared_step</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.shared_step</code>(**`batch`**:`Any`, **`batch_idx`**:`int`, **`stage`**:`str`)\n",
       "\n",
       "The common training/validation/test step. Override for custom behavior. This step\n",
       "is shared between training/validation/test step. For training/validation/test steps\n",
       "`stage` is train/val/test respectively. You training logic should go here avoid directly overriding\n",
       "training/validation/test step methods."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.training_step\" class=\"doc_header\"><code>GaleTask.training_step</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.training_step</code>(**`batch`**:`Any`, **`batch_idx`**:`int`)\n",
       "\n",
       "The training step of the LightningModule. For common use cases you need\n",
       "not need to override this method. See [`GaleTask.shared_step()`](/gale/core.classes.html#GaleTask.shared_step())"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.validation_step\" class=\"doc_header\"><code>GaleTask.validation_step</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.validation_step</code>(**`batch`**:`Any`, **`batch_idx`**:`int`)\n",
       "\n",
       "The validation step of the LightningModule. For common use cases you need\n",
       "not need to override this method. See [`GaleTask.shared_step()`](/gale/core.classes.html#GaleTask.shared_step())"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.test_step\" class=\"doc_header\"><code>GaleTask.test_step</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.test_step</code>(**`batch`**:`Any`, **`batch_idx`**:`int`)\n",
       "\n",
       "The test step of the LightningModule. For common use cases you need\n",
       "not need to override this method. See [`GaleTask.shared_step()`](/gale/core.classes.html#GaleTask.shared_step())"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 53;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.shared_step)\\nshow_doc(GaleTask.training_step)\\nshow_doc(GaleTask.validation_step)\\nshow_doc(GaleTask.test_step)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.shared_step)\\nshow_doc(GaleTask.training_step)\\nshow_doc(GaleTask.validation_step)\\nshow_doc(GaleTask.test_step)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.shared_step)\n",
    "show_doc(GaleTask.training_step)\n",
    "show_doc(GaleTask.validation_step)\n",
    "show_doc(GaleTask.test_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 54;\n",
       "                var nbb_unformatted_code = \"# export\\n@patch\\ndef setup_optimization(self: GaleTask, optim_config: DictConfig = None):\\n    \\\"\\\"\\\"\\n    Prepares an optimizer from a string name and its optional config parameters.\\n\\n    Args:\\n    1. `optim_config`: A `dictionary`/`DictConfig` or instance of `OptimizationConfig`.\\n    \\\"\\\"\\\"\\n    # If config was not explicitly passed to us\\n    if optim_config is None:\\n        # See if internal config has `optim` namespace\\n        if self._cfg is not None and hasattr(self._cfg, \\\"optimization\\\"):\\n            optim_config = self._cfg.optimization\\n\\n    # If config is still None, or internal config has no Optim, return without instantiation\\n    if optim_config is None:\\n        msg = \\\"No optimizer config provided, therefore no optimizer was created\\\"\\n        log_main_process(_logger, logging.WARNING, msg)\\n        return\\n\\n    else:\\n        # Preserve the configuration\\n        if not isinstance(optim_config, DictConfig):\\n            optim_config = OmegaConf.create(optim_config)\\n\\n        # prepare the optimization config\\n        self.prepare_optimization_config(optim_config)\\n\\n        # Setup optimizer and scheduler\\n        self._optimizer = self.build_optimizer(self.param_dicts)\\n        self._scheduler = self.build_lr_scheduler(self._optimizer)\\n\\n\\n@patch\\ndef configure_optimizers(self: GaleTask):\\n    \\\"\\\"\\\"\\n    Choose what optimizers and learning-rate schedulers to use in your optimization.\\n    See https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html\\n    \\\"\\\"\\\"\\n    # if self.setup_optimization() has been called manually no\\n    # need to call again\\n    if self._optimizer is noop and self._scheduler is noop:\\n        self.setup_optimization()\\n\\n    if self._scheduler is None:\\n        return self._optimizer\\n    else:\\n        return [self._optimizer], [self._scheduler]\";\n",
       "                var nbb_formatted_code = \"# export\\n@patch\\ndef setup_optimization(self: GaleTask, optim_config: DictConfig = None):\\n    \\\"\\\"\\\"\\n    Prepares an optimizer from a string name and its optional config parameters.\\n\\n    Args:\\n    1. `optim_config`: A `dictionary`/`DictConfig` or instance of `OptimizationConfig`.\\n    \\\"\\\"\\\"\\n    # If config was not explicitly passed to us\\n    if optim_config is None:\\n        # See if internal config has `optim` namespace\\n        if self._cfg is not None and hasattr(self._cfg, \\\"optimization\\\"):\\n            optim_config = self._cfg.optimization\\n\\n    # If config is still None, or internal config has no Optim, return without instantiation\\n    if optim_config is None:\\n        msg = \\\"No optimizer config provided, therefore no optimizer was created\\\"\\n        log_main_process(_logger, logging.WARNING, msg)\\n        return\\n\\n    else:\\n        # Preserve the configuration\\n        if not isinstance(optim_config, DictConfig):\\n            optim_config = OmegaConf.create(optim_config)\\n\\n        # prepare the optimization config\\n        self.prepare_optimization_config(optim_config)\\n\\n        # Setup optimizer and scheduler\\n        self._optimizer = self.build_optimizer(self.param_dicts)\\n        self._scheduler = self.build_lr_scheduler(self._optimizer)\\n\\n\\n@patch\\ndef configure_optimizers(self: GaleTask):\\n    \\\"\\\"\\\"\\n    Choose what optimizers and learning-rate schedulers to use in your optimization.\\n    See https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html\\n    \\\"\\\"\\\"\\n    # if self.setup_optimization() has been called manually no\\n    # need to call again\\n    if self._optimizer is noop and self._scheduler is noop:\\n        self.setup_optimization()\\n\\n    if self._scheduler is None:\\n        return self._optimizer\\n    else:\\n        return [self._optimizer], [self._scheduler]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@patch\n",
    "def setup_optimization(self: GaleTask, optim_config: DictConfig = None):\n",
    "    \"\"\"\n",
    "    Prepares an optimizer from a string name and its optional config parameters.\n",
    "\n",
    "    Args:\n",
    "    1. `optim_config`: A `dictionary`/`DictConfig` or instance of `OptimizationConfig`.\n",
    "    \"\"\"\n",
    "    # If config was not explicitly passed to us\n",
    "    if optim_config is None:\n",
    "        # See if internal config has `optim` namespace\n",
    "        if self._cfg is not None and hasattr(self._cfg, \"optimization\"):\n",
    "            optim_config = self._cfg.optimization\n",
    "\n",
    "    # If config is still None, or internal config has no Optim, return without instantiation\n",
    "    if optim_config is None:\n",
    "        msg = \"No optimizer config provided, therefore no optimizer was created\"\n",
    "        log_main_process(_logger, logging.WARNING, msg)\n",
    "        return\n",
    "\n",
    "    else:\n",
    "        # Preserve the configuration\n",
    "        if not isinstance(optim_config, DictConfig):\n",
    "            optim_config = OmegaConf.create(optim_config)\n",
    "\n",
    "        # prepare the optimization config\n",
    "        self.prepare_optimization_config(optim_config)\n",
    "\n",
    "        # Setup optimizer and scheduler\n",
    "        self._optimizer = self.build_optimizer(self.param_dicts)\n",
    "        self._scheduler = self.build_lr_scheduler(self._optimizer)\n",
    "\n",
    "\n",
    "@patch\n",
    "def configure_optimizers(self: GaleTask):\n",
    "    \"\"\"\n",
    "    Choose what optimizers and learning-rate schedulers to use in your optimization.\n",
    "    See https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html\n",
    "    \"\"\"\n",
    "    # if self.setup_optimization() has been called manually no\n",
    "    # need to call again\n",
    "    if self._optimizer is noop and self._scheduler is noop:\n",
    "        self.setup_optimization()\n",
    "\n",
    "    if self._scheduler is None:\n",
    "        return self._optimizer\n",
    "    else:\n",
    "        return [self._optimizer], [self._scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.setup_optimization\" class=\"doc_header\"><code>GaleTask.setup_optimization</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.setup_optimization</code>(**`optim_config`**:`DictConfig`=*`None`*)\n",
       "\n",
       "Prepares an optimizer from a string name and its optional config parameters.\n",
       "\n",
       "Args:\n",
       "1. `optim_config`: A `dictionary`/`DictConfig` or instance of `OptimizationConfig`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.configure_optimizers\" class=\"doc_header\"><code>GaleTask.configure_optimizers</code><a href=\"__main__.py#L35\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.configure_optimizers</code>()\n",
       "\n",
       "Choose what optimizers and learning-rate schedulers to use in your optimization.\n",
       "See https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 55;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.setup_optimization)\\nshow_doc(GaleTask.configure_optimizers)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.setup_optimization)\\nshow_doc(GaleTask.configure_optimizers)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.setup_optimization)\n",
    "show_doc(GaleTask.configure_optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.utils.logger.ipynb.\n",
      "Converted 00a_core.utils.visualize.ipynb.\n",
      "Converted 00b_core.utils.structures.ipynb.\n",
      "Converted 01_core.nn.utils.ipynb.\n",
      "Converted 01a_core.nn.losses.ipynb.\n",
      "Converted 02_core.nn.optim.optimizers.ipynb.\n",
      "Converted 02a_core.nn.optim.lr_schedulers.ipynb.\n",
      "Converted 03_core.config.ipynb.\n",
      "Converted 03a_core.classes.ipynb.\n",
      "Converted 04_classification.modelling.backbones.ipynb.\n",
      "Converted 04a_classification.modelling.heads.ipynb.\n",
      "Converted 04b_classification.modelling.meta_arch.general.ipynb.\n",
      "Converted 04b_classification.modelling.meta_arch.vit.ipynb.\n",
      "Converted 05_collections.pandas.ipynb.\n",
      "Converted 06a_collections.callbacks.notebook.ipynb.\n",
      "Converted 06b_collections.callbacks.ema.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 56;\n",
       "                var nbb_unformatted_code = \"# hide\\nnotebook2script()\";\n",
       "                var nbb_formatted_code = \"# hide\\nnotebook2script()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gale",
   "language": "python",
   "name": "gale"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
