{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp collections.callbacks.ema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from nbdev.export import *\n",
    "from nbdev.export import Config\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Exponential Moving Average Callback for PyTorch Lightning\n",
    "> This is intended to allow functionality like https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import logging\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "from timm.utils.model import get_state_dict, unwrap_model\n",
    "from timm.utils.model_ema import ModelEmaV2\n",
    "\n",
    "from gale.core.utils.logger import log_main_process\n",
    "\n",
    "_logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class EMACallback(Callback):\n",
    "    \"\"\"\n",
    "    Model Exponential Moving Average. Empirically it has been found that using the moving average \n",
    "    of the trained parameters of a deep network is better than using its trained parameters directly.\n",
    "    \n",
    "    If `use_ema_weights`, then the ema parameters of the network is set after training end.\n",
    "    \"\"\"\n",
    "    def __init__(self, decay=0.9999, use_ema_weights: bool = True):\n",
    "        self.decay = decay\n",
    "        self.ema = None\n",
    "        self.use_ema_weights = use_ema_weights\n",
    "\n",
    "    def on_fit_start(self, trainer, pl_module):\n",
    "        \"Initialize `ModelEmaV2` from timm to keep a copy of the moving average of the weights\"\n",
    "        self.ema = ModelEmaV2(pl_module, decay=self.decay, device=None)\n",
    "\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "        \"Update the stored parameters using a moving average\"\n",
    "        # Update currently maintained parameters.\n",
    "        self.ema.update(pl_module)\n",
    "\n",
    "    def on_validation_epoch_start(self, trainer, pl_module):\n",
    "        \"do validation using the stored parameters\"\n",
    "        # save original parameters before replacing with EMA version\n",
    "        self.store(pl_module.parameters())\n",
    "\n",
    "        # update the LightningModule with the EMA weights\n",
    "        # ~ Copy EMA parameters to LightningModule\n",
    "        self.copy_to(self.ema.module.parameters(), pl_module.parameters())\n",
    "\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        \"Restore original parameters to resume training later\"\n",
    "        self.restore(pl_module.parameters())\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        # update the LightningModule with the EMA weights\n",
    "        if self.use_ema_weights:\n",
    "            self.copy_to(self.ema.module.parameters(), pl_module.parameters())\n",
    "            msg = \"Model weights replaced with the EMA version.\"\n",
    "            log_main_process(_logger, logging.INFO, msg)\n",
    "            \n",
    "    def on_save_checkpoint(self, trainer, pl_module, checkpoint):\n",
    "        if self.ema is not None:\n",
    "            return {\"state_dict_ema\": get_state_dict(self.ema, unwrap_model)}\n",
    "    \n",
    "    def on_load_checkpoint(self, callback_state):\n",
    "        if self.ema is not None:\n",
    "            self.ema.module.load_state_dict(callback_state[\"state_dict_ema\"])\n",
    "\n",
    "    def store(self, parameters):\n",
    "        \"Save the current parameters for restoring later.\"\n",
    "        self.collected_params = [param.clone() for param in parameters]       \n",
    "\n",
    "    def restore(self, parameters):\n",
    "        \"\"\"\n",
    "        Restore the parameters stored with the `store` method.\n",
    "        Useful to validate the model with EMA parameters without affecting the\n",
    "        original optimization process.\n",
    "        \"\"\"\n",
    "        for c_param, param in zip(self.collected_params, parameters):\n",
    "            param.data.copy_(c_param.data)\n",
    "\n",
    "    def copy_to(self, shadow_parameters, parameters):\n",
    "        \"Copy current parameters into given collection of parameters.\"\n",
    "        for s_param, param in zip(shadow_parameters, parameters):\n",
    "            if param.requires_grad:\n",
    "                param.data.copy_(s_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"EMACallback\" class=\"doc_header\"><code>class</code> <code>EMACallback</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>EMACallback</code>(**`decay`**=*`0.9999`*, **`use_ema_weights`**:`bool`=*`True`*) :: `Callback`\n",
       "\n",
       "Model Exponential Moving Average. Empirically it has been found that using the moving average \n",
       "of the trained parameters of a deep network is better than using its trained parameters directly.\n",
       "\n",
       "If `use_ema_weights`, then the ema parameters of the network is set after training end."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(EMACallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.utils.logger.ipynb.\n",
      "Converted 00a_core.utils.visualize.ipynb.\n",
      "Converted 00b_core.utils.structures.ipynb.\n",
      "Converted 01_core.nn.utils.ipynb.\n",
      "Converted 01a_core.nn.losses.ipynb.\n",
      "Converted 02_core.nn.optim.optimizers.ipynb.\n",
      "Converted 02a_core.nn.optim.lr_schedulers.ipynb.\n",
      "Converted 03_core.config.ipynb.\n",
      "Converted 03a_core.classes.ipynb.\n",
      "Converted 04_classification.modelling.backbones.ipynb.\n",
      "Converted 04a_classification.modelling.heads.ipynb.\n",
      "Converted 04b_classification.modelling.meta_arch.image_classifier.ipynb.\n",
      "Converted 05_collections.pandas.ipynb.\n",
      "Converted 06a_collections.callbacks.notebook.ipynb.\n",
      "Converted 06b_collections.callbacks.ema.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gale",
   "language": "python",
   "name": "gale"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
