{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core.nn.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# hide\\nimport warnings\\n\\nfrom nbdev.export import *\\nfrom nbdev.showdoc import *\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\";\n",
       "                var nbb_formatted_code = \"# hide\\nimport warnings\\n\\nfrom nbdev.export import *\\nfrom nbdev.showdoc import *\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "import warnings\n",
    "\n",
    "from nbdev.export import *\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "> Custom loss functions in `Gale`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# export\\nfrom typing import *\\n\\nimport torch\\nimport torch.nn.functional as F\\nfrom fastcore.all import store_attr\\nfrom fvcore.nn import sigmoid_focal_loss\\nfrom torch import Tensor, nn\\n\\nfrom gale.core.nn.utils import maybe_convert_to_onehot\";\n",
       "                var nbb_formatted_code = \"# export\\nfrom typing import *\\n\\nimport torch\\nimport torch.nn.functional as F\\nfrom fastcore.all import store_attr\\nfrom fvcore.nn import sigmoid_focal_loss\\nfrom torch import Tensor, nn\\n\\nfrom gale.core.nn.utils import maybe_convert_to_onehot\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "from typing import *\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from fastcore.all import store_attr\n",
    "from fvcore.nn import sigmoid_focal_loss\n",
    "from torch import Tensor, nn\n",
    "\n",
    "from gale.core.nn.utils import maybe_convert_to_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"from fastcore.test import *\";\n",
       "                var nbb_formatted_code = \"from fastcore.test import *\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# export\\nclass LabelSmoothingCrossEntropy(nn.Module):\\n    \\\"Cross Entropy Loss with Label Smoothing\\\"\\n\\n    def __init__(\\n        self, eps: float = 0.1, reduction: str = \\\"mean\\\", weight: Optional[Tensor] = None\\n    ):\\n        super(LabelSmoothingCrossEntropy, self).__init__()\\n        store_attr(\\\"eps, reduction, weight\\\")\\n\\n    def forward(self, input: Tensor, target: Tensor):\\n        \\\"\\\"\\\"\\n        Shape:\\n        - Input  : $(N,C)$ where $N$ is the mini-batch size and $C$ is the total number of classes\\n        - Target : $(N)$ where each value is $0 \\\\leq {targets}[i] \\\\leq C-10\\u2264targets[i]\\u2264C\\u22121$\\n        - Output: scalar. If `reduction` is `none`, then $(N, *)$ , same shape as input.\\n        \\\"\\\"\\\"\\n        c = input.size()[1]\\n        log_preds = F.log_softmax(input, dim=1)\\n        if self.reduction == \\\"sum\\\":\\n            loss = -log_preds.sum()\\n        else:\\n            loss = -log_preds.sum(dim=1)\\n            if self.reduction == \\\"mean\\\":\\n                loss = loss.mean()\\n        loss = loss * self.eps / c + (1 - self.eps) * F.nll_loss(\\n            log_preds, target.long(), weight=self.weight, reduction=self.reduction\\n        )\\n        return loss\";\n",
       "                var nbb_formatted_code = \"# export\\nclass LabelSmoothingCrossEntropy(nn.Module):\\n    \\\"Cross Entropy Loss with Label Smoothing\\\"\\n\\n    def __init__(\\n        self, eps: float = 0.1, reduction: str = \\\"mean\\\", weight: Optional[Tensor] = None\\n    ):\\n        super(LabelSmoothingCrossEntropy, self).__init__()\\n        store_attr(\\\"eps, reduction, weight\\\")\\n\\n    def forward(self, input: Tensor, target: Tensor):\\n        \\\"\\\"\\\"\\n        Shape:\\n        - Input  : $(N,C)$ where $N$ is the mini-batch size and $C$ is the total number of classes\\n        - Target : $(N)$ where each value is $0 \\\\leq {targets}[i] \\\\leq C-10\\u2264targets[i]\\u2264C\\u22121$\\n        - Output: scalar. If `reduction` is `none`, then $(N, *)$ , same shape as input.\\n        \\\"\\\"\\\"\\n        c = input.size()[1]\\n        log_preds = F.log_softmax(input, dim=1)\\n        if self.reduction == \\\"sum\\\":\\n            loss = -log_preds.sum()\\n        else:\\n            loss = -log_preds.sum(dim=1)\\n            if self.reduction == \\\"mean\\\":\\n                loss = loss.mean()\\n        loss = loss * self.eps / c + (1 - self.eps) * F.nll_loss(\\n            log_preds, target.long(), weight=self.weight, reduction=self.reduction\\n        )\\n        return loss\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"Cross Entropy Loss with Label Smoothing\"\n",
    "\n",
    "    def __init__(\n",
    "        self, eps: float = 0.1, reduction: str = \"mean\", weight: Optional[Tensor] = None\n",
    "    ):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        store_attr(\"eps, reduction, weight\")\n",
    "\n",
    "    def forward(self, input: Tensor, target: Tensor):\n",
    "        \"\"\"\n",
    "        Shape:\n",
    "        - Input  : $(N,C)$ where $N$ is the mini-batch size and $C$ is the total number of classes\n",
    "        - Target : $(N)$ where each value is $0 \\leq {targets}[i] \\leq C-10≤targets[i]≤C−1$\n",
    "        - Output: scalar. If `reduction` is `none`, then $(N, *)$ , same shape as input.\n",
    "        \"\"\"\n",
    "        c = input.size()[1]\n",
    "        log_preds = F.log_softmax(input, dim=1)\n",
    "        if self.reduction == \"sum\":\n",
    "            loss = -log_preds.sum()\n",
    "        else:\n",
    "            loss = -log_preds.sum(dim=1)\n",
    "            if self.reduction == \"mean\":\n",
    "                loss = loss.mean()\n",
    "        loss = loss * self.eps / c + (1 - self.eps) * F.nll_loss(\n",
    "            log_preds, target.long(), weight=self.weight, reduction=self.reduction\n",
    "        )\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"LabelSmoothingCrossEntropy.forward\" class=\"doc_header\"><code>LabelSmoothingCrossEntropy.forward</code><a href=\"__main__.py#L11\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>LabelSmoothingCrossEntropy.forward</code>(**`input`**:`Tensor`, **`target`**:`Tensor`)\n",
       "\n",
       "Shape:\n",
       "- Input  : $(N,C)$ where $N$ is the mini-batch size and $C$ is the total number of classes\n",
       "- Target : $(N)$ where each value is $0 \\leq {targets}[i] \\leq C-10≤targets[i]≤C−1$\n",
       "- Output: scalar. If `reduction` is `none`, then $(N, *)$ , same shape as input."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"show_doc(LabelSmoothingCrossEntropy.forward)\";\n",
       "                var nbb_formatted_code = \"show_doc(LabelSmoothingCrossEntropy.forward)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(LabelSmoothingCrossEntropy.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"criterion = LabelSmoothingCrossEntropy(reduction=\\\"mean\\\")\\n\\noutput = torch.randn(32, 5, requires_grad=True)\\ntarget = torch.empty(32, dtype=torch.long).random_(5)\\n\\nloss = criterion(output, target)\";\n",
       "                var nbb_formatted_code = \"criterion = LabelSmoothingCrossEntropy(reduction=\\\"mean\\\")\\n\\noutput = torch.randn(32, 5, requires_grad=True)\\ntarget = torch.empty(32, dtype=torch.long).random_(5)\\n\\nloss = criterion(output, target)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "criterion = LabelSmoothingCrossEntropy(reduction=\"mean\")\n",
    "\n",
    "output = torch.randn(32, 5, requires_grad=True)\n",
    "target = torch.empty(32, dtype=torch.long).random_(5)\n",
    "\n",
    "loss = criterion(output, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Focal Loss](https://arxiv.org/pdf/1708.02002.pdf) is the same as cross entropy except easy-to-classify observations are down-weighted in the loss calculation. The strength of down-weighting is proportional to the size of the gamma parameter. Put another way, the larger gamma the less the easy-to-classify observations contribute to the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"# export\\nclass BinarySigmoidFocalLoss(nn.Module):\\n    \\\"\\\"\\\"\\n    Creates a criterion that computes the focal loss between binary `input` and `target`.\\n    Focal Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\\n\\n    Source: https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/focal_loss.py\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        alpha: float = -1,\\n        gamma: float = 2,\\n        reduction: str = \\\"mean\\\",\\n    ):\\n        super(BinarySigmoidFocalLoss, self).__init__()\\n        store_attr(\\\"alpha, gamma, reduction\\\")\\n\\n    def forward(self, input: Tensor, target: Tensor):\\n        \\\"\\\"\\\"\\n        Shape:\\n        - Input: : $(N, *)$ where $*$ means, any number of additional dimensions.\\n        - Target: : $(N, *)$, same shape as the input.\\n        - Output: scalar. If `reduction` is 'none', then $(N, *)$ , same shape as input.\\n        \\\"\\\"\\\"\\n        loss = sigmoid_focal_loss(input, target, self.gamma, self.alpha, self.reduction)\\n        return loss\";\n",
       "                var nbb_formatted_code = \"# export\\nclass BinarySigmoidFocalLoss(nn.Module):\\n    \\\"\\\"\\\"\\n    Creates a criterion that computes the focal loss between binary `input` and `target`.\\n    Focal Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\\n\\n    Source: https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/focal_loss.py\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        alpha: float = -1,\\n        gamma: float = 2,\\n        reduction: str = \\\"mean\\\",\\n    ):\\n        super(BinarySigmoidFocalLoss, self).__init__()\\n        store_attr(\\\"alpha, gamma, reduction\\\")\\n\\n    def forward(self, input: Tensor, target: Tensor):\\n        \\\"\\\"\\\"\\n        Shape:\\n        - Input: : $(N, *)$ where $*$ means, any number of additional dimensions.\\n        - Target: : $(N, *)$, same shape as the input.\\n        - Output: scalar. If `reduction` is 'none', then $(N, *)$ , same shape as input.\\n        \\\"\\\"\\\"\\n        loss = sigmoid_focal_loss(input, target, self.gamma, self.alpha, self.reduction)\\n        return loss\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class BinarySigmoidFocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates a criterion that computes the focal loss between binary `input` and `target`.\n",
    "    Focal Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n",
    "\n",
    "    Source: https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/focal_loss.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha: float = -1,\n",
    "        gamma: float = 2,\n",
    "        reduction: str = \"mean\",\n",
    "    ):\n",
    "        super(BinarySigmoidFocalLoss, self).__init__()\n",
    "        store_attr(\"alpha, gamma, reduction\")\n",
    "\n",
    "    def forward(self, input: Tensor, target: Tensor):\n",
    "        \"\"\"\n",
    "        Shape:\n",
    "        - Input: : $(N, *)$ where $*$ means, any number of additional dimensions.\n",
    "        - Target: : $(N, *)$, same shape as the input.\n",
    "        - Output: scalar. If `reduction` is 'none', then $(N, *)$ , same shape as input.\n",
    "        \"\"\"\n",
    "        loss = sigmoid_focal_loss(input, target, self.gamma, self.alpha, self.reduction)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"BinarySigmoidFocalLoss.forward\" class=\"doc_header\"><code>BinarySigmoidFocalLoss.forward</code><a href=\"__main__.py#L19\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>BinarySigmoidFocalLoss.forward</code>(**`input`**:`Tensor`, **`target`**:`Tensor`)\n",
       "\n",
       "Shape:\n",
       "- Input: : $(N, *)$ where $*$ means, any number of additional dimensions.\n",
       "- Target: : $(N, *)$, same shape as the input.\n",
       "- Output: scalar. If `reduction` is 'none', then $(N, *)$ , same shape as input."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"show_doc(BinarySigmoidFocalLoss.forward)\";\n",
       "                var nbb_formatted_code = \"show_doc(BinarySigmoidFocalLoss.forward)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BinarySigmoidFocalLoss.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"criterion = BinarySigmoidFocalLoss(reduction=\\\"mean\\\")\\n\\ntarget = torch.ones([10, 64], dtype=torch.float32)\\noutput = torch.full([10, 64], 1.5)\\n\\nloss = criterion(output, target)\";\n",
       "                var nbb_formatted_code = \"criterion = BinarySigmoidFocalLoss(reduction=\\\"mean\\\")\\n\\ntarget = torch.ones([10, 64], dtype=torch.float32)\\noutput = torch.full([10, 64], 1.5)\\n\\nloss = criterion(output, target)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "criterion = BinarySigmoidFocalLoss(reduction=\"mean\")\n",
    "\n",
    "target = torch.ones([10, 64], dtype=torch.float32)\n",
    "output = torch.full([10, 64], 1.5)\n",
    "\n",
    "loss = criterion(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"# export\\nclass FocalLoss(nn.Module):\\n    \\\"\\\"\\\"\\n    Same as `nn.CrossEntropyLoss` but with focal paramter, `gamma`.\\n    Focal Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\\n\\n    Source: https://kornia.readthedocs.io/en/latest/_modules/kornia/losses/focal.html\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        alpha: float = 1,\\n        gamma: float = 2,\\n        reduction: str = \\\"mean\\\",\\n        eps: float = 1e-8,\\n    ):\\n\\n        super(FocalLoss, self).__init__()\\n        store_attr(\\\"alpha, gamma, reduction, eps\\\")\\n\\n    def forward(self, input: Tensor, target: Tensor):\\n        \\\"\\\"\\\"\\n        Shape:\\n        - Input  : $(N,C)$ where $N$ is the mini-batch size and $C$ is the total number of classes\\n        - Target : $(N)$ where each value is $0 \\\\leq {targets}[i] \\\\leq C-10\\u2264targets[i]\\u2264C\\u22121$\\n        \\\"\\\"\\\"\\n        if not len(input.shape) >= 2:\\n            raise ValueError(\\n                \\\"Invalid input shape, we expect BxCx*. Got: {}\\\".format(input.shape)\\n            )\\n\\n        if input.size(0) != target.size(0):\\n            raise ValueError(\\n                \\\"Expected input batch_size ({}) to match target batch_size ({}).\\\".format(\\n                    input.size(0), target.size(0)\\n                )\\n            )\\n\\n        n = input.size(0)\\n\\n        # compute softmax over the classes axis\\n        softmax_inputs: Tensor = F.softmax(input, dim=1) + self.eps\\n\\n        # create the labels one hot tensor\\n        one_hot_targs: Tensor = maybe_convert_to_onehot(target, softmax_inputs)\\n\\n        # compute the actual focal loss\\n        focal_weight = torch.pow(-softmax_inputs + 1.0, self.gamma)\\n        focal_factor = -self.alpha * focal_weight * torch.log(softmax_inputs)\\n\\n        loss = torch.sum(one_hot_targs * focal_factor, dim=1)\\n\\n        if self.reduction == \\\"none\\\":\\n            loss = loss\\n        elif self.reduction == \\\"mean\\\":\\n            loss = torch.mean(loss)\\n        elif self.reduction == \\\"sum\\\":\\n            loss = torch.sum(loss)\\n        else:\\n            raise NotImplementedError(\\\"Invalid reduction mode: {}\\\".format(self.reduction))\\n        return loss\";\n",
       "                var nbb_formatted_code = \"# export\\nclass FocalLoss(nn.Module):\\n    \\\"\\\"\\\"\\n    Same as `nn.CrossEntropyLoss` but with focal paramter, `gamma`.\\n    Focal Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\\n\\n    Source: https://kornia.readthedocs.io/en/latest/_modules/kornia/losses/focal.html\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        alpha: float = 1,\\n        gamma: float = 2,\\n        reduction: str = \\\"mean\\\",\\n        eps: float = 1e-8,\\n    ):\\n\\n        super(FocalLoss, self).__init__()\\n        store_attr(\\\"alpha, gamma, reduction, eps\\\")\\n\\n    def forward(self, input: Tensor, target: Tensor):\\n        \\\"\\\"\\\"\\n        Shape:\\n        - Input  : $(N,C)$ where $N$ is the mini-batch size and $C$ is the total number of classes\\n        - Target : $(N)$ where each value is $0 \\\\leq {targets}[i] \\\\leq C-10\\u2264targets[i]\\u2264C\\u22121$\\n        \\\"\\\"\\\"\\n        if not len(input.shape) >= 2:\\n            raise ValueError(\\n                \\\"Invalid input shape, we expect BxCx*. Got: {}\\\".format(input.shape)\\n            )\\n\\n        if input.size(0) != target.size(0):\\n            raise ValueError(\\n                \\\"Expected input batch_size ({}) to match target batch_size ({}).\\\".format(\\n                    input.size(0), target.size(0)\\n                )\\n            )\\n\\n        n = input.size(0)\\n\\n        # compute softmax over the classes axis\\n        softmax_inputs: Tensor = F.softmax(input, dim=1) + self.eps\\n\\n        # create the labels one hot tensor\\n        one_hot_targs: Tensor = maybe_convert_to_onehot(target, softmax_inputs)\\n\\n        # compute the actual focal loss\\n        focal_weight = torch.pow(-softmax_inputs + 1.0, self.gamma)\\n        focal_factor = -self.alpha * focal_weight * torch.log(softmax_inputs)\\n\\n        loss = torch.sum(one_hot_targs * focal_factor, dim=1)\\n\\n        if self.reduction == \\\"none\\\":\\n            loss = loss\\n        elif self.reduction == \\\"mean\\\":\\n            loss = torch.mean(loss)\\n        elif self.reduction == \\\"sum\\\":\\n            loss = torch.sum(loss)\\n        else:\\n            raise NotImplementedError(\\n                \\\"Invalid reduction mode: {}\\\".format(self.reduction)\\n            )\\n        return loss\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Same as `nn.CrossEntropyLoss` but with focal paramter, `gamma`.\n",
    "    Focal Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n",
    "\n",
    "    Source: https://kornia.readthedocs.io/en/latest/_modules/kornia/losses/focal.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha: float = 1,\n",
    "        gamma: float = 2,\n",
    "        reduction: str = \"mean\",\n",
    "        eps: float = 1e-8,\n",
    "    ):\n",
    "\n",
    "        super(FocalLoss, self).__init__()\n",
    "        store_attr(\"alpha, gamma, reduction, eps\")\n",
    "\n",
    "    def forward(self, input: Tensor, target: Tensor):\n",
    "        \"\"\"\n",
    "        Shape:\n",
    "        - Input  : $(N,C)$ where $N$ is the mini-batch size and $C$ is the total number of classes\n",
    "        - Target : $(N)$ where each value is $0 \\leq {targets}[i] \\leq C-10≤targets[i]≤C−1$\n",
    "        \"\"\"\n",
    "        if not len(input.shape) >= 2:\n",
    "            raise ValueError(\n",
    "                \"Invalid input shape, we expect BxCx*. Got: {}\".format(input.shape)\n",
    "            )\n",
    "\n",
    "        if input.size(0) != target.size(0):\n",
    "            raise ValueError(\n",
    "                \"Expected input batch_size ({}) to match target batch_size ({}).\".format(\n",
    "                    input.size(0), target.size(0)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        n = input.size(0)\n",
    "\n",
    "        # compute softmax over the classes axis\n",
    "        softmax_inputs: Tensor = F.softmax(input, dim=1) + self.eps\n",
    "\n",
    "        # create the labels one hot tensor\n",
    "        one_hot_targs: Tensor = maybe_convert_to_onehot(target, softmax_inputs)\n",
    "\n",
    "        # compute the actual focal loss\n",
    "        focal_weight = torch.pow(-softmax_inputs + 1.0, self.gamma)\n",
    "        focal_factor = -self.alpha * focal_weight * torch.log(softmax_inputs)\n",
    "\n",
    "        loss = torch.sum(one_hot_targs * focal_factor, dim=1)\n",
    "\n",
    "        if self.reduction == \"none\":\n",
    "            loss = loss\n",
    "        elif self.reduction == \"mean\":\n",
    "            loss = torch.mean(loss)\n",
    "        elif self.reduction == \"sum\":\n",
    "            loss = torch.sum(loss)\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"Invalid reduction mode: {}\".format(self.reduction)\n",
    "            )\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments to `FocalLoss`:\n",
    "- `alpha` (float): Weighting factor $\\alpha$ in `[0, 1]`.\n",
    "- `gamma` (float, optional): Focusing parameter $\\gamma$ >= 0. Default 2.\n",
    "- `reduction` (str, optional): Specifies the reduction to apply to the\n",
    "- `output`: `none` | `mean` | `sum`. \n",
    "  * `none`: no reduction will be applied,\n",
    "  * `mean`: the sum of the output will be divided by the number of elements in the output \n",
    "  * `sum`: the output will be summed. \n",
    "  * Default: `none`.\n",
    "- `eps` (float, optional): Scalar to enforce numerical stabiliy. Default: 1e-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"FocalLoss.forward\" class=\"doc_header\"><code>FocalLoss.forward</code><a href=\"__main__.py#L21\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>FocalLoss.forward</code>(**`input`**:`Tensor`, **`target`**:`Tensor`)\n",
       "\n",
       "Shape:\n",
       "- Input  : $(N,C)$ where $N$ is the mini-batch size and $C$ is the total number of classes\n",
       "- Target : $(N)$ where each value is $0 \\leq {targets}[i] \\leq C-10≤targets[i]≤C−1$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"show_doc(FocalLoss.forward)\";\n",
       "                var nbb_formatted_code = \"show_doc(FocalLoss.forward)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(FocalLoss.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"criterion = FocalLoss(alpha=0.5, gamma=2.0, reduction=\\\"mean\\\")\\n\\nN = 5  # num_classes\\ninput = torch.randn(32, N, requires_grad=True)\\ntarget = torch.empty(32, dtype=torch.long).random_(N)\\nloss = criterion(input, target)\\n\\n\\n# Compare focal loss with gamma = 0 ,cross entropy\\nfl = FocalLoss(gamma=0, reduction=\\\"mean\\\")\\nce = nn.CrossEntropyLoss(reduction=\\\"mean\\\")\\noutput = torch.randn(32, N, requires_grad=True)\\ntarget = torch.empty(32, dtype=torch.long).random_(N)\\ntest_close(fl(output, target), ce(output, target))\\n\\n# Test focal loss with gamma > 0 is different than cross entropy\\nfl = FocalLoss(gamma=2)\\nwith torch.no_grad():\\n    test_ne(fl(output, target), ce(output, target))\";\n",
       "                var nbb_formatted_code = \"criterion = FocalLoss(alpha=0.5, gamma=2.0, reduction=\\\"mean\\\")\\n\\nN = 5  # num_classes\\ninput = torch.randn(32, N, requires_grad=True)\\ntarget = torch.empty(32, dtype=torch.long).random_(N)\\nloss = criterion(input, target)\\n\\n\\n# Compare focal loss with gamma = 0 ,cross entropy\\nfl = FocalLoss(gamma=0, reduction=\\\"mean\\\")\\nce = nn.CrossEntropyLoss(reduction=\\\"mean\\\")\\noutput = torch.randn(32, N, requires_grad=True)\\ntarget = torch.empty(32, dtype=torch.long).random_(N)\\ntest_close(fl(output, target), ce(output, target))\\n\\n# Test focal loss with gamma > 0 is different than cross entropy\\nfl = FocalLoss(gamma=2)\\nwith torch.no_grad():\\n    test_ne(fl(output, target), ce(output, target))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "criterion = FocalLoss(alpha=0.5, gamma=2.0, reduction=\"mean\")\n",
    "\n",
    "N = 5  # num_classes\n",
    "input = torch.randn(32, N, requires_grad=True)\n",
    "target = torch.empty(32, dtype=torch.long).random_(N)\n",
    "loss = criterion(input, target)\n",
    "\n",
    "\n",
    "# Compare focal loss with gamma = 0 ,cross entropy\n",
    "fl = FocalLoss(gamma=0, reduction=\"mean\")\n",
    "ce = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "output = torch.randn(32, N, requires_grad=True)\n",
    "target = torch.empty(32, dtype=torch.long).random_(N)\n",
    "test_close(fl(output, target), ce(output, target))\n",
    "\n",
    "# Test focal loss with gamma > 0 is different than cross entropy\n",
    "fl = FocalLoss(gamma=2)\n",
    "with torch.no_grad():\n",
    "    test_ne(fl(output, target), ce(output, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.logging.ipynb.\n",
      "Converted 00a_core.structures.ipynb.\n",
      "Converted 00b_core.visualize.ipynb.\n",
      "Converted 01_core.nn.utils.ipynb.\n",
      "Converted 01a_core.nn.losses.ipynb.\n",
      "Converted 01b_core.nn.optim.optimizers.ipynb.\n",
      "Converted 01c_core.nn.optim.lr_schedulers.ipynb.\n",
      "Converted 03_collections.pandas.ipynb.\n",
      "Converted 03a_collections.callbacks.notebook.ipynb.\n",
      "Converted 03b_collections.callbacks.ema.ipynb.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"notebook2script()\";\n",
       "                var nbb_formatted_code = \"notebook2script()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gale",
   "language": "python",
   "name": "gale"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
