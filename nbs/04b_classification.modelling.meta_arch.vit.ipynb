{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# skip\n",
    "!git clone https://github.com/benihime91/gale # install gale on colab\n",
    "!pip install -e \"gale[dev]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp classification.modelling.meta_arch.vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# hide\\n%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\\n%matplotlib inline\";\n",
       "                var nbb_formatted_code = \"# hide\\n%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\\n%matplotlib inline\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "%load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# hide\\nimport warnings\\n\\nfrom nbdev.export import *\\nfrom nbdev.showdoc import *\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\";\n",
       "                var nbb_formatted_code = \"# hide\\nimport warnings\\n\\nfrom nbdev.export import *\\nfrom nbdev.showdoc import *\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "import warnings\n",
    "\n",
    "from nbdev.export import *\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Architectures : Vision Transformer (ViT) \n",
    "> Pretrained Vision Transformers modified for use in gale from timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# export\\nimport logging\\nfrom typing import *\\n\\nimport torch\\nfrom fastcore.all import store_attr, use_kwargs_dict\\nfrom omegaconf import DictConfig, OmegaConf\\nfrom pytorch_lightning.core.memory import get_human_readable_count\\nfrom timm import create_model\\nfrom timm.models.vision_transformer import VisionTransformer\\nfrom timm.optim.optim_factory import add_weight_decay\\n\\nfrom gale.core.classes import GaleModule\\nfrom gale.core.nn.activations import ACTIVATION_REGISTRY\\nfrom gale.core.nn.shape_spec import ShapeSpec\\nfrom gale.core.nn.utils import trainable_params\\n\\n_logger = logging.getLogger(__name__)\";\n",
       "                var nbb_formatted_code = \"# export\\nimport logging\\nfrom typing import *\\n\\nimport torch\\nfrom fastcore.all import store_attr, use_kwargs_dict\\nfrom omegaconf import DictConfig, OmegaConf\\nfrom pytorch_lightning.core.memory import get_human_readable_count\\nfrom timm import create_model\\nfrom timm.models.vision_transformer import VisionTransformer\\nfrom timm.optim.optim_factory import add_weight_decay\\n\\nfrom gale.core.classes import GaleModule\\nfrom gale.core.nn.activations import ACTIVATION_REGISTRY\\nfrom gale.core.nn.shape_spec import ShapeSpec\\nfrom gale.core.nn.utils import trainable_params\\n\\n_logger = logging.getLogger(__name__)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "import logging\n",
    "from typing import *\n",
    "\n",
    "import torch\n",
    "from fastcore.all import store_attr, use_kwargs_dict\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from pytorch_lightning.core.memory import get_human_readable_count\n",
    "from timm import create_model\n",
    "from timm.models.vision_transformer import VisionTransformer\n",
    "from timm.optim.optim_factory import add_weight_decay\n",
    "\n",
    "from gale.core.classes import GaleModule\n",
    "from gale.core.nn.activations import ACTIVATION_REGISTRY\n",
    "from gale.core.nn.shape_spec import ShapeSpec\n",
    "from gale.core.nn.utils import trainable_params\n",
    "\n",
    "_logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# export\\n# @TODO: Add support for Discriminative Lr's\\nclass ViT(GaleModule):\\n    \\\"\\\"\\\"\\n    A interface to create a Vision Transformer from timm. For available model check :\\n    https://github.com/rwightman/pytorch-image-models/timm/models/vision_transformer.py\\n    \\\"\\\"\\\"\\n\\n    @use_kwargs_dict(\\n        keep=True,\\n        num_classes=1000,\\n        drop_rate=0.0,\\n        attn_drop_rate=0.0,\\n        drop_path_rate=0.0,\\n    )\\n    def __init__(\\n        self,\\n        model_name: str,\\n        input_shape: ShapeSpec,\\n        lr: float = 1e-03,\\n        wd: float = 1e-05,\\n        pretrained: bool = True,\\n        freeze_to: Optional[int] = None,\\n        finetune: Optional[bool] = None,\\n        act: Optional[str] = None,\\n        reset_classifier: bool = True,\\n        filter_wd: bool = True,\\n        **kwargs,\\n    ):\\n        \\\"\\\"\\\"\\n        Arguments:\\n        1. `input_shape` (ShapeSpec): input image shape. For ViT `height=width` and check the above link for avilable model shapes.\\n        2. `model_name` (str): name of the ViT model, check the above link for avilable models.\\n        3. `pretrained` (bool): load weights pretrained on imagenet.\\n        4. `act` (str): name of the activation layer. Must be registerd in `ACTIVATION_REGISTRY`\\n        5. `num_classes` (int): num output classes.\\n        6. `drop_rate` (float): dropout rate.\\n        7. `attn_drop_rate` (float): attention dropout rate.\\n        8. `drop_path_rate` (float): stochastic depth rate.\\n        9. `reset_classifier` (bool): resets the weights of the classifier.\\n        10. `freeze_to` (int): Freeze the param meter groups of the model upto n.\\n        11. `finetune` (bool): Freeze all the layers and keep only the `classifier` trainable.\\n        \\\"\\\"\\\"\\n        super(ViT, self).__init__()\\n        # create model from timm\\n        assert input_shape.height == input_shape.width\\n        in_chans = input_shape.channels\\n\\n        if act is not None:\\n            act = ACTIVATION_REGISTRY.get(act)\\n        # fmt: off\\n        self.model: VisionTransformer = create_model(model_name, pretrained, in_chans=in_chans, act=act, **kwargs)\\n        # fmt: on\\n        assert isinstance(self.model, VisionTransformer)\\n\\n        if reset_classifier:\\n            num_cls = kwargs.pop(\\\"num_classes\\\")\\n            self.model.reset_classifier(num_cls)\\n\\n        if freeze_to is not None:\\n            self.freeze_to(freeze_to)\\n\\n        if finetune:\\n            if freeze_to is not None and isinstance(freeze_to, int):\\n                msg = \\\"You have sprecified freeze_to along with finetune\\\"\\n                _logger.warning(msg)\\n            _logger.info(\\\"Freezing all the model parameters except for the classifier\\\")\\n            self.freeze()\\n\\n            classifier = [\\\"head\\\", \\\"head_dist\\\"]\\n\\n            for name, module in self.model.named_children():\\n                if name in classifier:\\n                    for p in module.parameters():\\n                        p.requires_grad_(True)\\n\\n        store_attr(\\\"wd, lr, filter_wd\\\")\\n\\n    def forward(self, batched_inputs: torch.Tensor) -> torch.Tensor:\\n        \\\"\\\"\\\"\\n        Runs the batched_inputs through the created model.\\n        \\\"\\\"\\\"\\n        out = self.model(batched_inputs)\\n        return out\\n\\n    @classmethod\\n    def from_config_dict(cls, cfg: DictConfig):\\n        \\\"\\\"\\\"\\n        Instantiate the Meta Architecture from gale config\\n        \\\"\\\"\\\"\\n        # fmt: off\\n        input_shape = ShapeSpec(cfg.input.channels, cfg.input.height, cfg.input.width)\\n        _logger.debug(f\\\"Inputs: {input_shape}\\\")\\n        instance = super().from_config_dict(cfg.model.meta_architecture.init_args, input_shape=input_shape)\\n        param_count = get_human_readable_count(sum([m.numel() for m in instance.parameters()]))\\n        _logger.debug('{} created, param count: {}.'.format(cfg.model.meta_architecture.init_args.model_name, param_count))\\n        # fmt: on\\n        return instance\\n\\n    def build_param_dicts(self):\\n        \\\"\\\"\\\"\\n        Builds up the Paramters dicts for optimization.\\n        \\\"\\\"\\\"\\n        if self.filter_wd:\\n            param_lists = add_weight_decay(\\n                self.model,\\n                weight_decay=self.wd,\\n                skip_list=self.model.no_weight_decay(),\\n            )\\n            param_lists[0][\\\"lr\\\"] = self.lr\\n            param_lists[1][\\\"lr\\\"] = self.lr\\n        else:\\n            ps = trainable_params(self.model)\\n            param_lists = dict(params=ps, lr=self.lr, wd=self.wd)\\n        return param_lists\\n\\n    def get_lrs(self) -> List:\\n        \\\"\\\"\\\"\\n        Returns a List containining the Lrs' for\\n        each parameter group. This is required to build schedulers\\n        like `torch.optim.lr_scheduler.OneCycleScheduler` which needs\\n        the max lrs' for all the Param Groups.\\n        \\\"\\\"\\\"\\n        lrs = []\\n\\n        for p in self.build_param_dicts():\\n            lrs.append(p[\\\"lr\\\"])\\n        return lrs\";\n",
       "                var nbb_formatted_code = \"# export\\n# @TODO: Add support for Discriminative Lr's\\nclass ViT(GaleModule):\\n    \\\"\\\"\\\"\\n    A interface to create a Vision Transformer from timm. For available model check :\\n    https://github.com/rwightman/pytorch-image-models/timm/models/vision_transformer.py\\n    \\\"\\\"\\\"\\n\\n    @use_kwargs_dict(\\n        keep=True,\\n        num_classes=1000,\\n        drop_rate=0.0,\\n        attn_drop_rate=0.0,\\n        drop_path_rate=0.0,\\n    )\\n    def __init__(\\n        self,\\n        model_name: str,\\n        input_shape: ShapeSpec,\\n        lr: float = 1e-03,\\n        wd: float = 1e-05,\\n        pretrained: bool = True,\\n        freeze_to: Optional[int] = None,\\n        finetune: Optional[bool] = None,\\n        act: Optional[str] = None,\\n        reset_classifier: bool = True,\\n        filter_wd: bool = True,\\n        **kwargs,\\n    ):\\n        \\\"\\\"\\\"\\n        Arguments:\\n        1. `input_shape` (ShapeSpec): input image shape. For ViT `height=width` and check the above link for avilable model shapes.\\n        2. `model_name` (str): name of the ViT model, check the above link for avilable models.\\n        3. `pretrained` (bool): load weights pretrained on imagenet.\\n        4. `act` (str): name of the activation layer. Must be registerd in `ACTIVATION_REGISTRY`\\n        5. `num_classes` (int): num output classes.\\n        6. `drop_rate` (float): dropout rate.\\n        7. `attn_drop_rate` (float): attention dropout rate.\\n        8. `drop_path_rate` (float): stochastic depth rate.\\n        9. `reset_classifier` (bool): resets the weights of the classifier.\\n        10. `freeze_to` (int): Freeze the param meter groups of the model upto n.\\n        11. `finetune` (bool): Freeze all the layers and keep only the `classifier` trainable.\\n        \\\"\\\"\\\"\\n        super(ViT, self).__init__()\\n        # create model from timm\\n        assert input_shape.height == input_shape.width\\n        in_chans = input_shape.channels\\n\\n        if act is not None:\\n            act = ACTIVATION_REGISTRY.get(act)\\n        # fmt: off\\n        self.model: VisionTransformer = create_model(model_name, pretrained, in_chans=in_chans, act=act, **kwargs)\\n        # fmt: on\\n        assert isinstance(self.model, VisionTransformer)\\n\\n        if reset_classifier:\\n            num_cls = kwargs.pop(\\\"num_classes\\\")\\n            self.model.reset_classifier(num_cls)\\n\\n        if freeze_to is not None:\\n            self.freeze_to(freeze_to)\\n\\n        if finetune:\\n            if freeze_to is not None and isinstance(freeze_to, int):\\n                msg = \\\"You have sprecified freeze_to along with finetune\\\"\\n                _logger.warning(msg)\\n            _logger.info(\\\"Freezing all the model parameters except for the classifier\\\")\\n            self.freeze()\\n\\n            classifier = [\\\"head\\\", \\\"head_dist\\\"]\\n\\n            for name, module in self.model.named_children():\\n                if name in classifier:\\n                    for p in module.parameters():\\n                        p.requires_grad_(True)\\n\\n        store_attr(\\\"wd, lr, filter_wd\\\")\\n\\n    def forward(self, batched_inputs: torch.Tensor) -> torch.Tensor:\\n        \\\"\\\"\\\"\\n        Runs the batched_inputs through the created model.\\n        \\\"\\\"\\\"\\n        out = self.model(batched_inputs)\\n        return out\\n\\n    @classmethod\\n    def from_config_dict(cls, cfg: DictConfig):\\n        \\\"\\\"\\\"\\n        Instantiate the Meta Architecture from gale config\\n        \\\"\\\"\\\"\\n        # fmt: off\\n        input_shape = ShapeSpec(cfg.input.channels, cfg.input.height, cfg.input.width)\\n        _logger.debug(f\\\"Inputs: {input_shape}\\\")\\n        instance = super().from_config_dict(cfg.model.meta_architecture.init_args, input_shape=input_shape)\\n        param_count = get_human_readable_count(sum([m.numel() for m in instance.parameters()]))\\n        _logger.debug('{} created, param count: {}.'.format(cfg.model.meta_architecture.init_args.model_name, param_count))\\n        # fmt: on\\n        return instance\\n\\n    def build_param_dicts(self):\\n        \\\"\\\"\\\"\\n        Builds up the Paramters dicts for optimization.\\n        \\\"\\\"\\\"\\n        if self.filter_wd:\\n            param_lists = add_weight_decay(\\n                self.model,\\n                weight_decay=self.wd,\\n                skip_list=self.model.no_weight_decay(),\\n            )\\n            param_lists[0][\\\"lr\\\"] = self.lr\\n            param_lists[1][\\\"lr\\\"] = self.lr\\n        else:\\n            ps = trainable_params(self.model)\\n            param_lists = dict(params=ps, lr=self.lr, wd=self.wd)\\n        return param_lists\\n\\n    def get_lrs(self) -> List:\\n        \\\"\\\"\\\"\\n        Returns a List containining the Lrs' for\\n        each parameter group. This is required to build schedulers\\n        like `torch.optim.lr_scheduler.OneCycleScheduler` which needs\\n        the max lrs' for all the Param Groups.\\n        \\\"\\\"\\\"\\n        lrs = []\\n\\n        for p in self.build_param_dicts():\\n            lrs.append(p[\\\"lr\\\"])\\n        return lrs\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "# @TODO: Add support for Discriminative Lr's\n",
    "class ViT(GaleModule):\n",
    "    \"\"\"\n",
    "    A interface to create a Vision Transformer from timm. For available model check :\n",
    "    https://github.com/rwightman/pytorch-image-models/timm/models/vision_transformer.py\n",
    "    \"\"\"\n",
    "\n",
    "    @use_kwargs_dict(\n",
    "        keep=True,\n",
    "        num_classes=1000,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "    )\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        input_shape: ShapeSpec,\n",
    "        lr: float = 1e-03,\n",
    "        wd: float = 1e-05,\n",
    "        pretrained: bool = True,\n",
    "        freeze_to: Optional[int] = None,\n",
    "        finetune: Optional[bool] = None,\n",
    "        act: Optional[str] = None,\n",
    "        reset_classifier: bool = True,\n",
    "        filter_wd: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        1. `input_shape` (ShapeSpec): input image shape. For ViT `height=width` and check the above link for avilable model shapes.\n",
    "        2. `model_name` (str): name of the ViT model, check the above link for avilable models.\n",
    "        3. `pretrained` (bool): load weights pretrained on imagenet.\n",
    "        4. `act` (str): name of the activation layer. Must be registerd in `ACTIVATION_REGISTRY`\n",
    "        5. `num_classes` (int): num output classes.\n",
    "        6. `drop_rate` (float): dropout rate.\n",
    "        7. `attn_drop_rate` (float): attention dropout rate.\n",
    "        8. `drop_path_rate` (float): stochastic depth rate.\n",
    "        9. `reset_classifier` (bool): resets the weights of the classifier.\n",
    "        10. `freeze_to` (int): Freeze the param meter groups of the model upto n.\n",
    "        11. `finetune` (bool): Freeze all the layers and keep only the `classifier` trainable.\n",
    "        \"\"\"\n",
    "        super(ViT, self).__init__()\n",
    "        # create model from timm\n",
    "        assert input_shape.height == input_shape.width\n",
    "        in_chans = input_shape.channels\n",
    "\n",
    "        if act is not None:\n",
    "            act = ACTIVATION_REGISTRY.get(act)\n",
    "        # fmt: off\n",
    "        self.model: VisionTransformer = create_model(model_name, pretrained, in_chans=in_chans, act=act, **kwargs)\n",
    "        # fmt: on\n",
    "        assert isinstance(self.model, VisionTransformer)\n",
    "\n",
    "        if reset_classifier:\n",
    "            num_cls = kwargs.pop(\"num_classes\")\n",
    "            self.model.reset_classifier(num_cls)\n",
    "\n",
    "        if freeze_to is not None:\n",
    "            self.freeze_to(freeze_to)\n",
    "\n",
    "        if finetune:\n",
    "            if freeze_to is not None and isinstance(freeze_to, int):\n",
    "                msg = \"You have sprecified freeze_to along with finetune\"\n",
    "                _logger.warning(msg)\n",
    "            _logger.info(\"Freezing all the model parameters except for the classifier\")\n",
    "            self.freeze()\n",
    "\n",
    "            classifier = [\"head\", \"head_dist\"]\n",
    "\n",
    "            for name, module in self.model.named_children():\n",
    "                if name in classifier:\n",
    "                    for p in module.parameters():\n",
    "                        p.requires_grad_(True)\n",
    "\n",
    "        store_attr(\"wd, lr, filter_wd\")\n",
    "\n",
    "    def forward(self, batched_inputs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Runs the batched_inputs through the created model.\n",
    "        \"\"\"\n",
    "        out = self.model(batched_inputs)\n",
    "        return out\n",
    "\n",
    "    @classmethod\n",
    "    def from_config_dict(cls, cfg: DictConfig):\n",
    "        \"\"\"\n",
    "        Instantiate the Meta Architecture from gale config\n",
    "        \"\"\"\n",
    "        # fmt: off\n",
    "        input_shape = ShapeSpec(cfg.input.channels, cfg.input.height, cfg.input.width)\n",
    "        _logger.debug(f\"Inputs: {input_shape}\")\n",
    "        instance = super().from_config_dict(cfg.model.meta_architecture.init_args, input_shape=input_shape)\n",
    "        param_count = get_human_readable_count(sum([m.numel() for m in instance.parameters()]))\n",
    "        _logger.debug('{} created, param count: {}.'.format(cfg.model.meta_architecture.init_args.model_name, param_count))\n",
    "        # fmt: on\n",
    "        return instance\n",
    "\n",
    "    def build_param_dicts(self):\n",
    "        \"\"\"\n",
    "        Builds up the Paramters dicts for optimization.\n",
    "        \"\"\"\n",
    "        if self.filter_wd:\n",
    "            param_lists = add_weight_decay(\n",
    "                self.model,\n",
    "                weight_decay=self.wd,\n",
    "                skip_list=self.model.no_weight_decay(),\n",
    "            )\n",
    "            param_lists[0][\"lr\"] = self.lr\n",
    "            param_lists[1][\"lr\"] = self.lr\n",
    "        else:\n",
    "            ps = trainable_params(self.model)\n",
    "            param_lists = dict(params=ps, lr=self.lr, wd=self.wd)\n",
    "        return param_lists\n",
    "\n",
    "    def get_lrs(self) -> List:\n",
    "        \"\"\"\n",
    "        Returns a List containining the Lrs' for\n",
    "        each parameter group. This is required to build schedulers\n",
    "        like `torch.optim.lr_scheduler.OneCycleScheduler` which needs\n",
    "        the max lrs' for all the Param Groups.\n",
    "        \"\"\"\n",
    "        lrs = []\n",
    "\n",
    "        for p in self.build_param_dicts():\n",
    "            lrs.append(p[\"lr\"])\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"ViT\" class=\"doc_header\"><code>class</code> <code>ViT</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>ViT</code>(**`model_name`**:`str`, **`input_shape`**:`ShapeSpec`, **`lr`**:`float`=*`0.001`*, **`wd`**:`float`=*`1e-05`*, **`pretrained`**:`bool`=*`True`*, **`freeze_to`**:`Optional`\\[`int`\\]=*`None`*, **`finetune`**:`Optional`\\[`bool`\\]=*`None`*, **`act`**:`Optional`\\[`str`\\]=*`None`*, **`reset_classifier`**:`bool`=*`True`*, **`filter_wd`**:`bool`=*`True`*, **`num_classes`**=*`1000`*, **`drop_rate`**=*`0.0`*, **`attn_drop_rate`**=*`0.0`*, **`drop_path_rate`**=*`0.0`*, **\\*\\*`kwargs`**) :: [`GaleModule`](/gale/core.classes.html#GaleModule)\n",
       "\n",
       "A interface to create a Vision Transformer from timm. For available model check :\n",
       "https://github.com/rwightman/pytorch-image-models/timm/models/vision_transformer.py"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"show_doc(ViT)\";\n",
       "                var nbb_formatted_code = \"show_doc(ViT)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ViT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Arguments :**\n",
    "\n",
    "1. `input_shape` (ShapeSpec): input image shape. For ViT `height=width` and check the above link for avilable model shapes.\n",
    "2. `model_name` (str): name of the ViT model, check the above link for avilable models.\n",
    "3. `pretrained` (bool): load weights pretrained on imagenet.\n",
    "4. `act` (str): name of the activation layer. Must be registerd in `ACTIVATION_REGISTRY`\n",
    "5. `num_classes` (int): num output classes.\n",
    "6. `drop_rate` (float): dropout rate.\n",
    "7. `attn_drop_rate` (float): attention dropout rate.\n",
    "8. `drop_path_rate` (float): stochastic depth rate.\n",
    "9. `reset_classifier` (bool): resets the weights of the classifier.\n",
    "10. `freeze_to` (int): Freeze the param meter groups of the model upto n.\n",
    "11. `finetune` (bool): Freeze all the layers and keep only the `classifier` trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"inp = ShapeSpec(3, 224, 224)\\n\\nm = ViT(\\n    model_name=\\\"vit_small_patch16_224\\\",\\n    pretrained=False,\\n    input_shape=inp,\\n    finetune=True,\\n    reset_classifier=True,\\n    num_classes=10,\\n)\";\n",
       "                var nbb_formatted_code = \"inp = ShapeSpec(3, 224, 224)\\n\\nm = ViT(\\n    model_name=\\\"vit_small_patch16_224\\\",\\n    pretrained=False,\\n    input_shape=inp,\\n    finetune=True,\\n    reset_classifier=True,\\n    num_classes=10,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inp = ShapeSpec(3, 224, 224)\n",
    "\n",
    "m = ViT(\n",
    "    model_name=\"vit_small_patch16_224\",\n",
    "    pretrained=False,\n",
    "    input_shape=inp,\n",
    "    finetune=True,\n",
    "    reset_classifier=True,\n",
    "    num_classes=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"i = torch.randn(2, inp.channels, inp.height, inp.width)\\no = m(i)\";\n",
       "                var nbb_formatted_code = \"i = torch.randn(2, inp.channels, inp.height, inp.width)\\no = m(i)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = torch.randn(2, inp.channels, inp.height, inp.width)\n",
    "o = m(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to `GeneralizedImageClassifier` we can also instantiate `ViT` from a config. `ViT` does not require neither a `backbone` nor a `head` configuration. We just need the particular initialization arguments for the vit model defined in `model_name`.\n",
    "\n",
    "> Note: You input shape must match the dimensions that the Vision Transformer model supports. Unlike `GeneralizedImageClassifier`, `ViT` is dependent on the shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"from dataclasses import dataclass, field\\nfrom omegaconf import MISSING, OmegaConf\";\n",
       "                var nbb_formatted_code = \"from dataclasses import dataclass, field\\nfrom omegaconf import MISSING, OmegaConf\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from omegaconf import MISSING, OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"@dataclass\\nclass ModelConf:\\n    model_name: str = \\\"vit_small_patch16_224\\\"\\n    pretrained: bool = False\\n    finetune: bool = True\\n    reset_classifier: bool = True\\n    num_classes: int = 10\\n\\n\\ninp = ShapeSpec(3, 224, 224)\\n\\nmeta_args = OmegaConf.structured(ModelConf())\\n\\nmeta = OmegaConf.create()\\nmeta.name = \\\"ViT\\\"\\nmeta.init_args = meta_args\\n\\ni = OmegaConf.create()\\ni.channels = inp.channels\\ni.height = inp.height\\ni.width = inp.width\\n\\nC = OmegaConf.create()\\nC.input = i\\nC.model = OmegaConf.create()\\nC.model.meta_architecture = meta\\n\\n# print(OmegaConf.to_yaml(C, resolve=True))\";\n",
       "                var nbb_formatted_code = \"@dataclass\\nclass ModelConf:\\n    model_name: str = \\\"vit_small_patch16_224\\\"\\n    pretrained: bool = False\\n    finetune: bool = True\\n    reset_classifier: bool = True\\n    num_classes: int = 10\\n\\n\\ninp = ShapeSpec(3, 224, 224)\\n\\nmeta_args = OmegaConf.structured(ModelConf())\\n\\nmeta = OmegaConf.create()\\nmeta.name = \\\"ViT\\\"\\nmeta.init_args = meta_args\\n\\ni = OmegaConf.create()\\ni.channels = inp.channels\\ni.height = inp.height\\ni.width = inp.width\\n\\nC = OmegaConf.create()\\nC.input = i\\nC.model = OmegaConf.create()\\nC.model.meta_architecture = meta\\n\\n# print(OmegaConf.to_yaml(C, resolve=True))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dataclass\n",
    "class ModelConf:\n",
    "    model_name: str = \"vit_small_patch16_224\"\n",
    "    pretrained: bool = False\n",
    "    finetune: bool = True\n",
    "    reset_classifier: bool = True\n",
    "    num_classes: int = 10\n",
    "\n",
    "\n",
    "inp = ShapeSpec(3, 224, 224)\n",
    "\n",
    "meta_args = OmegaConf.structured(ModelConf())\n",
    "\n",
    "meta = OmegaConf.create()\n",
    "meta.name = \"ViT\"\n",
    "meta.init_args = meta_args\n",
    "\n",
    "i = OmegaConf.create()\n",
    "i.channels = inp.channels\n",
    "i.height = inp.height\n",
    "i.width = inp.width\n",
    "\n",
    "C = OmegaConf.create()\n",
    "C.input = i\n",
    "C.model = OmegaConf.create()\n",
    "C.model.meta_architecture = meta\n",
    "\n",
    "# print(OmegaConf.to_yaml(C, resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"m = ViT.from_config_dict(C)\\n\\nassert isinstance(m, ViT)\\nassert isinstance(m.model, VisionTransformer)\";\n",
       "                var nbb_formatted_code = \"m = ViT.from_config_dict(C)\\n\\nassert isinstance(m, ViT)\\nassert isinstance(m.model, VisionTransformer)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = ViT.from_config_dict(C)\n",
    "\n",
    "assert isinstance(m, ViT)\n",
    "assert isinstance(m.model, VisionTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT(\n",
      "  (model): VisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=2304, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=2304, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=2304, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=2304, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=2304, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=2304, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=2304, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=2304, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (pre_logits): Identity()\n",
      "    (head): Linear(in_features=768, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"# collapse-output\\nprint(m)\";\n",
       "                var nbb_formatted_code = \"# collapse-output\\nprint(m)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# collapse-output\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"i = torch.randn(2, inp.channels, inp.height, inp.width)\\no = m(i)\";\n",
       "                var nbb_formatted_code = \"i = torch.randn(2, inp.channels, inp.height, inp.width)\\no = m(i)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = torch.randn(2, inp.channels, inp.height, inp.width)\n",
    "o = m(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.utils.logger.ipynb.\n",
      "Converted 00a_core.utils.visualize.ipynb.\n",
      "Converted 00b_core.utils.structures.ipynb.\n",
      "Converted 01_core.nn.utils.ipynb.\n",
      "Converted 01a_core.nn.losses.ipynb.\n",
      "Converted 02_core.nn.optim.optimizers.ipynb.\n",
      "Converted 02a_core.nn.optim.lr_schedulers.ipynb.\n",
      "Converted 03_core.classes.ipynb.\n",
      "Converted 04_classification.modelling.backbones.ipynb.\n",
      "Converted 04a_classification.modelling.heads.ipynb.\n",
      "Converted 04b_classification.modelling.meta_arch.common.ipynb.\n",
      "Converted 04b_classification.modelling.meta_arch.vit.ipynb.\n",
      "Converted 05_classification.data.common.ipynb.\n",
      "Converted 05a_classification.data.transforms.ipynb.\n",
      "Converted 05b_classification.data.build.ipynb.\n",
      "Converted 06_classification.task.ipynb.\n",
      "Converted 07_collections.pandas.ipynb.\n",
      "Converted 07a_collections.callbacks.notebook.ipynb.\n",
      "Converted 07b_collections.callbacks.ema.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"# hide\\nnotebook2script()\";\n",
       "                var nbb_formatted_code = \"# hide\\nnotebook2script()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gale",
   "language": "python",
   "name": "gale"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
