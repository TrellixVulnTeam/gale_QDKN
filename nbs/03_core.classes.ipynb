{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# skip\n",
    "!git clone https://github.com/benihime91/gale # install gale on colab\n",
    "!pip install -e \"gale[dev]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# hide\\n%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\\n%matplotlib inline\";\n",
       "                var nbb_formatted_code = \"# hide\\n%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\\n%matplotlib inline\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "%load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# hide\\nimport warnings\\n\\nfrom nbdev.export import *\\nfrom nbdev.showdoc import *\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\";\n",
       "                var nbb_formatted_code = \"# hide\\nimport warnings\\n\\nfrom nbdev.export import *\\nfrom nbdev.showdoc import *\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "import warnings\n",
    "\n",
    "from nbdev.export import *\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes\n",
    "> Interfaces common to all `Modules` and `Models` in Gale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# export\\nimport copy\\nimport logging\\nimport math\\nfrom abc import ABC, ABCMeta, abstractmethod\\nfrom contextlib import contextmanager\\nfrom typing import *\\n\\nimport hydra\\nimport pytorch_lightning as pl\\nimport torch\\nimport torchmetrics\\nfrom fastcore.all import L, ifnone, noop, patch\\nfrom omegaconf import DictConfig, OmegaConf\\nfrom torch.nn import Module\\n\\nfrom gale.core.nn.optim import OPTIM_REGISTRY, SCHEDULER_REGISTRY\\nfrom gale.core.nn.utils import params, trainable_params\\nfrom gale.core.utils.logger import log_main_process\\n\\n_logger = logging.getLogger(__name__)\";\n",
       "                var nbb_formatted_code = \"# export\\nimport copy\\nimport logging\\nimport math\\nfrom abc import ABC, ABCMeta, abstractmethod\\nfrom contextlib import contextmanager\\nfrom typing import *\\n\\nimport hydra\\nimport pytorch_lightning as pl\\nimport torch\\nimport torchmetrics\\nfrom fastcore.all import L, ifnone, noop, patch\\nfrom omegaconf import DictConfig, OmegaConf\\nfrom torch.nn import Module\\n\\nfrom gale.core.nn.optim import OPTIM_REGISTRY, SCHEDULER_REGISTRY\\nfrom gale.core.nn.utils import params, trainable_params\\nfrom gale.core.utils.logger import log_main_process\\n\\n_logger = logging.getLogger(__name__)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "import copy\n",
    "import logging\n",
    "import math\n",
    "from abc import ABC, ABCMeta, abstractmethod\n",
    "from contextlib import contextmanager\n",
    "from typing import *\n",
    "\n",
    "import hydra\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchmetrics\n",
    "from fastcore.all import L, ifnone, noop, patch\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from torch.nn import Module\n",
    "\n",
    "from gale.core.nn.optim import OPTIM_REGISTRY, SCHEDULER_REGISTRY\n",
    "from gale.core.nn.utils import params, trainable_params\n",
    "from gale.core.utils.logger import log_main_process\n",
    "\n",
    "_logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurable-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# export\\nclass Configurable(ABC):\\n    \\\"\\\"\\\"\\n    Helper Class to instantiate obj from config\\n    \\\"\\\"\\\"\\n\\n    @classmethod\\n    def from_config_dict(cls, config: DictConfig, **kwargs):\\n        \\\"\\\"\\\"\\n        Instantiates object using `DictConfig-based` configuration. You can optionally\\n        pass in extra `kwargs`\\n        \\\"\\\"\\\"\\n        # Resolve the config dict\\n        if isinstance(config, DictConfig):\\n            config = OmegaConf.to_container(config, resolve=True)\\n            config = OmegaConf.create(config)\\n\\n        if \\\"_target_\\\" in config:\\n            # regular hydra-based instantiation\\n            instance = hydra.utils.instantiate(config=config, **kwargs)\\n        else:\\n            # instantiate directly using kwargs\\n            try:\\n                instance = cls(cfg=config, **kwargs)\\n            except:\\n                cfg = OmegaConf.to_container(config, resolve=True)\\n                instance = cls(**config, **kwargs)\\n\\n        if not hasattr(instance, \\\"_cfg\\\"):\\n            instance._cfg = config\\n        return instance\\n\\n    def to_config_dict(self) -> DictConfig:\\n        # fmt: off\\n        \\\"\\\"\\\"Returns object's configuration to config dictionary\\\"\\\"\\\"\\n        if hasattr(self, \\\"_cfg\\\") and self._cfg is not None and isinstance(self._cfg, DictConfig):\\n            # Resolve the config dict\\n            config = OmegaConf.to_container(self._cfg, resolve=True)\\n            config = OmegaConf.create(config)\\n            OmegaConf.set_struct(config, True)\\n            self._cfg = config\\n\\n            return self._cfg\\n        else:\\n            raise NotImplementedError(\\\"to_config_dict() can currently only return object._cfg but current object does not have it.\\\")\\n        # fmt: on\";\n",
       "                var nbb_formatted_code = \"# export\\nclass Configurable(ABC):\\n    \\\"\\\"\\\"\\n    Helper Class to instantiate obj from config\\n    \\\"\\\"\\\"\\n\\n    @classmethod\\n    def from_config_dict(cls, config: DictConfig, **kwargs):\\n        \\\"\\\"\\\"\\n        Instantiates object using `DictConfig-based` configuration. You can optionally\\n        pass in extra `kwargs`\\n        \\\"\\\"\\\"\\n        # Resolve the config dict\\n        if isinstance(config, DictConfig):\\n            config = OmegaConf.to_container(config, resolve=True)\\n            config = OmegaConf.create(config)\\n\\n        if \\\"_target_\\\" in config:\\n            # regular hydra-based instantiation\\n            instance = hydra.utils.instantiate(config=config, **kwargs)\\n        else:\\n            # instantiate directly using kwargs\\n            try:\\n                instance = cls(cfg=config, **kwargs)\\n            except:\\n                cfg = OmegaConf.to_container(config, resolve=True)\\n                instance = cls(**config, **kwargs)\\n\\n        if not hasattr(instance, \\\"_cfg\\\"):\\n            instance._cfg = config\\n        return instance\\n\\n    def to_config_dict(self) -> DictConfig:\\n        # fmt: off\\n        \\\"\\\"\\\"Returns object's configuration to config dictionary\\\"\\\"\\\"\\n        if hasattr(self, \\\"_cfg\\\") and self._cfg is not None and isinstance(self._cfg, DictConfig):\\n            # Resolve the config dict\\n            config = OmegaConf.to_container(self._cfg, resolve=True)\\n            config = OmegaConf.create(config)\\n            OmegaConf.set_struct(config, True)\\n            self._cfg = config\\n\\n            return self._cfg\\n        else:\\n            raise NotImplementedError(\\\"to_config_dict() can currently only return object._cfg but current object does not have it.\\\")\\n        # fmt: on\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class Configurable(ABC):\n",
    "    \"\"\"\n",
    "    Helper Class to instantiate obj from config\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_config_dict(cls, config: DictConfig, **kwargs):\n",
    "        \"\"\"\n",
    "        Instantiates object using `DictConfig-based` configuration. You can optionally\n",
    "        pass in extra `kwargs`\n",
    "        \"\"\"\n",
    "        # Resolve the config dict\n",
    "        if isinstance(config, DictConfig):\n",
    "            config = OmegaConf.to_container(config, resolve=True)\n",
    "            config = OmegaConf.create(config)\n",
    "\n",
    "        if \"_target_\" in config:\n",
    "            # regular hydra-based instantiation\n",
    "            instance = hydra.utils.instantiate(config=config, **kwargs)\n",
    "        else:\n",
    "            # instantiate directly using kwargs\n",
    "            try:\n",
    "                instance = cls(cfg=config, **kwargs)\n",
    "            except:\n",
    "                cfg = OmegaConf.to_container(config, resolve=True)\n",
    "                instance = cls(**config, **kwargs)\n",
    "\n",
    "        if not hasattr(instance, \"_cfg\"):\n",
    "            instance._cfg = config\n",
    "        return instance\n",
    "\n",
    "    def to_config_dict(self) -> DictConfig:\n",
    "        # fmt: off\n",
    "        \"\"\"Returns object's configuration to config dictionary\"\"\"\n",
    "        if hasattr(self, \"_cfg\") and self._cfg is not None and isinstance(self._cfg, DictConfig):\n",
    "            # Resolve the config dict\n",
    "            config = OmegaConf.to_container(self._cfg, resolve=True)\n",
    "            config = OmegaConf.create(config)\n",
    "            OmegaConf.set_struct(config, True)\n",
    "            self._cfg = config\n",
    "\n",
    "            return self._cfg\n",
    "        else:\n",
    "            raise NotImplementedError(\"to_config_dict() can currently only return object._cfg but current object does not have it.\")\n",
    "        # fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class provides a common interface for modules so that, they can be easy loaded from a Hydra Config file. This class also supports instantiating via hydra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Configurable.from_config_dict\" class=\"doc_header\"><code>Configurable.from_config_dict</code><a href=\"__main__.py#L7\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Configurable.from_config_dict</code>(**`config`**:`DictConfig`, **\\*\\*`kwargs`**)\n",
       "\n",
       "Instantiates object using `DictConfig-based` configuration. You can optionally\n",
       "pass in extra `kwargs`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"show_doc(Configurable.from_config_dict)\";\n",
       "                var nbb_formatted_code = \"show_doc(Configurable.from_config_dict)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Configurable.from_config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Configurable.to_config_dict\" class=\"doc_header\"><code>Configurable.to_config_dict</code><a href=\"__main__.py#L33\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Configurable.to_config_dict</code>()\n",
       "\n",
       "Returns object's configuration to config dictionary"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"show_doc(Configurable.to_config_dict)\";\n",
       "                var nbb_formatted_code = \"show_doc(Configurable.to_config_dict)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Configurable.to_config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GaleModule-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"# export\\nclass GaleModule(Module, Configurable, metaclass=ABCMeta):\\n    \\\"\\\"\\\"\\n    Abstract class offering interface which should be implemented by all `Backbones`,\\n    `Heads` and `Meta Archs` in gale.\\n    \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def forward(self) -> Any:\\n        \\\"\\\"\\\"\\n        The main logic for the model lives here. Can return either features, logits\\n        or loss.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def build_param_dicts(self) -> Union[Iterable, List[Dict], Dict, List]:\\n        \\\"\\\"\\\"\\n        Should return the iterable of parameters to optimize or dicts defining parameter groups\\n        for the Module.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @property\\n    def param_lists(self):\\n        \\\"Returns the list of paramters in the module\\\"\\n        return [p for p in self.parameters()]\\n\\n    def all_params(self, n=slice(None), with_grad=False):\\n        \\\"List of `param_groups` upto n\\\"\\n        res = L(p for p in self.param_lists[n])\\n        # fmt: off\\n        return L(o for o in res if hasattr(o, \\\"grad\\\") and o.grad is not None) if with_grad else res\\n        # fmt: on\\n\\n    def _set_require_grad(self, rg, p):\\n        p.requires_grad_(rg)\\n\\n    def unfreeze(self) -> None:\\n        \\\"\\\"\\\"\\n        Unfreeze all parameters for training.\\n        \\\"\\\"\\\"\\n        for param in self.parameters():\\n            param.requires_grad = True\\n\\n        self.train()\\n\\n    def freeze(self) -> None:\\n        \\\"\\\"\\\"\\n        Freeze all params for inference & set model to eval\\n        \\\"\\\"\\\"\\n        for param in self.parameters():\\n            param.requires_grad = False\\n        self.eval()\\n\\n    def freeze_to(self, n) -> None:\\n        \\\"Freeze parameter groups up to `n`\\\"\\n        self.frozen_idx = n if n >= 0 else len(self.param_lists) + n\\n        if self.frozen_idx >= len(self.param_lists):\\n            # fmt: off\\n            _logger.warning(f\\\"Freezing {self.frozen_idx} groups; model has {len(self.param_lists)}; whole model is frozen.\\\")\\n            # fmt: on\\n\\n        for o in self.all_params(slice(n, None)):\\n            self._set_require_grad(True, o)\\n\\n        for o in self.all_params(slice(None, n)):\\n            self._set_require_grad(False, o)\\n\\n    @contextmanager\\n    def as_frozen(self):\\n        \\\"\\\"\\\"\\n        Context manager which temporarily freezes a module, yields control\\n        and finally unfreezes the module.\\n        \\\"\\\"\\\"\\n        self.freeze()\\n\\n        try:\\n            yield\\n        finally:\\n            self.unfreeze()\";\n",
       "                var nbb_formatted_code = \"# export\\nclass GaleModule(Module, Configurable, metaclass=ABCMeta):\\n    \\\"\\\"\\\"\\n    Abstract class offering interface which should be implemented by all `Backbones`,\\n    `Heads` and `Meta Archs` in gale.\\n    \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def forward(self) -> Any:\\n        \\\"\\\"\\\"\\n        The main logic for the model lives here. Can return either features, logits\\n        or loss.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def build_param_dicts(self) -> Union[Iterable, List[Dict], Dict, List]:\\n        \\\"\\\"\\\"\\n        Should return the iterable of parameters to optimize or dicts defining parameter groups\\n        for the Module.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @property\\n    def param_lists(self):\\n        \\\"Returns the list of paramters in the module\\\"\\n        return [p for p in self.parameters()]\\n\\n    def all_params(self, n=slice(None), with_grad=False):\\n        \\\"List of `param_groups` upto n\\\"\\n        res = L(p for p in self.param_lists[n])\\n        # fmt: off\\n        return L(o for o in res if hasattr(o, \\\"grad\\\") and o.grad is not None) if with_grad else res\\n        # fmt: on\\n\\n    def _set_require_grad(self, rg, p):\\n        p.requires_grad_(rg)\\n\\n    def unfreeze(self) -> None:\\n        \\\"\\\"\\\"\\n        Unfreeze all parameters for training.\\n        \\\"\\\"\\\"\\n        for param in self.parameters():\\n            param.requires_grad = True\\n\\n        self.train()\\n\\n    def freeze(self) -> None:\\n        \\\"\\\"\\\"\\n        Freeze all params for inference & set model to eval\\n        \\\"\\\"\\\"\\n        for param in self.parameters():\\n            param.requires_grad = False\\n        self.eval()\\n\\n    def freeze_to(self, n) -> None:\\n        \\\"Freeze parameter groups up to `n`\\\"\\n        self.frozen_idx = n if n >= 0 else len(self.param_lists) + n\\n        if self.frozen_idx >= len(self.param_lists):\\n            # fmt: off\\n            _logger.warning(f\\\"Freezing {self.frozen_idx} groups; model has {len(self.param_lists)}; whole model is frozen.\\\")\\n            # fmt: on\\n\\n        for o in self.all_params(slice(n, None)):\\n            self._set_require_grad(True, o)\\n\\n        for o in self.all_params(slice(None, n)):\\n            self._set_require_grad(False, o)\\n\\n    @contextmanager\\n    def as_frozen(self):\\n        \\\"\\\"\\\"\\n        Context manager which temporarily freezes a module, yields control\\n        and finally unfreezes the module.\\n        \\\"\\\"\\\"\\n        self.freeze()\\n\\n        try:\\n            yield\\n        finally:\\n            self.unfreeze()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class GaleModule(Module, Configurable, metaclass=ABCMeta):\n",
    "    \"\"\"\n",
    "    Abstract class offering interface which should be implemented by all `Backbones`,\n",
    "    `Heads` and `Meta Archs` in gale.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self) -> Any:\n",
    "        \"\"\"\n",
    "        The main logic for the model lives here. Can return either features, logits\n",
    "        or loss.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def build_param_dicts(self) -> Union[Iterable, List[Dict], Dict, List]:\n",
    "        \"\"\"\n",
    "        Should return the iterable of parameters to optimize or dicts defining parameter groups\n",
    "        for the Module.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def param_lists(self):\n",
    "        \"Returns the list of paramters in the module\"\n",
    "        return [p for p in self.parameters()]\n",
    "\n",
    "    def all_params(self, n=slice(None), with_grad=False):\n",
    "        \"List of `param_groups` upto n\"\n",
    "        res = L(p for p in self.param_lists[n])\n",
    "        # fmt: off\n",
    "        return L(o for o in res if hasattr(o, \"grad\") and o.grad is not None) if with_grad else res\n",
    "        # fmt: on\n",
    "\n",
    "    def _set_require_grad(self, rg, p):\n",
    "        p.requires_grad_(rg)\n",
    "\n",
    "    def unfreeze(self) -> None:\n",
    "        \"\"\"\n",
    "        Unfreeze all parameters for training.\n",
    "        \"\"\"\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.train()\n",
    "\n",
    "    def freeze(self) -> None:\n",
    "        \"\"\"\n",
    "        Freeze all params for inference & set model to eval\n",
    "        \"\"\"\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.eval()\n",
    "\n",
    "    def freeze_to(self, n) -> None:\n",
    "        \"Freeze parameter groups up to `n`\"\n",
    "        self.frozen_idx = n if n >= 0 else len(self.param_lists) + n\n",
    "        if self.frozen_idx >= len(self.param_lists):\n",
    "            # fmt: off\n",
    "            _logger.warning(f\"Freezing {self.frozen_idx} groups; model has {len(self.param_lists)}; whole model is frozen.\")\n",
    "            # fmt: on\n",
    "\n",
    "        for o in self.all_params(slice(n, None)):\n",
    "            self._set_require_grad(True, o)\n",
    "\n",
    "        for o in self.all_params(slice(None, n)):\n",
    "            self._set_require_grad(False, o)\n",
    "\n",
    "    @contextmanager\n",
    "    def as_frozen(self):\n",
    "        \"\"\"\n",
    "        Context manager which temporarily freezes a module, yields control\n",
    "        and finally unfreezes the module.\n",
    "        \"\"\"\n",
    "        self.freeze()\n",
    "\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            self.unfreeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any Module that is Registerd in Gale should inherit from this class or its subclass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internals-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.forward\" class=\"doc_header\"><code>GaleModule.forward</code><a href=\"__main__.py#L8\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleModule.forward</code>()\n",
       "\n",
       "The main logic for the model lives here. Can return either features, logits\n",
       "or loss."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleModule.forward)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleModule.forward)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleModule.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.build_param_dicts\" class=\"doc_header\"><code>GaleModule.build_param_dicts</code><a href=\"__main__.py#L16\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleModule.build_param_dicts</code>()\n",
       "\n",
       "Should return the iterable of parameters to optimize or dicts defining parameter groups\n",
       "for the Module."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleModule.build_param_dicts)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleModule.build_param_dicts)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleModule.build_param_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Configurable.from_config_dict\" class=\"doc_header\"><code>Configurable.from_config_dict</code><a href=\"__main__.py#L7\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Configurable.from_config_dict</code>(**`config`**:`DictConfig`, **\\*\\*`kwargs`**)\n",
       "\n",
       "Instantiates object using `DictConfig-based` configuration. You can optionally\n",
       "pass in extra `kwargs`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleModule.from_config_dict)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleModule.from_config_dict)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleModule.from_config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.param_lists\" class=\"doc_header\"><code>GaleModule.param_lists</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "Returns the list of paramters in the module"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleModule.param_lists)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleModule.param_lists)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleModule.param_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.all_params\" class=\"doc_header\"><code>GaleModule.all_params</code><a href=\"__main__.py#L29\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleModule.all_params</code>(**`n`**=*`slice(None, None, None)`*, **`with_grad`**=*`False`*)\n",
       "\n",
       "List of `param_groups` upto n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleModule.all_params)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleModule.all_params)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleModule.all_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.freeze\" class=\"doc_header\"><code>GaleModule.freeze</code><a href=\"__main__.py#L48\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleModule.freeze</code>()\n",
       "\n",
       "Freeze all params for inference & set model to eval"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleModule.freeze)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleModule.freeze)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleModule.freeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.freeze_to\" class=\"doc_header\"><code>GaleModule.freeze_to</code><a href=\"__main__.py#L56\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleModule.freeze_to</code>(**`n`**)\n",
       "\n",
       "Freeze parameter groups up to `n`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleModule.freeze_to)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleModule.freeze_to)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleModule.freeze_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.unfreeze\" class=\"doc_header\"><code>GaleModule.unfreeze</code><a href=\"__main__.py#L39\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleModule.unfreeze</code>()\n",
       "\n",
       "Unfreeze all parameters for training."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleModule.unfreeze)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleModule.unfreeze)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleModule.unfreeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.as_frozen\" class=\"doc_header\"><code>GaleModule.as_frozen</code><a href=\"__main__.py#L70\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleModule.as_frozen</code>()\n",
       "\n",
       "Context manager which temporarily freezes a module, yields control\n",
       "and finally unfreezes the module."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleModule.as_frozen)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleModule.as_frozen)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleModule.as_frozen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GaleTask -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"# export\\ndef get_callable_name(fn_or_class: Union[Callable, object]) -> str:\\n    return getattr(fn_or_class, \\\"__name__\\\", fn_or_class.__class__.__name__).lower()\\n\\n\\ndef get_callable_dict(fn: Union[Callable, Mapping, Sequence]) -> Union[Dict, Mapping]:\\n    if isinstance(fn, Mapping):\\n        return fn\\n    elif isinstance(fn, Sequence):\\n        return {get_callable_name(f): f for f in fn}\\n    elif callable(fn):\\n        return {get_callable_name(fn): fn}\\n\\n\\ndef setup_metrics(\\n    metrics: Union[torchmetrics.Metric, Mapping, Sequence, None]\\n) -> torch.nn.ModuleDict:\\n    m = {} if metrics is None else get_callable_dict(metrics)\\n    return torch.nn.ModuleDict(m)\";\n",
       "                var nbb_formatted_code = \"# export\\ndef get_callable_name(fn_or_class: Union[Callable, object]) -> str:\\n    return getattr(fn_or_class, \\\"__name__\\\", fn_or_class.__class__.__name__).lower()\\n\\n\\ndef get_callable_dict(fn: Union[Callable, Mapping, Sequence]) -> Union[Dict, Mapping]:\\n    if isinstance(fn, Mapping):\\n        return fn\\n    elif isinstance(fn, Sequence):\\n        return {get_callable_name(f): f for f in fn}\\n    elif callable(fn):\\n        return {get_callable_name(fn): fn}\\n\\n\\ndef setup_metrics(\\n    metrics: Union[torchmetrics.Metric, Mapping, Sequence, None]\\n) -> torch.nn.ModuleDict:\\n    m = {} if metrics is None else get_callable_dict(metrics)\\n    return torch.nn.ModuleDict(m)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "def get_callable_name(fn_or_class: Union[Callable, object]) -> str:\n",
    "    return getattr(fn_or_class, \"__name__\", fn_or_class.__class__.__name__).lower()\n",
    "\n",
    "\n",
    "def get_callable_dict(fn: Union[Callable, Mapping, Sequence]) -> Union[Dict, Mapping]:\n",
    "    if isinstance(fn, Mapping):\n",
    "        return fn\n",
    "    elif isinstance(fn, Sequence):\n",
    "        return {get_callable_name(f): f for f in fn}\n",
    "    elif callable(fn):\n",
    "        return {get_callable_name(fn): fn}\n",
    "\n",
    "\n",
    "def setup_metrics(\n",
    "    metrics: Union[torchmetrics.Metric, Mapping, Sequence, None]\n",
    ") -> torch.nn.ModuleDict:\n",
    "    m = {} if metrics is None else get_callable_dict(metrics)\n",
    "    return torch.nn.ModuleDict(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"# export\\n# fmt: off\\nclass GaleTask(pl.LightningModule):\\n    \\\"\\\"\\\"\\n    Interface for Pytorch-lightning based Gale modules\\n    \\\"\\\"\\\"\\n    is_restored = True\\n    \\n    def __init__(\\n        self,\\n        cfg: DictConfig,\\n        trainer: Optional[pl.Trainer] = None,\\n        metrics: Union[torchmetrics.Metric, Mapping, Sequence, None] = None,\\n    ):\\n        \\\"\\\"\\\"\\n        Base class from which all PyTorch Lightning Tasks in Gale should inherit.\\n        Provides a few helper functions primarily for optimization.\\n\\n        Arguments:\\n        1. `cfg` `(DictConfig)`:  configuration object. cfg object should be inherited from `BaseGaleConfig`.\\n        2. `trainer` `(Optional, pl.Trainer)`: Pytorch Lightning Trainer instance\\n        3. `metrics` `(Optional)`: Metrics to compute for training and evaluation.\\n        \\\"\\\"\\\"\\n        super().__init__()\\n        self._cfg = OmegaConf.create(cfg)\\n        self._cfg = OmegaConf.structured(cfg)\\n        \\n        if trainer is not None and not isinstance(trainer, pl.Trainer):\\n            msg = f\\\"Trainer constructor argument must be either None or pl.Trainer.But got {type(trainer)} instead.\\\"\\n            raise ValueError(msg) \\n        \\n        self._train_dl = noop\\n        self._validation_dl = noop\\n        self._test_dl = noop\\n        \\n        self._optimizer = noop\\n        self._scheduler = noop\\n        \\n        self._trainer = ifnone(trainer, noop)\\n        self._metrics = setup_metrics(metrics)\\n        self._model = noop \\n        \\n        self.save_hyperparameters(self._cfg)\\n        \\n        # if trained is not passed them the Model is being restored\\n        if self._trainer is not None:\\n            self.is_restored = False\\n        else:\\n            self.is_restored = True\\n               \\n        \\n    @abstractmethod\\n    def forward(self, x: torch.Tensor) -> Any:\\n        \\\"\\\"\\\"\\n        The Forward method for LightningModule, users should modify this method.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n        \\n        \\n    def shared_step(self, batch: Any, batch_idx: int, stage: str) -> Dict:\\n        \\\"\\\"\\\"\\n        The common training/validation/test step. Override for custom behavior. This step\\n        is shared between training/validation/test step. For training/validation/test steps\\n        `stage` is train/val/test respectively. You training logic should go here avoid directly overriding\\n        training/validation/test step methods. This step needs to return a dictionary contatining\\n        the loss to optimize and values to log.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n        \\n    def training_step(self, batch: Any, batch_idx: int) -> Any:\\n        \\\"\\\"\\\"\\n        The training step of the LightningModule. For common use cases you need\\n        not need to override this method. See `GaleTask.shared_step()`\\n        \\\"\\\"\\\"\\n        output = self.shared_step(batch, batch_idx, stage=\\\"train\\\")\\n        self.log_dict({f\\\"train/{k}\\\": v for k, v in output[\\\"logs\\\"].items()})\\n        return output[\\\"loss\\\"]\\n\\n    def validation_step(self, batch: Any, batch_idx: int) -> None:\\n        \\\"\\\"\\\"\\n        The validation step of the LightningModule. For common use cases you need\\n        not need to override this method. See `GaleTask.shared_step()`\\n        \\\"\\\"\\\"\\n        output = self.shared_step(batch, batch_idx, stage=\\\"validation\\\")\\n        self.log_dict({f\\\"val/{k}\\\": v for k, v in output[\\\"logs\\\"].items()})\\n\\n    def test_step(self, batch: Any, batch_idx: int) -> None:\\n        \\\"\\\"\\\"\\n        The test step of the LightningModule. For common use cases you need\\n        not need to override this method. See `GaleTask.shared_step()`\\n        \\\"\\\"\\\"\\n        output = self.shared_step(batch, batch_idx, stage=\\\"test\\\")\\n        self.log_dict({f\\\"test/{k}\\\": v for k, v in output[\\\"logs\\\"].items()})\\n    \\n    def configure_optimizers(self) -> Any:\\n        \\\"\\\"\\\"\\n        Choose what optimizers and learning-rate schedulers to use in your optimization.\\n        See https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html\\n        \\\"\\\"\\\"\\n        # if self.setup_optimization() has been called manually no\\n        # need to call again\\n        if self._optimizer is noop and self._scheduler is noop:\\n            self.setup_optimization()\\n\\n        if self._scheduler is None:\\n            return self._optimizer\\n        else:\\n            return [self._optimizer], [self._scheduler]\\n        \\n    def train_dataloader(self) -> torch.utils.data.DataLoader:\\n        \\\"Returns the Dataloader used for Training\\\"\\n        if self._train_dl is not None and self._train_dl is not noop:\\n            return self._train_dl\\n\\n    def val_dataloader(self) -> Any:\\n        \\\"Returns the List of Dataloaders or Dataloader used for Validation\\\"\\n        if self._validation_dl is not None and self._validation_dl is not noop:\\n            return self._validation_dl\\n\\n    def test_dataloader(self) -> Any:\\n        \\\"Returns the List of Dataloaders or Dataloader used for Testing\\\"\\n        if self._test_dl is not None and self._test_dl is not noop:\\n            return self._test_dl\\n    \\n    def process_optim_config(self, opt_conf: DictConfig) -> DictConfig:\\n        \\\"\\\"\\\"\\n        Prepares an optimizer from a string name and its optional config parameters.\\n        Preprocess the optimization config and adds some infered values like max_steps, max_epochs, etc.\\n        This method also fills in the values for `max_iters` & `epochs`, `steps_per_epoch` if \\n        the values are `-1`\\n        \\\"\\\"\\\"\\n        # some optimizers/schedulers need parameters only known dynamically\\n        # allow users to override the getter to instantiate them lazily\\n        \\n        opt_conf = copy.deepcopy(opt_conf)\\n        \\n        # Force into DictConfig structure\\n        opt_conf = OmegaConf.create(opt_conf)\\n        \\n        if self._trainer.max_epochs is None and self._trainer.max_steps is None:\\n            raise ValueError(\\\"Either one of max_epochs or max_epochs must be provided in Trainer\\\")\\n        else:\\n            max_steps, steps = self.num_training_steps()\\n            max_epochs = ifnone(self._trainer.max_epochs, max_steps // steps)\\n            \\n        vals = dict(steps_per_epoch=steps, max_steps=max_steps, max_epochs=max_epochs)\\n        \\n        # Force into native dictionary\\n        opt_conf = OmegaConf.to_container(opt_conf, resolve=True)\\n        \\n        for key,value in vals.items():\\n            if opt_conf[key] < 1:\\n                opt_conf[key] = value\\n                \\n        # populate values in learning rate schedulers initialization arguments\\n        opt_conf = OmegaConf.create(opt_conf)\\n        sched_config = OmegaConf.to_container(opt_conf.scheduler.init_args, resolve=True)\\n        \\n        # Force into DictConfig structure\\n        opt_conf = OmegaConf.create(opt_conf)\\n        \\n        # @TODO: Find a better way to do this\\n        if \\\"max_iters\\\" in sched_config:\\n            if sched_config[\\\"max_iters\\\"] == -1:\\n                OmegaConf.update(opt_conf, \\\"scheduler.init_args.max_iters\\\", max_steps)\\n                msg = f\\\"Set the value of 'max_iters' to be {max_steps}.\\\"\\n                log_main_process(_logger, logging.DEBUG, msg)\\n\\n        if \\\"epochs\\\" in sched_config:\\n            if sched_config[\\\"epochs\\\"] == -1:\\n                OmegaConf.update(opt_conf, \\\"scheduler.init_args.epochs\\\", max_epochs)\\n                msg = f\\\"Set the value of 'epochs' to be {max_epochs}.\\\"\\n                log_main_process(_logger, logging.DEBUG, msg)\\n\\n        if \\\"steps_per_epoch\\\" in sched_config:\\n            if sched_config[\\\"steps_per_epoch\\\"] == -1:\\n                OmegaConf.update(opt_conf, \\\"scheduler.init_args.steps_per_epoch\\\", steps)\\n                msg = f\\\"Set the value of 'steps_per_epoch' to be {steps}.\\\"\\n                log_main_process(_logger, logging.DEBUG, msg)\\n                \\n        if \\\"max_steps\\\" in sched_config:\\n            if sched_config[\\\"max_steps\\\"] == -1:\\n                OmegaConf.update(opt_conf, \\\"scheduler.init_args.max_steps\\\", max_steps)\\n                msg = f\\\"Set the value of 'max_steps' to be {max_steps}.\\\"\\n                log_main_process(_logger, logging.DEBUG, msg)\\n                \\n        return opt_conf\\n    \\n    def setup_optimization(self, conf: DictConfig = None):\\n        \\\"\\\"\\\"\\n        Prepares an optimizer from a string name and its optional config parameters.\\n        You can also manually call this method with a valid optimization config\\n        to setup the optimizers and lr_schedulers.\\n        \\\"\\\"\\\"\\n        if conf is None:\\n            # See if internal config has `optimization` namespace\\n            if self._cfg is not None and hasattr(self._cfg, 'optimization'):\\n                conf = self._cfg.optimization\\n        \\n        opt_conf = conf\\n        \\n        # If config is still None, or internal config has no Optim, return without instantiation\\n        if opt_conf is None:\\n            log_main_process(_logger, logging.WARNING, \\\"No optimization config found,therefore no optimizer was created\\\")\\n            self._optimizer, self._scheduler = None,None\\n        \\n        else:\\n            opt_conf = self.process_optim_config(opt_conf)\\n            self._optimizer = self.build_optimizer(opt_conf, params=self.param_dicts)\\n            self._scheduler = self.build_lr_scheduler(opt_conf, optimizer=self._optimizer)\\n            \\n    def build_optimizer(self, opt_conf: DictConfig, params: Any) -> Any:\\n        \\\"\\\"\\\"\\n        Builds a single optimizer from `opt_conf`. `params` are the parameter\\n        dict with the weights for the optimizer to optimizer.\\n        \\\"\\\"\\\"\\n        if opt_conf.optimizer.name is None:\\n            msg = \\\"Optimizer is None, so no optimizer will be created.\\\"\\n            log_main_process(_logger, logging.WARNING, msg)\\n            return None\\n        else:\\n            opt = opt_conf.optimizer\\n            opt = OPTIM_REGISTRY.get(opt.name)(params=params, **opt.init_args)\\n            msg = \\\"Created optimizer: {}\\\".format(opt.__class__.__name__)\\n            log_main_process(_logger, logging.DEBUG, msg)\\n            return opt\\n    \\n    def build_lr_scheduler(self, opt_conf: DictConfig, optimizer: torch.optim.Optimizer) -> Any:\\n        \\\"\\\"\\\"\\n        Build the Learning Rate scheduler for current task and optimizer.\\n        \\\"\\\"\\\"\\n        # model must have a max_lrs property\\n        # so that this value can be inferred to torch One Cycle Schedulers\\n        max_lrs = self._model.get_lrs()\\n        \\n        if opt_conf.scheduler.name is None:\\n            msg = \\\"scheduler is None, so no scheduler will be created.\\\"\\n            log_main_process(_logger, logging.DEBUG, msg)\\n            return None\\n        \\n        else:\\n            args = opt_conf.scheduler.init_args\\n            d_args = OmegaConf.to_container(args, resolve=True)\\n            kwds = {}\\n            \\n            # if a key value is ListConfig then we convert it to simple list\\n            # also dynamically compute the value of max_lrs\\n            for key, value in d_args.items():\\n                if isinstance(value, list):\\n                    kwds[key] = list(value)     \\n                elif key == \\\"max_lr\\\":\\n                    kwds[\\\"max_lr\\\"] = max_lrs                \\n                else:\\n                    kwds[key] = value\\n            instance = SCHEDULER_REGISTRY.get(opt_conf.scheduler.name)\\n            sch = instance(optimizer=optimizer, **kwds)\\n            \\n            # convert the lr_scheduler to pytorch-lightning LRScheduler dictionary format\\n            msg = \\\"Created lr_scheduler : {}.\\\".format(sch.__class__.__name__)\\n            log_main_process(_logger, logging.DEBUG, msg)\\n             \\n            sch = {\\n                \\\"scheduler\\\": sch, \\n                \\\"interval\\\": opt_conf.scheduler.interval, \\n                \\\"monitor\\\": opt_conf.scheduler.monitor\\n            }\\n            return sch        \\n\\n    def setup_training_data(self, *args, **kwargs) -> None:\\n        \\\"\\\"\\\"\\n        Setups data loader to be used in training\\n        \\\"\\\"\\\"\\n        pass\\n\\n    def setup_validation_data(self, *args, **kwargs) -> None:\\n        \\\"\\\"\\\"\\n        Setups data loader (s) to be used in validation\\n        \\\"\\\"\\\"\\n        pass\\n\\n    def setup_test_data(self, *args, **kwargs) -> None:\\n        \\\"\\\"\\\"\\n        (Optionally) Setups data loader to be used in test\\n        \\\"\\\"\\\"\\n        pass       \\n        \\n    @property\\n    def _is_model_being_restored(self):\\n        \\\"\\\"\\\"\\n        Wether the model is being used for inference of training.\\n        For training it is mandatory to pass in the Training while initializing\\n        the class\\n        \\\"\\\"\\\"\\n        return self.is_restored\\n\\n    @_is_model_being_restored.setter\\n    def _is_model_being_restored(self, x: bool):\\n        self.is_restored = x\\n        \\n    @property\\n    def metrics(self):\\n        \\\"\\\"\\\"\\n        Property that returns the metrics for the current Lightning Task\\n        \\\"\\\"\\\"\\n        return self._metrics\\n    \\n    @metrics.setter\\n    def metrics(self, metrics: Union[torchmetrics.Metric, Mapping, Sequence, None] = None):\\n        self._metrics = setup_metrics(metrics)\\n        \\n    def num_training_steps(self) -> int:\\n        \\\"\\\"\\\"\\n        Total training steps inferred from train dataloader and devices.\\n        \\\"\\\"\\\"\\n        if isinstance(self._trainer.limit_train_batches, int) and self._trainer.limit_train_batches != 0:\\n            dataset_size = self._trainer.limit_train_batches\\n        elif isinstance(self._trainer.limit_train_batches, float):\\n            dataset_size = len(self._train_dl)\\n            dataset_size = int(dataset_size * self._trainer.limit_train_batches)\\n        else:\\n            dataset_size = len(self._train_dl)\\n        \\n        num_devices = max(1, self._trainer.num_gpus, self._trainer.num_processes)\\n        \\n        if self._trainer.tpu_cores:\\n            num_devices = max(num_devices, self._trainer.tpu_cores)\\n        \\n        effective_batch_size = self._trainer.accumulate_grad_batches * num_devices\\n        max_estimated_steps = (dataset_size // effective_batch_size) * self._trainer.max_epochs\\n        \\n        if self._trainer.max_steps and self._trainer.max_steps < max_estimated_steps:\\n            return self._trainer.max_steps\\n        return max_estimated_steps, dataset_size\\n    \\n    @property\\n    def param_dicts(self) -> Union[Iterator, List[Dict]]:\\n        \\\"\\\"\\\"\\n        Property that returns the param dicts for optimization.\\n        Override for custom training behaviour. Currently returns all the trainable paramters.\\n        \\\"\\\"\\\"\\n        return trainable_params(self)\";\n",
       "                var nbb_formatted_code = \"# export\\n# fmt: off\\nclass GaleTask(pl.LightningModule):\\n    \\\"\\\"\\\"\\n    Interface for Pytorch-lightning based Gale modules\\n    \\\"\\\"\\\"\\n    is_restored = True\\n    \\n    def __init__(\\n        self,\\n        cfg: DictConfig,\\n        trainer: Optional[pl.Trainer] = None,\\n        metrics: Union[torchmetrics.Metric, Mapping, Sequence, None] = None,\\n    ):\\n        \\\"\\\"\\\"\\n        Base class from which all PyTorch Lightning Tasks in Gale should inherit.\\n        Provides a few helper functions primarily for optimization.\\n\\n        Arguments:\\n        1. `cfg` `(DictConfig)`:  configuration object. cfg object should be inherited from `BaseGaleConfig`.\\n        2. `trainer` `(Optional, pl.Trainer)`: Pytorch Lightning Trainer instance\\n        3. `metrics` `(Optional)`: Metrics to compute for training and evaluation.\\n        \\\"\\\"\\\"\\n        super().__init__()\\n        self._cfg = OmegaConf.create(cfg)\\n        self._cfg = OmegaConf.structured(cfg)\\n        \\n        if trainer is not None and not isinstance(trainer, pl.Trainer):\\n            msg = f\\\"Trainer constructor argument must be either None or pl.Trainer.But got {type(trainer)} instead.\\\"\\n            raise ValueError(msg) \\n        \\n        self._train_dl = noop\\n        self._validation_dl = noop\\n        self._test_dl = noop\\n        \\n        self._optimizer = noop\\n        self._scheduler = noop\\n        \\n        self._trainer = ifnone(trainer, noop)\\n        self._metrics = setup_metrics(metrics)\\n        self._model = noop \\n        \\n        self.save_hyperparameters(self._cfg)\\n        \\n        # if trained is not passed them the Model is being restored\\n        if self._trainer is not None:\\n            self.is_restored = False\\n        else:\\n            self.is_restored = True\\n               \\n        \\n    @abstractmethod\\n    def forward(self, x: torch.Tensor) -> Any:\\n        \\\"\\\"\\\"\\n        The Forward method for LightningModule, users should modify this method.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n        \\n        \\n    def shared_step(self, batch: Any, batch_idx: int, stage: str) -> Dict:\\n        \\\"\\\"\\\"\\n        The common training/validation/test step. Override for custom behavior. This step\\n        is shared between training/validation/test step. For training/validation/test steps\\n        `stage` is train/val/test respectively. You training logic should go here avoid directly overriding\\n        training/validation/test step methods. This step needs to return a dictionary contatining\\n        the loss to optimize and values to log.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n        \\n    def training_step(self, batch: Any, batch_idx: int) -> Any:\\n        \\\"\\\"\\\"\\n        The training step of the LightningModule. For common use cases you need\\n        not need to override this method. See `GaleTask.shared_step()`\\n        \\\"\\\"\\\"\\n        output = self.shared_step(batch, batch_idx, stage=\\\"train\\\")\\n        self.log_dict({f\\\"train/{k}\\\": v for k, v in output[\\\"logs\\\"].items()})\\n        return output[\\\"loss\\\"]\\n\\n    def validation_step(self, batch: Any, batch_idx: int) -> None:\\n        \\\"\\\"\\\"\\n        The validation step of the LightningModule. For common use cases you need\\n        not need to override this method. See `GaleTask.shared_step()`\\n        \\\"\\\"\\\"\\n        output = self.shared_step(batch, batch_idx, stage=\\\"validation\\\")\\n        self.log_dict({f\\\"val/{k}\\\": v for k, v in output[\\\"logs\\\"].items()})\\n\\n    def test_step(self, batch: Any, batch_idx: int) -> None:\\n        \\\"\\\"\\\"\\n        The test step of the LightningModule. For common use cases you need\\n        not need to override this method. See `GaleTask.shared_step()`\\n        \\\"\\\"\\\"\\n        output = self.shared_step(batch, batch_idx, stage=\\\"test\\\")\\n        self.log_dict({f\\\"test/{k}\\\": v for k, v in output[\\\"logs\\\"].items()})\\n    \\n    def configure_optimizers(self) -> Any:\\n        \\\"\\\"\\\"\\n        Choose what optimizers and learning-rate schedulers to use in your optimization.\\n        See https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html\\n        \\\"\\\"\\\"\\n        # if self.setup_optimization() has been called manually no\\n        # need to call again\\n        if self._optimizer is noop and self._scheduler is noop:\\n            self.setup_optimization()\\n\\n        if self._scheduler is None:\\n            return self._optimizer\\n        else:\\n            return [self._optimizer], [self._scheduler]\\n        \\n    def train_dataloader(self) -> torch.utils.data.DataLoader:\\n        \\\"Returns the Dataloader used for Training\\\"\\n        if self._train_dl is not None and self._train_dl is not noop:\\n            return self._train_dl\\n\\n    def val_dataloader(self) -> Any:\\n        \\\"Returns the List of Dataloaders or Dataloader used for Validation\\\"\\n        if self._validation_dl is not None and self._validation_dl is not noop:\\n            return self._validation_dl\\n\\n    def test_dataloader(self) -> Any:\\n        \\\"Returns the List of Dataloaders or Dataloader used for Testing\\\"\\n        if self._test_dl is not None and self._test_dl is not noop:\\n            return self._test_dl\\n    \\n    def process_optim_config(self, opt_conf: DictConfig) -> DictConfig:\\n        \\\"\\\"\\\"\\n        Prepares an optimizer from a string name and its optional config parameters.\\n        Preprocess the optimization config and adds some infered values like max_steps, max_epochs, etc.\\n        This method also fills in the values for `max_iters` & `epochs`, `steps_per_epoch` if \\n        the values are `-1`\\n        \\\"\\\"\\\"\\n        # some optimizers/schedulers need parameters only known dynamically\\n        # allow users to override the getter to instantiate them lazily\\n        \\n        opt_conf = copy.deepcopy(opt_conf)\\n        \\n        # Force into DictConfig structure\\n        opt_conf = OmegaConf.create(opt_conf)\\n        \\n        if self._trainer.max_epochs is None and self._trainer.max_steps is None:\\n            raise ValueError(\\\"Either one of max_epochs or max_epochs must be provided in Trainer\\\")\\n        else:\\n            max_steps, steps = self.num_training_steps()\\n            max_epochs = ifnone(self._trainer.max_epochs, max_steps // steps)\\n            \\n        vals = dict(steps_per_epoch=steps, max_steps=max_steps, max_epochs=max_epochs)\\n        \\n        # Force into native dictionary\\n        opt_conf = OmegaConf.to_container(opt_conf, resolve=True)\\n        \\n        for key,value in vals.items():\\n            if opt_conf[key] < 1:\\n                opt_conf[key] = value\\n                \\n        # populate values in learning rate schedulers initialization arguments\\n        opt_conf = OmegaConf.create(opt_conf)\\n        sched_config = OmegaConf.to_container(opt_conf.scheduler.init_args, resolve=True)\\n        \\n        # Force into DictConfig structure\\n        opt_conf = OmegaConf.create(opt_conf)\\n        \\n        # @TODO: Find a better way to do this\\n        if \\\"max_iters\\\" in sched_config:\\n            if sched_config[\\\"max_iters\\\"] == -1:\\n                OmegaConf.update(opt_conf, \\\"scheduler.init_args.max_iters\\\", max_steps)\\n                msg = f\\\"Set the value of 'max_iters' to be {max_steps}.\\\"\\n                log_main_process(_logger, logging.DEBUG, msg)\\n\\n        if \\\"epochs\\\" in sched_config:\\n            if sched_config[\\\"epochs\\\"] == -1:\\n                OmegaConf.update(opt_conf, \\\"scheduler.init_args.epochs\\\", max_epochs)\\n                msg = f\\\"Set the value of 'epochs' to be {max_epochs}.\\\"\\n                log_main_process(_logger, logging.DEBUG, msg)\\n\\n        if \\\"steps_per_epoch\\\" in sched_config:\\n            if sched_config[\\\"steps_per_epoch\\\"] == -1:\\n                OmegaConf.update(opt_conf, \\\"scheduler.init_args.steps_per_epoch\\\", steps)\\n                msg = f\\\"Set the value of 'steps_per_epoch' to be {steps}.\\\"\\n                log_main_process(_logger, logging.DEBUG, msg)\\n                \\n        if \\\"max_steps\\\" in sched_config:\\n            if sched_config[\\\"max_steps\\\"] == -1:\\n                OmegaConf.update(opt_conf, \\\"scheduler.init_args.max_steps\\\", max_steps)\\n                msg = f\\\"Set the value of 'max_steps' to be {max_steps}.\\\"\\n                log_main_process(_logger, logging.DEBUG, msg)\\n                \\n        return opt_conf\\n    \\n    def setup_optimization(self, conf: DictConfig = None):\\n        \\\"\\\"\\\"\\n        Prepares an optimizer from a string name and its optional config parameters.\\n        You can also manually call this method with a valid optimization config\\n        to setup the optimizers and lr_schedulers.\\n        \\\"\\\"\\\"\\n        if conf is None:\\n            # See if internal config has `optimization` namespace\\n            if self._cfg is not None and hasattr(self._cfg, 'optimization'):\\n                conf = self._cfg.optimization\\n        \\n        opt_conf = conf\\n        \\n        # If config is still None, or internal config has no Optim, return without instantiation\\n        if opt_conf is None:\\n            log_main_process(_logger, logging.WARNING, \\\"No optimization config found,therefore no optimizer was created\\\")\\n            self._optimizer, self._scheduler = None,None\\n        \\n        else:\\n            opt_conf = self.process_optim_config(opt_conf)\\n            self._optimizer = self.build_optimizer(opt_conf, params=self.param_dicts)\\n            self._scheduler = self.build_lr_scheduler(opt_conf, optimizer=self._optimizer)\\n            \\n    def build_optimizer(self, opt_conf: DictConfig, params: Any) -> Any:\\n        \\\"\\\"\\\"\\n        Builds a single optimizer from `opt_conf`. `params` are the parameter\\n        dict with the weights for the optimizer to optimizer.\\n        \\\"\\\"\\\"\\n        if opt_conf.optimizer.name is None:\\n            msg = \\\"Optimizer is None, so no optimizer will be created.\\\"\\n            log_main_process(_logger, logging.WARNING, msg)\\n            return None\\n        else:\\n            opt = opt_conf.optimizer\\n            opt = OPTIM_REGISTRY.get(opt.name)(params=params, **opt.init_args)\\n            msg = \\\"Created optimizer: {}\\\".format(opt.__class__.__name__)\\n            log_main_process(_logger, logging.DEBUG, msg)\\n            return opt\\n    \\n    def build_lr_scheduler(self, opt_conf: DictConfig, optimizer: torch.optim.Optimizer) -> Any:\\n        \\\"\\\"\\\"\\n        Build the Learning Rate scheduler for current task and optimizer.\\n        \\\"\\\"\\\"\\n        # model must have a max_lrs property\\n        # so that this value can be inferred to torch One Cycle Schedulers\\n        max_lrs = self._model.get_lrs()\\n        \\n        if opt_conf.scheduler.name is None:\\n            msg = \\\"scheduler is None, so no scheduler will be created.\\\"\\n            log_main_process(_logger, logging.DEBUG, msg)\\n            return None\\n        \\n        else:\\n            args = opt_conf.scheduler.init_args\\n            d_args = OmegaConf.to_container(args, resolve=True)\\n            kwds = {}\\n            \\n            # if a key value is ListConfig then we convert it to simple list\\n            # also dynamically compute the value of max_lrs\\n            for key, value in d_args.items():\\n                if isinstance(value, list):\\n                    kwds[key] = list(value)     \\n                elif key == \\\"max_lr\\\":\\n                    kwds[\\\"max_lr\\\"] = max_lrs                \\n                else:\\n                    kwds[key] = value\\n            instance = SCHEDULER_REGISTRY.get(opt_conf.scheduler.name)\\n            sch = instance(optimizer=optimizer, **kwds)\\n            \\n            # convert the lr_scheduler to pytorch-lightning LRScheduler dictionary format\\n            msg = \\\"Created lr_scheduler : {}.\\\".format(sch.__class__.__name__)\\n            log_main_process(_logger, logging.DEBUG, msg)\\n             \\n            sch = {\\n                \\\"scheduler\\\": sch, \\n                \\\"interval\\\": opt_conf.scheduler.interval, \\n                \\\"monitor\\\": opt_conf.scheduler.monitor\\n            }\\n            return sch        \\n\\n    def setup_training_data(self, *args, **kwargs) -> None:\\n        \\\"\\\"\\\"\\n        Setups data loader to be used in training\\n        \\\"\\\"\\\"\\n        pass\\n\\n    def setup_validation_data(self, *args, **kwargs) -> None:\\n        \\\"\\\"\\\"\\n        Setups data loader (s) to be used in validation\\n        \\\"\\\"\\\"\\n        pass\\n\\n    def setup_test_data(self, *args, **kwargs) -> None:\\n        \\\"\\\"\\\"\\n        (Optionally) Setups data loader to be used in test\\n        \\\"\\\"\\\"\\n        pass       \\n        \\n    @property\\n    def _is_model_being_restored(self):\\n        \\\"\\\"\\\"\\n        Wether the model is being used for inference of training.\\n        For training it is mandatory to pass in the Training while initializing\\n        the class\\n        \\\"\\\"\\\"\\n        return self.is_restored\\n\\n    @_is_model_being_restored.setter\\n    def _is_model_being_restored(self, x: bool):\\n        self.is_restored = x\\n        \\n    @property\\n    def metrics(self):\\n        \\\"\\\"\\\"\\n        Property that returns the metrics for the current Lightning Task\\n        \\\"\\\"\\\"\\n        return self._metrics\\n    \\n    @metrics.setter\\n    def metrics(self, metrics: Union[torchmetrics.Metric, Mapping, Sequence, None] = None):\\n        self._metrics = setup_metrics(metrics)\\n        \\n    def num_training_steps(self) -> int:\\n        \\\"\\\"\\\"\\n        Total training steps inferred from train dataloader and devices.\\n        \\\"\\\"\\\"\\n        if isinstance(self._trainer.limit_train_batches, int) and self._trainer.limit_train_batches != 0:\\n            dataset_size = self._trainer.limit_train_batches\\n        elif isinstance(self._trainer.limit_train_batches, float):\\n            dataset_size = len(self._train_dl)\\n            dataset_size = int(dataset_size * self._trainer.limit_train_batches)\\n        else:\\n            dataset_size = len(self._train_dl)\\n        \\n        num_devices = max(1, self._trainer.num_gpus, self._trainer.num_processes)\\n        \\n        if self._trainer.tpu_cores:\\n            num_devices = max(num_devices, self._trainer.tpu_cores)\\n        \\n        effective_batch_size = self._trainer.accumulate_grad_batches * num_devices\\n        max_estimated_steps = (dataset_size // effective_batch_size) * self._trainer.max_epochs\\n        \\n        if self._trainer.max_steps and self._trainer.max_steps < max_estimated_steps:\\n            return self._trainer.max_steps\\n        return max_estimated_steps, dataset_size\\n    \\n    @property\\n    def param_dicts(self) -> Union[Iterator, List[Dict]]:\\n        \\\"\\\"\\\"\\n        Property that returns the param dicts for optimization.\\n        Override for custom training behaviour. Currently returns all the trainable paramters.\\n        \\\"\\\"\\\"\\n        return trainable_params(self)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "# fmt: off\n",
    "class GaleTask(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Interface for Pytorch-lightning based Gale modules\n",
    "    \"\"\"\n",
    "    is_restored = True\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: DictConfig,\n",
    "        trainer: Optional[pl.Trainer] = None,\n",
    "        metrics: Union[torchmetrics.Metric, Mapping, Sequence, None] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Base class from which all PyTorch Lightning Tasks in Gale should inherit.\n",
    "        Provides a few helper functions primarily for optimization.\n",
    "\n",
    "        Arguments:\n",
    "        1. `cfg` `(DictConfig)`:  configuration object. cfg object should be inherited from `BaseGaleConfig`.\n",
    "        2. `trainer` `(Optional, pl.Trainer)`: Pytorch Lightning Trainer instance\n",
    "        3. `metrics` `(Optional)`: Metrics to compute for training and evaluation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._cfg = OmegaConf.create(cfg)\n",
    "        self._cfg = OmegaConf.structured(cfg)\n",
    "        \n",
    "        if trainer is not None and not isinstance(trainer, pl.Trainer):\n",
    "            msg = f\"Trainer constructor argument must be either None or pl.Trainer.But got {type(trainer)} instead.\"\n",
    "            raise ValueError(msg) \n",
    "        \n",
    "        self._train_dl = noop\n",
    "        self._validation_dl = noop\n",
    "        self._test_dl = noop\n",
    "        \n",
    "        self._optimizer = noop\n",
    "        self._scheduler = noop\n",
    "        \n",
    "        self._trainer = ifnone(trainer, noop)\n",
    "        self._metrics = setup_metrics(metrics)\n",
    "        self._model = noop \n",
    "        \n",
    "        self.save_hyperparameters(self._cfg)\n",
    "        \n",
    "        # if trained is not passed them the Model is being restored\n",
    "        if self._trainer is not None:\n",
    "            self.is_restored = False\n",
    "        else:\n",
    "            self.is_restored = True\n",
    "               \n",
    "        \n",
    "    @abstractmethod\n",
    "    def forward(self, x: torch.Tensor) -> Any:\n",
    "        \"\"\"\n",
    "        The Forward method for LightningModule, users should modify this method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        \n",
    "    def shared_step(self, batch: Any, batch_idx: int, stage: str) -> Dict:\n",
    "        \"\"\"\n",
    "        The common training/validation/test step. Override for custom behavior. This step\n",
    "        is shared between training/validation/test step. For training/validation/test steps\n",
    "        `stage` is train/val/test respectively. You training logic should go here avoid directly overriding\n",
    "        training/validation/test step methods. This step needs to return a dictionary contatining\n",
    "        the loss to optimize and values to log.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def training_step(self, batch: Any, batch_idx: int) -> Any:\n",
    "        \"\"\"\n",
    "        The training step of the LightningModule. For common use cases you need\n",
    "        not need to override this method. See `GaleTask.shared_step()`\n",
    "        \"\"\"\n",
    "        output = self.shared_step(batch, batch_idx, stage=\"train\")\n",
    "        self.log_dict({f\"train/{k}\": v for k, v in output[\"logs\"].items()})\n",
    "        return output[\"loss\"]\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int) -> None:\n",
    "        \"\"\"\n",
    "        The validation step of the LightningModule. For common use cases you need\n",
    "        not need to override this method. See `GaleTask.shared_step()`\n",
    "        \"\"\"\n",
    "        output = self.shared_step(batch, batch_idx, stage=\"validation\")\n",
    "        self.log_dict({f\"val/{k}\": v for k, v in output[\"logs\"].items()})\n",
    "\n",
    "    def test_step(self, batch: Any, batch_idx: int) -> None:\n",
    "        \"\"\"\n",
    "        The test step of the LightningModule. For common use cases you need\n",
    "        not need to override this method. See `GaleTask.shared_step()`\n",
    "        \"\"\"\n",
    "        output = self.shared_step(batch, batch_idx, stage=\"test\")\n",
    "        self.log_dict({f\"test/{k}\": v for k, v in output[\"logs\"].items()})\n",
    "    \n",
    "    def configure_optimizers(self) -> Any:\n",
    "        \"\"\"\n",
    "        Choose what optimizers and learning-rate schedulers to use in your optimization.\n",
    "        See https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html\n",
    "        \"\"\"\n",
    "        # if self.setup_optimization() has been called manually no\n",
    "        # need to call again\n",
    "        if self._optimizer is noop and self._scheduler is noop:\n",
    "            self.setup_optimization()\n",
    "\n",
    "        if self._scheduler is None:\n",
    "            return self._optimizer\n",
    "        else:\n",
    "            return [self._optimizer], [self._scheduler]\n",
    "        \n",
    "    def train_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        \"Returns the Dataloader used for Training\"\n",
    "        if self._train_dl is not None and self._train_dl is not noop:\n",
    "            return self._train_dl\n",
    "\n",
    "    def val_dataloader(self) -> Any:\n",
    "        \"Returns the List of Dataloaders or Dataloader used for Validation\"\n",
    "        if self._validation_dl is not None and self._validation_dl is not noop:\n",
    "            return self._validation_dl\n",
    "\n",
    "    def test_dataloader(self) -> Any:\n",
    "        \"Returns the List of Dataloaders or Dataloader used for Testing\"\n",
    "        if self._test_dl is not None and self._test_dl is not noop:\n",
    "            return self._test_dl\n",
    "    \n",
    "    def process_optim_config(self, opt_conf: DictConfig) -> DictConfig:\n",
    "        \"\"\"\n",
    "        Prepares an optimizer from a string name and its optional config parameters.\n",
    "        Preprocess the optimization config and adds some infered values like max_steps, max_epochs, etc.\n",
    "        This method also fills in the values for `max_iters` & `epochs`, `steps_per_epoch` if \n",
    "        the values are `-1`\n",
    "        \"\"\"\n",
    "        # some optimizers/schedulers need parameters only known dynamically\n",
    "        # allow users to override the getter to instantiate them lazily\n",
    "        \n",
    "        opt_conf = copy.deepcopy(opt_conf)\n",
    "        \n",
    "        # Force into DictConfig structure\n",
    "        opt_conf = OmegaConf.create(opt_conf)\n",
    "        \n",
    "        if self._trainer.max_epochs is None and self._trainer.max_steps is None:\n",
    "            raise ValueError(\"Either one of max_epochs or max_epochs must be provided in Trainer\")\n",
    "        else:\n",
    "            max_steps, steps = self.num_training_steps()\n",
    "            max_epochs = ifnone(self._trainer.max_epochs, max_steps // steps)\n",
    "            \n",
    "        vals = dict(steps_per_epoch=steps, max_steps=max_steps, max_epochs=max_epochs)\n",
    "        \n",
    "        # Force into native dictionary\n",
    "        opt_conf = OmegaConf.to_container(opt_conf, resolve=True)\n",
    "        \n",
    "        for key,value in vals.items():\n",
    "            if opt_conf[key] < 1:\n",
    "                opt_conf[key] = value\n",
    "                \n",
    "        # populate values in learning rate schedulers initialization arguments\n",
    "        opt_conf = OmegaConf.create(opt_conf)\n",
    "        sched_config = OmegaConf.to_container(opt_conf.scheduler.init_args, resolve=True)\n",
    "        \n",
    "        # Force into DictConfig structure\n",
    "        opt_conf = OmegaConf.create(opt_conf)\n",
    "        \n",
    "        # @TODO: Find a better way to do this\n",
    "        if \"max_iters\" in sched_config:\n",
    "            if sched_config[\"max_iters\"] == -1:\n",
    "                OmegaConf.update(opt_conf, \"scheduler.init_args.max_iters\", max_steps)\n",
    "                msg = f\"Set the value of 'max_iters' to be {max_steps}.\"\n",
    "                log_main_process(_logger, logging.DEBUG, msg)\n",
    "\n",
    "        if \"epochs\" in sched_config:\n",
    "            if sched_config[\"epochs\"] == -1:\n",
    "                OmegaConf.update(opt_conf, \"scheduler.init_args.epochs\", max_epochs)\n",
    "                msg = f\"Set the value of 'epochs' to be {max_epochs}.\"\n",
    "                log_main_process(_logger, logging.DEBUG, msg)\n",
    "\n",
    "        if \"steps_per_epoch\" in sched_config:\n",
    "            if sched_config[\"steps_per_epoch\"] == -1:\n",
    "                OmegaConf.update(opt_conf, \"scheduler.init_args.steps_per_epoch\", steps)\n",
    "                msg = f\"Set the value of 'steps_per_epoch' to be {steps}.\"\n",
    "                log_main_process(_logger, logging.DEBUG, msg)\n",
    "                \n",
    "        if \"max_steps\" in sched_config:\n",
    "            if sched_config[\"max_steps\"] == -1:\n",
    "                OmegaConf.update(opt_conf, \"scheduler.init_args.max_steps\", max_steps)\n",
    "                msg = f\"Set the value of 'max_steps' to be {max_steps}.\"\n",
    "                log_main_process(_logger, logging.DEBUG, msg)\n",
    "                \n",
    "        return opt_conf\n",
    "    \n",
    "    def setup_optimization(self, conf: DictConfig = None):\n",
    "        \"\"\"\n",
    "        Prepares an optimizer from a string name and its optional config parameters.\n",
    "        You can also manually call this method with a valid optimization config\n",
    "        to setup the optimizers and lr_schedulers.\n",
    "        \"\"\"\n",
    "        if conf is None:\n",
    "            # See if internal config has `optimization` namespace\n",
    "            if self._cfg is not None and hasattr(self._cfg, 'optimization'):\n",
    "                conf = self._cfg.optimization\n",
    "        \n",
    "        opt_conf = conf\n",
    "        \n",
    "        # If config is still None, or internal config has no Optim, return without instantiation\n",
    "        if opt_conf is None:\n",
    "            log_main_process(_logger, logging.WARNING, \"No optimization config found,therefore no optimizer was created\")\n",
    "            self._optimizer, self._scheduler = None,None\n",
    "        \n",
    "        else:\n",
    "            opt_conf = self.process_optim_config(opt_conf)\n",
    "            self._optimizer = self.build_optimizer(opt_conf, params=self.param_dicts)\n",
    "            self._scheduler = self.build_lr_scheduler(opt_conf, optimizer=self._optimizer)\n",
    "            \n",
    "    def build_optimizer(self, opt_conf: DictConfig, params: Any) -> Any:\n",
    "        \"\"\"\n",
    "        Builds a single optimizer from `opt_conf`. `params` are the parameter\n",
    "        dict with the weights for the optimizer to optimizer.\n",
    "        \"\"\"\n",
    "        if opt_conf.optimizer.name is None:\n",
    "            msg = \"Optimizer is None, so no optimizer will be created.\"\n",
    "            log_main_process(_logger, logging.WARNING, msg)\n",
    "            return None\n",
    "        else:\n",
    "            opt = opt_conf.optimizer\n",
    "            opt = OPTIM_REGISTRY.get(opt.name)(params=params, **opt.init_args)\n",
    "            msg = \"Created optimizer: {}\".format(opt.__class__.__name__)\n",
    "            log_main_process(_logger, logging.DEBUG, msg)\n",
    "            return opt\n",
    "    \n",
    "    def build_lr_scheduler(self, opt_conf: DictConfig, optimizer: torch.optim.Optimizer) -> Any:\n",
    "        \"\"\"\n",
    "        Build the Learning Rate scheduler for current task and optimizer.\n",
    "        \"\"\"\n",
    "        # model must have a max_lrs property\n",
    "        # so that this value can be inferred to torch One Cycle Schedulers\n",
    "        max_lrs = self._model.get_lrs()\n",
    "        \n",
    "        if opt_conf.scheduler.name is None:\n",
    "            msg = \"scheduler is None, so no scheduler will be created.\"\n",
    "            log_main_process(_logger, logging.DEBUG, msg)\n",
    "            return None\n",
    "        \n",
    "        else:\n",
    "            args = opt_conf.scheduler.init_args\n",
    "            d_args = OmegaConf.to_container(args, resolve=True)\n",
    "            kwds = {}\n",
    "            \n",
    "            # if a key value is ListConfig then we convert it to simple list\n",
    "            # also dynamically compute the value of max_lrs\n",
    "            for key, value in d_args.items():\n",
    "                if isinstance(value, list):\n",
    "                    kwds[key] = list(value)     \n",
    "                elif key == \"max_lr\":\n",
    "                    kwds[\"max_lr\"] = max_lrs                \n",
    "                else:\n",
    "                    kwds[key] = value\n",
    "            instance = SCHEDULER_REGISTRY.get(opt_conf.scheduler.name)\n",
    "            sch = instance(optimizer=optimizer, **kwds)\n",
    "            \n",
    "            # convert the lr_scheduler to pytorch-lightning LRScheduler dictionary format\n",
    "            msg = \"Created lr_scheduler : {}.\".format(sch.__class__.__name__)\n",
    "            log_main_process(_logger, logging.DEBUG, msg)\n",
    "             \n",
    "            sch = {\n",
    "                \"scheduler\": sch, \n",
    "                \"interval\": opt_conf.scheduler.interval, \n",
    "                \"monitor\": opt_conf.scheduler.monitor\n",
    "            }\n",
    "            return sch        \n",
    "\n",
    "    def setup_training_data(self, *args, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Setups data loader to be used in training\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def setup_validation_data(self, *args, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Setups data loader (s) to be used in validation\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def setup_test_data(self, *args, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        (Optionally) Setups data loader to be used in test\n",
    "        \"\"\"\n",
    "        pass       \n",
    "        \n",
    "    @property\n",
    "    def _is_model_being_restored(self):\n",
    "        \"\"\"\n",
    "        Wether the model is being used for inference of training.\n",
    "        For training it is mandatory to pass in the Training while initializing\n",
    "        the class\n",
    "        \"\"\"\n",
    "        return self.is_restored\n",
    "\n",
    "    @_is_model_being_restored.setter\n",
    "    def _is_model_being_restored(self, x: bool):\n",
    "        self.is_restored = x\n",
    "        \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        \"\"\"\n",
    "        Property that returns the metrics for the current Lightning Task\n",
    "        \"\"\"\n",
    "        return self._metrics\n",
    "    \n",
    "    @metrics.setter\n",
    "    def metrics(self, metrics: Union[torchmetrics.Metric, Mapping, Sequence, None] = None):\n",
    "        self._metrics = setup_metrics(metrics)\n",
    "        \n",
    "    def num_training_steps(self) -> int:\n",
    "        \"\"\"\n",
    "        Total training steps inferred from train dataloader and devices.\n",
    "        \"\"\"\n",
    "        if isinstance(self._trainer.limit_train_batches, int) and self._trainer.limit_train_batches != 0:\n",
    "            dataset_size = self._trainer.limit_train_batches\n",
    "        elif isinstance(self._trainer.limit_train_batches, float):\n",
    "            dataset_size = len(self._train_dl)\n",
    "            dataset_size = int(dataset_size * self._trainer.limit_train_batches)\n",
    "        else:\n",
    "            dataset_size = len(self._train_dl)\n",
    "        \n",
    "        num_devices = max(1, self._trainer.num_gpus, self._trainer.num_processes)\n",
    "        \n",
    "        if self._trainer.tpu_cores:\n",
    "            num_devices = max(num_devices, self._trainer.tpu_cores)\n",
    "        \n",
    "        effective_batch_size = self._trainer.accumulate_grad_batches * num_devices\n",
    "        max_estimated_steps = (dataset_size // effective_batch_size) * self._trainer.max_epochs\n",
    "        \n",
    "        if self._trainer.max_steps and self._trainer.max_steps < max_estimated_steps:\n",
    "            return self._trainer.max_steps\n",
    "        return max_estimated_steps, dataset_size\n",
    "    \n",
    "    @property\n",
    "    def param_dicts(self) -> Union[Iterator, List[Dict]]:\n",
    "        \"\"\"\n",
    "        Property that returns the param dicts for optimization.\n",
    "        Override for custom training behaviour. Currently returns all the trainable paramters.\n",
    "        \"\"\"\n",
    "        return trainable_params(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.metrics\" class=\"doc_header\"><code>GaleTask.metrics</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "Property that returns the metrics for the current Lightning Task"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.metrics)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.metrics)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.param_dicts\" class=\"doc_header\"><code>GaleTask.param_dicts</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "Property that returns the param dicts for optimization.\n",
       "Override for custom training behaviour. Currently returns all the trainable paramters."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 21;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.param_dicts)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.param_dicts)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.param_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask._is_model_being_restored\" class=\"doc_header\"><code>GaleTask._is_model_being_restored</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "Wether the model is being used for inference of training.\n",
       "For training it is mandatory to pass in the Training while initializing\n",
       "the class"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 22;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask._is_model_being_restored)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask._is_model_being_restored)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask._is_model_being_restored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.forward\" class=\"doc_header\"><code>GaleTask.forward</code><a href=\"__main__.py#L52\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.forward</code>(**`x`**:`Tensor`)\n",
       "\n",
       "The Forward method for LightningModule, users should modify this method."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 23;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.forward)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.forward)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.num_training_steps\" class=\"doc_header\"><code>GaleTask.num_training_steps</code><a href=\"__main__.py#L311\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.num_training_steps</code>()\n",
       "\n",
       "Total training steps inferred from train dataloader and devices."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 24;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.num_training_steps)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.num_training_steps)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.shared_step\" class=\"doc_header\"><code>GaleTask.shared_step</code><a href=\"__main__.py#L60\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.shared_step</code>(**`batch`**:`Any`, **`batch_idx`**:`int`, **`stage`**:`str`)\n",
       "\n",
       "The common training/validation/test step. Override for custom behavior. This step\n",
       "is shared between training/validation/test step. For training/validation/test steps\n",
       "`stage` is train/val/test respectively. You training logic should go here avoid directly overriding\n",
       "training/validation/test step methods. This step needs to return a dictionary contatining\n",
       "the loss to optimize and values to log."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 25;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.shared_step)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.shared_step)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.shared_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.training_step\" class=\"doc_header\"><code>GaleTask.training_step</code><a href=\"__main__.py#L70\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.training_step</code>(**`batch`**:`Any`, **`batch_idx`**:`int`)\n",
       "\n",
       "The training step of the LightningModule. For common use cases you need\n",
       "not need to override this method. See [`GaleTask.shared_step()`](/gale/core.classes.html#GaleTask.shared_step())"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 26;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.training_step)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.training_step)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.training_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.validation_step\" class=\"doc_header\"><code>GaleTask.validation_step</code><a href=\"__main__.py#L79\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.validation_step</code>(**`batch`**:`Any`, **`batch_idx`**:`int`)\n",
       "\n",
       "The validation step of the LightningModule. For common use cases you need\n",
       "not need to override this method. See [`GaleTask.shared_step()`](/gale/core.classes.html#GaleTask.shared_step())"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 27;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.validation_step)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.validation_step)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.validation_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.test_step\" class=\"doc_header\"><code>GaleTask.test_step</code><a href=\"__main__.py#L87\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.test_step</code>(**`batch`**:`Any`, **`batch_idx`**:`int`)\n",
       "\n",
       "The test step of the LightningModule. For common use cases you need\n",
       "not need to override this method. See [`GaleTask.shared_step()`](/gale/core.classes.html#GaleTask.shared_step())"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 28;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.test_step)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.test_step)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.test_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.configure_optimizers\" class=\"doc_header\"><code>GaleTask.configure_optimizers</code><a href=\"__main__.py#L95\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.configure_optimizers</code>()\n",
       "\n",
       "Choose what optimizers and learning-rate schedulers to use in your optimization.\n",
       "See https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 29;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.configure_optimizers)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.configure_optimizers)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.configure_optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.process_optim_config\" class=\"doc_header\"><code>GaleTask.process_optim_config</code><a href=\"__main__.py#L125\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.process_optim_config</code>(**`opt_conf`**:`DictConfig`)\n",
       "\n",
       "Prepares an optimizer from a string name and its optional config parameters.\n",
       "Preprocess the optimization config and adds some infered values like max_steps, max_epochs, etc.\n",
       "This method also fills in the values for `max_iters` & `epochs`, `steps_per_epoch` if \n",
       "the values are `-1`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 30;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.process_optim_config)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.process_optim_config)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.process_optim_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.setup_optimization\" class=\"doc_header\"><code>GaleTask.setup_optimization</code><a href=\"__main__.py#L189\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.setup_optimization</code>(**`conf`**:`DictConfig`=*`None`*)\n",
       "\n",
       "Prepares an optimizer from a string name and its optional config parameters.\n",
       "You can also manually call this method with a valid optimization config\n",
       "to setup the optimizers and lr_schedulers."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 31;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.setup_optimization)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.setup_optimization)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.setup_optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.build_optimizer\" class=\"doc_header\"><code>GaleTask.build_optimizer</code><a href=\"__main__.py#L212\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.build_optimizer</code>(**`opt_conf`**:`DictConfig`, **`params`**:`Any`)\n",
       "\n",
       "Builds a single optimizer from `opt_conf`. `params` are the parameter\n",
       "dict with the weights for the optimizer to optimizer."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 32;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.build_optimizer)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.build_optimizer)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.build_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.build_lr_scheduler\" class=\"doc_header\"><code>GaleTask.build_lr_scheduler</code><a href=\"__main__.py#L228\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.build_lr_scheduler</code>(**`opt_conf`**:`DictConfig`, **`optimizer`**:`Optimizer`)\n",
       "\n",
       "Build the Learning Rate scheduler for current task and optimizer."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 33;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.build_lr_scheduler)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.build_lr_scheduler)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.build_lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.train_dataloader\" class=\"doc_header\"><code>GaleTask.train_dataloader</code><a href=\"__main__.py#L110\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.train_dataloader</code>()\n",
       "\n",
       "Returns the Dataloader used for Training"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 34;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.train_dataloader)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.train_dataloader)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.val_dataloader\" class=\"doc_header\"><code>GaleTask.val_dataloader</code><a href=\"__main__.py#L115\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.val_dataloader</code>()\n",
       "\n",
       "Returns the List of Dataloaders or Dataloader used for Validation"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 35;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.val_dataloader)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.val_dataloader)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.test_dataloader\" class=\"doc_header\"><code>GaleTask.test_dataloader</code><a href=\"__main__.py#L120\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.test_dataloader</code>()\n",
       "\n",
       "Returns the List of Dataloaders or Dataloader used for Testing"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 36;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.test_dataloader)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.test_dataloader)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.setup_training_data\" class=\"doc_header\"><code>GaleTask.setup_training_data</code><a href=\"__main__.py#L269\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.setup_training_data</code>(**\\*`args`**, **\\*\\*`kwargs`**)\n",
       "\n",
       "Setups data loader to be used in training"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 37;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.setup_training_data)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.setup_training_data)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.setup_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.setup_validation_data\" class=\"doc_header\"><code>GaleTask.setup_validation_data</code><a href=\"__main__.py#L275\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.setup_validation_data</code>(**\\*`args`**, **\\*\\*`kwargs`**)\n",
       "\n",
       "Setups data loader (s) to be used in validation"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 38;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.setup_validation_data)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.setup_validation_data)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.setup_validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.setup_test_data\" class=\"doc_header\"><code>GaleTask.setup_test_data</code><a href=\"__main__.py#L281\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.setup_test_data</code>(**\\*`args`**, **\\*\\*`kwargs`**)\n",
       "\n",
       "(Optionally) Setups data loader to be used in test"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 39;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.setup_test_data)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.setup_test_data)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.setup_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.utils.logger.ipynb.\n",
      "Converted 00a_core.utils.visualize.ipynb.\n",
      "Converted 00b_core.utils.structures.ipynb.\n",
      "Converted 01_core.nn.utils.ipynb.\n",
      "Converted 01a_core.nn.losses.ipynb.\n",
      "Converted 02_core.nn.optim.optimizers.ipynb.\n",
      "Converted 02a_core.nn.optim.lr_schedulers.ipynb.\n",
      "Converted 03_core.classes.ipynb.\n",
      "Converted 04_classification.modelling.backbones.ipynb.\n",
      "Converted 04a_classification.modelling.heads.ipynb.\n",
      "Converted 04b_classification.modelling.meta_arch.common.ipynb.\n",
      "Converted 04b_classification.modelling.meta_arch.vit.ipynb.\n",
      "Converted 05_classification.data.common.ipynb.\n",
      "Converted 05a_classification.data.transforms.ipynb.\n",
      "Converted 05b_classification.data.build.ipynb.\n",
      "Converted 06_classification.task.ipynb.\n",
      "Converted 07_collections.pandas.ipynb.\n",
      "Converted 07a_collections.callbacks.notebook.ipynb.\n",
      "Converted 07b_collections.callbacks.ema.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 40;\n",
       "                var nbb_unformatted_code = \"# hide\\nnotebook2script()\";\n",
       "                var nbb_formatted_code = \"# hide\\nnotebook2script()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gale_dev",
   "language": "python",
   "name": "gale_dev"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
