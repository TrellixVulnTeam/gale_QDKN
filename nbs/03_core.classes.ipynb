{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# skip\n",
    "!git clone https://github.com/benihime91/gale # install gale on colab\n",
    "!pip install -e \"gale[dev]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# hide\\n%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\\n%matplotlib inline\";\n",
       "                var nbb_formatted_code = \"# hide\\n%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\\n%matplotlib inline\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "%load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# hide\\nimport warnings\\n\\nfrom nbdev.export import *\\nfrom nbdev.showdoc import *\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\";\n",
       "                var nbb_formatted_code = \"# hide\\nimport warnings\\n\\nfrom nbdev.export import *\\nfrom nbdev.showdoc import *\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "import warnings\n",
    "\n",
    "from nbdev.export import *\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes\n",
    "> Interfaces common to all `Modules` and `Models` in Gale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# export\\nimport copy\\nimport logging\\nimport math\\nfrom abc import ABC, ABCMeta, abstractmethod\\nfrom contextlib import contextmanager\\nfrom typing import *\\n\\nimport hydra\\nimport pytorch_lightning as pl\\nimport torch\\nimport torchmetrics\\nfrom fastcore.all import L, noop, patch\\nfrom omegaconf import DictConfig, OmegaConf\\nfrom torch.nn import Module\\n\\nfrom gale.core.nn.optim import OPTIM_REGISTRY, SCHEDULER_REGISTRY\\nfrom gale.core.nn.utils import params, trainable_params\\nfrom gale.core.utils.logger import log_main_process\\n\\n_logger = logging.getLogger(__name__)\";\n",
       "                var nbb_formatted_code = \"# export\\nimport copy\\nimport logging\\nimport math\\nfrom abc import ABC, ABCMeta, abstractmethod\\nfrom contextlib import contextmanager\\nfrom typing import *\\n\\nimport hydra\\nimport pytorch_lightning as pl\\nimport torch\\nimport torchmetrics\\nfrom fastcore.all import L, noop, patch\\nfrom omegaconf import DictConfig, OmegaConf\\nfrom torch.nn import Module\\n\\nfrom gale.core.nn.optim import OPTIM_REGISTRY, SCHEDULER_REGISTRY\\nfrom gale.core.nn.utils import params, trainable_params\\nfrom gale.core.utils.logger import log_main_process\\n\\n_logger = logging.getLogger(__name__)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "import copy\n",
    "import logging\n",
    "import math\n",
    "from abc import ABC, ABCMeta, abstractmethod\n",
    "from contextlib import contextmanager\n",
    "from typing import *\n",
    "\n",
    "import hydra\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchmetrics\n",
    "from fastcore.all import L, noop, patch\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from torch.nn import Module\n",
    "\n",
    "from gale.core.nn.optim import OPTIM_REGISTRY, SCHEDULER_REGISTRY\n",
    "from gale.core.nn.utils import params, trainable_params\n",
    "from gale.core.utils.logger import log_main_process\n",
    "\n",
    "_logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurable-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# export\\nclass Configurable(ABC):\\n    \\\"\\\"\\\"\\n    Helper Class to instantiate obj from config\\n    \\\"\\\"\\\"\\n\\n    @classmethod\\n    def from_config_dict(cls, config: DictConfig, **kwargs):\\n        \\\"\\\"\\\"\\n        Instantiates object using `DictConfig-based` configuration. You can optionally\\n        pass in extra `kwargs`\\n        \\\"\\\"\\\"\\n        # Resolve the config dict\\n        if isinstance(config, DictConfig):\\n            config = OmegaConf.to_container(config, resolve=True)\\n            config = OmegaConf.create(config)\\n\\n        if \\\"_target_\\\" in config:\\n            # regular hydra-based instantiation\\n            instance = hydra.utils.instantiate(config=config, **kwargs)\\n        else:\\n            # instantiate directly using kwargs\\n            try:\\n                instance = cls(cfg=config, **kwargs)\\n            except:\\n                cfg = OmegaConf.to_container(config, resolve=True)\\n                instance = cls(**config, **kwargs)\\n\\n        if not hasattr(instance, \\\"_cfg\\\"):\\n            instance._cfg = config\\n        return instance\\n\\n    def to_config_dict(self) -> DictConfig:\\n        # fmt: off\\n        \\\"\\\"\\\"Returns object's configuration to config dictionary\\\"\\\"\\\"\\n        if hasattr(self, \\\"_cfg\\\") and self._cfg is not None and isinstance(self._cfg, DictConfig):\\n            # Resolve the config dict\\n            config = OmegaConf.to_container(self._cfg, resolve=True)\\n            config = OmegaConf.create(config)\\n            OmegaConf.set_struct(config, True)\\n            self._cfg = config\\n\\n            return self._cfg\\n        else:\\n            raise NotImplementedError(\\\"to_config_dict() can currently only return object._cfg but current object does not have it.\\\")\\n        # fmt: on\";\n",
       "                var nbb_formatted_code = \"# export\\nclass Configurable(ABC):\\n    \\\"\\\"\\\"\\n    Helper Class to instantiate obj from config\\n    \\\"\\\"\\\"\\n\\n    @classmethod\\n    def from_config_dict(cls, config: DictConfig, **kwargs):\\n        \\\"\\\"\\\"\\n        Instantiates object using `DictConfig-based` configuration. You can optionally\\n        pass in extra `kwargs`\\n        \\\"\\\"\\\"\\n        # Resolve the config dict\\n        if isinstance(config, DictConfig):\\n            config = OmegaConf.to_container(config, resolve=True)\\n            config = OmegaConf.create(config)\\n\\n        if \\\"_target_\\\" in config:\\n            # regular hydra-based instantiation\\n            instance = hydra.utils.instantiate(config=config, **kwargs)\\n        else:\\n            # instantiate directly using kwargs\\n            try:\\n                instance = cls(cfg=config, **kwargs)\\n            except:\\n                cfg = OmegaConf.to_container(config, resolve=True)\\n                instance = cls(**config, **kwargs)\\n\\n        if not hasattr(instance, \\\"_cfg\\\"):\\n            instance._cfg = config\\n        return instance\\n\\n    def to_config_dict(self) -> DictConfig:\\n        # fmt: off\\n        \\\"\\\"\\\"Returns object's configuration to config dictionary\\\"\\\"\\\"\\n        if hasattr(self, \\\"_cfg\\\") and self._cfg is not None and isinstance(self._cfg, DictConfig):\\n            # Resolve the config dict\\n            config = OmegaConf.to_container(self._cfg, resolve=True)\\n            config = OmegaConf.create(config)\\n            OmegaConf.set_struct(config, True)\\n            self._cfg = config\\n\\n            return self._cfg\\n        else:\\n            raise NotImplementedError(\\\"to_config_dict() can currently only return object._cfg but current object does not have it.\\\")\\n        # fmt: on\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class Configurable(ABC):\n",
    "    \"\"\"\n",
    "    Helper Class to instantiate obj from config\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_config_dict(cls, config: DictConfig, **kwargs):\n",
    "        \"\"\"\n",
    "        Instantiates object using `DictConfig-based` configuration. You can optionally\n",
    "        pass in extra `kwargs`\n",
    "        \"\"\"\n",
    "        # Resolve the config dict\n",
    "        if isinstance(config, DictConfig):\n",
    "            config = OmegaConf.to_container(config, resolve=True)\n",
    "            config = OmegaConf.create(config)\n",
    "\n",
    "        if \"_target_\" in config:\n",
    "            # regular hydra-based instantiation\n",
    "            instance = hydra.utils.instantiate(config=config, **kwargs)\n",
    "        else:\n",
    "            # instantiate directly using kwargs\n",
    "            try:\n",
    "                instance = cls(cfg=config, **kwargs)\n",
    "            except:\n",
    "                cfg = OmegaConf.to_container(config, resolve=True)\n",
    "                instance = cls(**config, **kwargs)\n",
    "\n",
    "        if not hasattr(instance, \"_cfg\"):\n",
    "            instance._cfg = config\n",
    "        return instance\n",
    "\n",
    "    def to_config_dict(self) -> DictConfig:\n",
    "        # fmt: off\n",
    "        \"\"\"Returns object's configuration to config dictionary\"\"\"\n",
    "        if hasattr(self, \"_cfg\") and self._cfg is not None and isinstance(self._cfg, DictConfig):\n",
    "            # Resolve the config dict\n",
    "            config = OmegaConf.to_container(self._cfg, resolve=True)\n",
    "            config = OmegaConf.create(config)\n",
    "            OmegaConf.set_struct(config, True)\n",
    "            self._cfg = config\n",
    "\n",
    "            return self._cfg\n",
    "        else:\n",
    "            raise NotImplementedError(\"to_config_dict() can currently only return object._cfg but current object does not have it.\")\n",
    "        # fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class provides a common interface for modules so that, they can be easy loaded from a Hydra Config file. This class also supports instantiating via hydra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Configurable.from_config_dict\" class=\"doc_header\"><code>Configurable.from_config_dict</code><a href=\"__main__.py#L7\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Configurable.from_config_dict</code>(**`config`**:`DictConfig`, **\\*\\*`kwargs`**)\n",
       "\n",
       "Instantiates object using `DictConfig-based` configuration. You can optionally\n",
       "pass in extra `kwargs`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"show_doc(Configurable.from_config_dict)\";\n",
       "                var nbb_formatted_code = \"show_doc(Configurable.from_config_dict)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Configurable.from_config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Configurable.to_config_dict\" class=\"doc_header\"><code>Configurable.to_config_dict</code><a href=\"__main__.py#L33\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Configurable.to_config_dict</code>()\n",
       "\n",
       "Returns object's configuration to config dictionary"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"show_doc(Configurable.to_config_dict)\";\n",
       "                var nbb_formatted_code = \"show_doc(Configurable.to_config_dict)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Configurable.to_config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GaleModule-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"# export\\nclass GaleModule(Module, Configurable, metaclass=ABCMeta):\\n    \\\"\\\"\\\"\\n    Abstract class offering interface which should be implemented by all `Backbones`,\\n    `Heads` and `Meta Archs` in gale.\\n    \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def forward(self) -> Any:\\n        \\\"\\\"\\\"\\n        The main logic for the model lives here. Can return either features, logits\\n        or loss.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def build_param_dicts(self) -> Union[Iterable, List[Dict], Dict, List]:\\n        \\\"\\\"\\\"\\n        Should return the iterable of parameters to optimize or dicts defining parameter groups\\n        for the Module.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @property\\n    def param_lists(self):\\n        \\\"Returns the list of paramters in the module\\\"\\n        return [p for p in self.parameters()]\\n\\n    def all_params(self, n=slice(None), with_grad=False):\\n        \\\"List of `param_groups` upto n\\\"\\n        res = L(p for p in self.param_lists[n])\\n        # fmt: off\\n        return L(o for o in res if hasattr(o, \\\"grad\\\") and o.grad is not None) if with_grad else res\\n        # fmt: on\\n\\n    def _set_require_grad(self, rg, p):\\n        p.requires_grad_(rg)\\n\\n    def unfreeze(self) -> None:\\n        \\\"\\\"\\\"\\n        Unfreeze all parameters for training.\\n        \\\"\\\"\\\"\\n        for param in self.parameters():\\n            param.requires_grad = True\\n\\n        self.train()\\n\\n    def freeze(self) -> None:\\n        \\\"\\\"\\\"\\n        Freeze all params for inference & set model to eval\\n        \\\"\\\"\\\"\\n        for param in self.parameters():\\n            param.requires_grad = False\\n        self.eval()\\n\\n    def freeze_to(self, n) -> None:\\n        \\\"Freeze parameter groups up to `n`\\\"\\n        self.frozen_idx = n if n >= 0 else len(self.param_lists) + n\\n        if self.frozen_idx >= len(self.param_lists):\\n            # fmt: off\\n            _logger.warning(f\\\"Freezing {self.frozen_idx} groups; model has {len(self.param_lists)}; whole model is frozen.\\\")\\n            # fmt: on\\n\\n        for o in self.all_params(slice(n, None)):\\n            self._set_require_grad(True, o)\\n\\n        for o in self.all_params(slice(None, n)):\\n            self._set_require_grad(False, o)\\n\\n    @contextmanager\\n    def as_frozen(self):\\n        \\\"\\\"\\\"\\n        Context manager which temporarily freezes a module, yields control\\n        and finally unfreezes the module.\\n        \\\"\\\"\\\"\\n        self.freeze()\\n\\n        try:\\n            yield\\n        finally:\\n            self.unfreeze()\";\n",
       "                var nbb_formatted_code = \"# export\\nclass GaleModule(Module, Configurable, metaclass=ABCMeta):\\n    \\\"\\\"\\\"\\n    Abstract class offering interface which should be implemented by all `Backbones`,\\n    `Heads` and `Meta Archs` in gale.\\n    \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def forward(self) -> Any:\\n        \\\"\\\"\\\"\\n        The main logic for the model lives here. Can return either features, logits\\n        or loss.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def build_param_dicts(self) -> Union[Iterable, List[Dict], Dict, List]:\\n        \\\"\\\"\\\"\\n        Should return the iterable of parameters to optimize or dicts defining parameter groups\\n        for the Module.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @property\\n    def param_lists(self):\\n        \\\"Returns the list of paramters in the module\\\"\\n        return [p for p in self.parameters()]\\n\\n    def all_params(self, n=slice(None), with_grad=False):\\n        \\\"List of `param_groups` upto n\\\"\\n        res = L(p for p in self.param_lists[n])\\n        # fmt: off\\n        return L(o for o in res if hasattr(o, \\\"grad\\\") and o.grad is not None) if with_grad else res\\n        # fmt: on\\n\\n    def _set_require_grad(self, rg, p):\\n        p.requires_grad_(rg)\\n\\n    def unfreeze(self) -> None:\\n        \\\"\\\"\\\"\\n        Unfreeze all parameters for training.\\n        \\\"\\\"\\\"\\n        for param in self.parameters():\\n            param.requires_grad = True\\n\\n        self.train()\\n\\n    def freeze(self) -> None:\\n        \\\"\\\"\\\"\\n        Freeze all params for inference & set model to eval\\n        \\\"\\\"\\\"\\n        for param in self.parameters():\\n            param.requires_grad = False\\n        self.eval()\\n\\n    def freeze_to(self, n) -> None:\\n        \\\"Freeze parameter groups up to `n`\\\"\\n        self.frozen_idx = n if n >= 0 else len(self.param_lists) + n\\n        if self.frozen_idx >= len(self.param_lists):\\n            # fmt: off\\n            _logger.warning(f\\\"Freezing {self.frozen_idx} groups; model has {len(self.param_lists)}; whole model is frozen.\\\")\\n            # fmt: on\\n\\n        for o in self.all_params(slice(n, None)):\\n            self._set_require_grad(True, o)\\n\\n        for o in self.all_params(slice(None, n)):\\n            self._set_require_grad(False, o)\\n\\n    @contextmanager\\n    def as_frozen(self):\\n        \\\"\\\"\\\"\\n        Context manager which temporarily freezes a module, yields control\\n        and finally unfreezes the module.\\n        \\\"\\\"\\\"\\n        self.freeze()\\n\\n        try:\\n            yield\\n        finally:\\n            self.unfreeze()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class GaleModule(Module, Configurable, metaclass=ABCMeta):\n",
    "    \"\"\"\n",
    "    Abstract class offering interface which should be implemented by all `Backbones`,\n",
    "    `Heads` and `Meta Archs` in gale.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self) -> Any:\n",
    "        \"\"\"\n",
    "        The main logic for the model lives here. Can return either features, logits\n",
    "        or loss.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def build_param_dicts(self) -> Union[Iterable, List[Dict], Dict, List]:\n",
    "        \"\"\"\n",
    "        Should return the iterable of parameters to optimize or dicts defining parameter groups\n",
    "        for the Module.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def param_lists(self):\n",
    "        \"Returns the list of paramters in the module\"\n",
    "        return [p for p in self.parameters()]\n",
    "\n",
    "    def all_params(self, n=slice(None), with_grad=False):\n",
    "        \"List of `param_groups` upto n\"\n",
    "        res = L(p for p in self.param_lists[n])\n",
    "        # fmt: off\n",
    "        return L(o for o in res if hasattr(o, \"grad\") and o.grad is not None) if with_grad else res\n",
    "        # fmt: on\n",
    "\n",
    "    def _set_require_grad(self, rg, p):\n",
    "        p.requires_grad_(rg)\n",
    "\n",
    "    def unfreeze(self) -> None:\n",
    "        \"\"\"\n",
    "        Unfreeze all parameters for training.\n",
    "        \"\"\"\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.train()\n",
    "\n",
    "    def freeze(self) -> None:\n",
    "        \"\"\"\n",
    "        Freeze all params for inference & set model to eval\n",
    "        \"\"\"\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.eval()\n",
    "\n",
    "    def freeze_to(self, n) -> None:\n",
    "        \"Freeze parameter groups up to `n`\"\n",
    "        self.frozen_idx = n if n >= 0 else len(self.param_lists) + n\n",
    "        if self.frozen_idx >= len(self.param_lists):\n",
    "            # fmt: off\n",
    "            _logger.warning(f\"Freezing {self.frozen_idx} groups; model has {len(self.param_lists)}; whole model is frozen.\")\n",
    "            # fmt: on\n",
    "\n",
    "        for o in self.all_params(slice(n, None)):\n",
    "            self._set_require_grad(True, o)\n",
    "\n",
    "        for o in self.all_params(slice(None, n)):\n",
    "            self._set_require_grad(False, o)\n",
    "\n",
    "    @contextmanager\n",
    "    def as_frozen(self):\n",
    "        \"\"\"\n",
    "        Context manager which temporarily freezes a module, yields control\n",
    "        and finally unfreezes the module.\n",
    "        \"\"\"\n",
    "        self.freeze()\n",
    "\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            self.unfreeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any Module that is Registerd in Gale should inherit from this class or its subclass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.forward\" class=\"doc_header\"><code>GaleModule.forward</code><a href=\"__main__.py#L8\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleModule.forward</code>()\n",
       "\n",
       "The main logic for the model lives here. Can return either features, logits\n",
       "or loss."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleModule.forward)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleModule.forward)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleModule.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.build_param_dicts\" class=\"doc_header\"><code>GaleModule.build_param_dicts</code><a href=\"__main__.py#L16\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleModule.build_param_dicts</code>()\n",
       "\n",
       "Should return the iterable of parameters to optimize or dicts defining parameter groups\n",
       "for the Module."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleModule.build_param_dicts)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleModule.build_param_dicts)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleModule.build_param_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Configurable.from_config_dict\" class=\"doc_header\"><code>Configurable.from_config_dict</code><a href=\"__main__.py#L7\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Configurable.from_config_dict</code>(**`config`**:`DictConfig`, **\\*\\*`kwargs`**)\n",
       "\n",
       "Instantiates object using `DictConfig-based` configuration. You can optionally\n",
       "pass in extra `kwargs`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleModule.from_config_dict)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleModule.from_config_dict)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleModule.from_config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.param_lists\" class=\"doc_header\"><code>GaleModule.param_lists</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "Returns the list of paramters in the module"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleModule.param_lists)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleModule.param_lists)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleModule.param_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.all_params\" class=\"doc_header\"><code>GaleModule.all_params</code><a href=\"__main__.py#L29\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleModule.all_params</code>(**`n`**=*`slice(None, None, None)`*, **`with_grad`**=*`False`*)\n",
       "\n",
       "List of `param_groups` upto n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleModule.all_params)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleModule.all_params)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleModule.all_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.freeze\" class=\"doc_header\"><code>GaleModule.freeze</code><a href=\"__main__.py#L48\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleModule.freeze</code>()\n",
       "\n",
       "Freeze all params for inference & set model to eval"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleModule.freeze)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleModule.freeze)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleModule.freeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.freeze_to\" class=\"doc_header\"><code>GaleModule.freeze_to</code><a href=\"__main__.py#L56\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleModule.freeze_to</code>(**`n`**)\n",
       "\n",
       "Freeze parameter groups up to `n`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleModule.freeze_to)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleModule.freeze_to)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleModule.freeze_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.unfreeze\" class=\"doc_header\"><code>GaleModule.unfreeze</code><a href=\"__main__.py#L39\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleModule.unfreeze</code>()\n",
       "\n",
       "Unfreeze all parameters for training."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleModule.unfreeze)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleModule.unfreeze)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleModule.unfreeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.as_frozen\" class=\"doc_header\"><code>GaleModule.as_frozen</code><a href=\"__main__.py#L70\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleModule.as_frozen</code>()\n",
       "\n",
       "Context manager which temporarily freezes a module, yields control\n",
       "and finally unfreezes the module."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleModule.as_frozen)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleModule.as_frozen)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleModule.as_frozen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OptimSchedBuilder-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"# export\\nclass OptimSchedBuilder:\\n    \\\"\\\"\\\"\\n    Interface that constructs an Optimizer and Scheduler a from config.\\n    \\\"\\\"\\\"\\n\\n    _train_dl: Callable\\n    _trainer: pl.Trainer\\n    optimization_cfg: DictConfig\";\n",
       "                var nbb_formatted_code = \"# export\\nclass OptimSchedBuilder:\\n    \\\"\\\"\\\"\\n    Interface that constructs an Optimizer and Scheduler a from config.\\n    \\\"\\\"\\\"\\n\\n    _train_dl: Callable\\n    _trainer: pl.Trainer\\n    optimization_cfg: DictConfig\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class OptimSchedBuilder:\n",
    "    \"\"\"\n",
    "    Interface that constructs an Optimizer and Scheduler a from config.\n",
    "    \"\"\"\n",
    "\n",
    "    _train_dl: Callable\n",
    "    _trainer: pl.Trainer\n",
    "    optimization_cfg: DictConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"# collapse-output\\nfrom dataclasses import dataclass, field\\n\\nfrom fastcore.all import Path\\nfrom nbdev.export import Config\\nfrom torch.utils.data import DataLoader\\nfrom torchvision.datasets import FashionMNIST\\nfrom gale.config import get_config\\n\\ndata_path = Path(Config().path(\\\"nbs_path\\\")) / \\\"data\\\"\\ndset = FashionMNIST(root=data_path, download=True)\\n\\ncfg = get_config()\";\n",
       "                var nbb_formatted_code = \"# collapse-output\\nfrom dataclasses import dataclass, field\\n\\nfrom fastcore.all import Path\\nfrom nbdev.export import Config\\nfrom torch.utils.data import DataLoader\\nfrom torchvision.datasets import FashionMNIST\\nfrom gale.config import get_config\\n\\ndata_path = Path(Config().path(\\\"nbs_path\\\")) / \\\"data\\\"\\ndset = FashionMNIST(root=data_path, download=True)\\n\\ncfg = get_config()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# collapse-output\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from fastcore.all import Path\n",
    "from nbdev.export import Config\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from gale.config import get_config\n",
    "\n",
    "data_path = Path(Config().path(\"nbs_path\")) / \"data\"\n",
    "dset = FashionMNIST(root=data_path, download=True)\n",
    "\n",
    "cfg = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer:\n",
      "  name: AdamW\n",
      "  init_args:\n",
      "    betas:\n",
      "    - 0.95\n",
      "    - 0.999\n",
      "    eps: 1.0e-05\n",
      "    weight_decay: 0.01\n",
      "    amsgrad: false\n",
      "scheduler:\n",
      "  name: OneCycleLR\n",
      "  init_args:\n",
      "    max_lr: -1\n",
      "    total_steps: null\n",
      "    epochs: -1\n",
      "    steps_per_epoch: null\n",
      "    pct_start: 0.3\n",
      "    anneal_strategy: cos\n",
      "    cycle_momentum: true\n",
      "    base_momentum: 0.85\n",
      "    max_momentum: 0.95\n",
      "    div_factor: 25.0\n",
      "    final_div_factor: 10000.0\n",
      "  interval: step\n",
      "  monitor: null\n",
      "steps_per_epoch: -1\n",
      "max_steps: -1\n",
      "max_epochs: -1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"# collapse-output\\nbuilder = OptimSchedBuilder()\\nbuilder._train_dl = DataLoader(dset, batch_size=32)\\nbuilder._trainer = pl.Trainer(max_epochs=10, accumulate_grad_batches=1)\\n\\nprint(OmegaConf.to_yaml(cfg.optimization))\";\n",
       "                var nbb_formatted_code = \"# collapse-output\\nbuilder = OptimSchedBuilder()\\nbuilder._train_dl = DataLoader(dset, batch_size=32)\\nbuilder._trainer = pl.Trainer(max_epochs=10, accumulate_grad_batches=1)\\n\\nprint(OmegaConf.to_yaml(cfg.optimization))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# collapse-output\n",
    "builder = OptimSchedBuilder()\n",
    "builder._train_dl = DataLoader(dset, batch_size=32)\n",
    "builder._trainer = pl.Trainer(max_epochs=10, accumulate_grad_batches=1)\n",
    "\n",
    "print(OmegaConf.to_yaml(cfg.optimization))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`OptimSchedBuilder` is used in to instantite the `optimizer` and the `lr_scheduler (optional)` given a OmegaConf config that follows the structure of gale default config ...\n",
    "\n",
    "A generic optimzation config structure for image classification is shown in above. Notice how some values are `-1`. These values are automatically computed during the training process. Also requested optimizers and schedulers should be present in `OPTIM_REGISTRY` and `SCHEDULER_REGISTRY` resp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 21;\n",
       "                var nbb_unformatted_code = \"# export\\n@patch\\ndef prepare_optimization_config(self: OptimSchedBuilder, config: DictConfig):\\n    \\\"\\\"\\\"\\n    Prepares `OptimizationConfig` config and adds some interval\\n    values and infers values like max_steps, max_epochs, etc.\\n\\n    This method also fills in the values for `max_iters` & `epochs`, `steps_per_epoch`\\n    which are required by some of the LearningRate Schedulers.\\n    \\\"\\\"\\\"\\n    opt_config = copy.deepcopy(config)\\n    self.optimization_cfg = opt_config\\n\\n    self.optimization_cfg[\\\"steps_per_epoch\\\"] = len(self._train_dl)\\n\\n    if self._trainer.max_epochs is None and self._trainer.max_steps is None:\\n        msg = \\\"Either one of max_epochs or max_epochs must be provided in Trainer\\\"\\n        log_main_process(_logger, logging.ERROR, msg)\\n        raise ValueError\\n\\n    # compute effective num training steps\\n    # fmt: off\\n    if isinstance(self._trainer.limit_train_batches, int) and self._trainer.limit_train_batches != 0:\\n    # fmt: on\\n        dataset_size = self.trainer.limit_train_batches\\n    \\n    elif isinstance(self._trainer.limit_train_batches, float):\\n        # limit_train_batches is a percentage of batches\\n        dataset_size = len(self._train_dl)\\n        dataset_size = int(dataset_size * self._trainer.limit_train_batches)\\n    \\n    else:\\n        dataset_size = len(self._train_dl)\\n\\n    num_devices = max(1, self._trainer.num_gpus, self._trainer.num_processes)\\n\\n    if self._trainer.tpu_cores:\\n        num_devices = max(num_devices, self._trainer.tpu_cores)\\n\\n    effective_batch_size = self._trainer.accumulate_grad_batches * num_devices\\n    max_steps = (dataset_size // effective_batch_size) * self._trainer.max_epochs\\n\\n    if self._trainer.max_steps is None:\\n        self.optimization_cfg[\\\"max_epochs\\\"] = self._trainer.max_epochs\\n        self.optimization_cfg[\\\"max_steps\\\"] = max_steps\\n\\n    else:\\n        epochs = self._trainer.max_steps * len(self._train_dl)\\n        self.optimization_cfg[\\\"max_steps\\\"] = self._trainer.max_steps\\n        self.optimization_cfg[\\\"max_epochs\\\"] = epochs\\n\\n    # covert config to Dictionary\\n    # fmt: off\\n    sched_config = OmegaConf.to_container(self.optimization_cfg.scheduler.init_args, resolve=True)\\n\\n    max_steps = self.optimization_cfg[\\\"max_steps\\\"]\\n    max_epochs = self.optimization_cfg[\\\"max_epochs\\\"]\\n    steps = self.optimization_cfg[\\\"steps_per_epoch\\\"]\\n\\n    # populate values in learning rate schedulers\\n    if \\\"max_iters\\\" in sched_config:\\n        if sched_config[\\\"max_iters\\\"] == -1:\\n            OmegaConf.update(self.optimization_cfg, \\\"scheduler.init_args.max_iters\\\", max_steps)\\n            msg = f\\\"Set the value of 'max_iters' to be {max_steps}.\\\"\\n            log_main_process(_logger, logging.DEBUG, msg)\\n\\n    if \\\"epochs\\\" in sched_config:\\n        if sched_config[\\\"epochs\\\"] == -1:\\n            OmegaConf.update(self.optimization_cfg, \\\"scheduler.init_args.epochs\\\", max_epochs)\\n            msg = f\\\"Set the value of 'epochs' to be {max_epochs}.\\\"\\n            log_main_process(_logger, logging.DEBUG, msg)\\n\\n    if \\\"steps_per_epoch\\\" in sched_config:\\n        if sched_config[\\\"steps_per_epoch\\\"] is None:\\n            OmegaConf.update(self.optimization_cfg, \\\"scheduler.init_args.steps_per_epoch\\\", steps)\\n            msg = f\\\"Set the value of 'steps_per_epoch' to be {steps}.\\\"\\n            log_main_process(_logger, logging.DEBUG, msg)\\n    # fmt: on\";\n",
       "                var nbb_formatted_code = \"# export\\n@patch\\ndef prepare_optimization_config(self: OptimSchedBuilder, config: DictConfig):\\n    \\\"\\\"\\\"\\n    Prepares `OptimizationConfig` config and adds some interval\\n    values and infers values like max_steps, max_epochs, etc.\\n\\n    This method also fills in the values for `max_iters` & `epochs`, `steps_per_epoch`\\n    which are required by some of the LearningRate Schedulers.\\n    \\\"\\\"\\\"\\n    opt_config = copy.deepcopy(config)\\n    self.optimization_cfg = opt_config\\n\\n    self.optimization_cfg[\\\"steps_per_epoch\\\"] = len(self._train_dl)\\n\\n    if self._trainer.max_epochs is None and self._trainer.max_steps is None:\\n        msg = \\\"Either one of max_epochs or max_epochs must be provided in Trainer\\\"\\n        log_main_process(_logger, logging.ERROR, msg)\\n        raise ValueError\\n\\n    # compute effective num training steps\\n    # fmt: off\\n    if isinstance(self._trainer.limit_train_batches, int) and self._trainer.limit_train_batches != 0:\\n    # fmt: on\\n        dataset_size = self.trainer.limit_train_batches\\n    \\n    elif isinstance(self._trainer.limit_train_batches, float):\\n        # limit_train_batches is a percentage of batches\\n        dataset_size = len(self._train_dl)\\n        dataset_size = int(dataset_size * self._trainer.limit_train_batches)\\n    \\n    else:\\n        dataset_size = len(self._train_dl)\\n\\n    num_devices = max(1, self._trainer.num_gpus, self._trainer.num_processes)\\n\\n    if self._trainer.tpu_cores:\\n        num_devices = max(num_devices, self._trainer.tpu_cores)\\n\\n    effective_batch_size = self._trainer.accumulate_grad_batches * num_devices\\n    max_steps = (dataset_size // effective_batch_size) * self._trainer.max_epochs\\n\\n    if self._trainer.max_steps is None:\\n        self.optimization_cfg[\\\"max_epochs\\\"] = self._trainer.max_epochs\\n        self.optimization_cfg[\\\"max_steps\\\"] = max_steps\\n\\n    else:\\n        epochs = self._trainer.max_steps * len(self._train_dl)\\n        self.optimization_cfg[\\\"max_steps\\\"] = self._trainer.max_steps\\n        self.optimization_cfg[\\\"max_epochs\\\"] = epochs\\n\\n    # covert config to Dictionary\\n    # fmt: off\\n    sched_config = OmegaConf.to_container(self.optimization_cfg.scheduler.init_args, resolve=True)\\n\\n    max_steps = self.optimization_cfg[\\\"max_steps\\\"]\\n    max_epochs = self.optimization_cfg[\\\"max_epochs\\\"]\\n    steps = self.optimization_cfg[\\\"steps_per_epoch\\\"]\\n\\n    # populate values in learning rate schedulers\\n    if \\\"max_iters\\\" in sched_config:\\n        if sched_config[\\\"max_iters\\\"] == -1:\\n            OmegaConf.update(self.optimization_cfg, \\\"scheduler.init_args.max_iters\\\", max_steps)\\n            msg = f\\\"Set the value of 'max_iters' to be {max_steps}.\\\"\\n            log_main_process(_logger, logging.DEBUG, msg)\\n\\n    if \\\"epochs\\\" in sched_config:\\n        if sched_config[\\\"epochs\\\"] == -1:\\n            OmegaConf.update(self.optimization_cfg, \\\"scheduler.init_args.epochs\\\", max_epochs)\\n            msg = f\\\"Set the value of 'epochs' to be {max_epochs}.\\\"\\n            log_main_process(_logger, logging.DEBUG, msg)\\n\\n    if \\\"steps_per_epoch\\\" in sched_config:\\n        if sched_config[\\\"steps_per_epoch\\\"] is None:\\n            OmegaConf.update(self.optimization_cfg, \\\"scheduler.init_args.steps_per_epoch\\\", steps)\\n            msg = f\\\"Set the value of 'steps_per_epoch' to be {steps}.\\\"\\n            log_main_process(_logger, logging.DEBUG, msg)\\n    # fmt: on\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@patch\n",
    "def prepare_optimization_config(self: OptimSchedBuilder, config: DictConfig):\n",
    "    \"\"\"\n",
    "    Prepares `OptimizationConfig` config and adds some interval\n",
    "    values and infers values like max_steps, max_epochs, etc.\n",
    "\n",
    "    This method also fills in the values for `max_iters` & `epochs`, `steps_per_epoch`\n",
    "    which are required by some of the LearningRate Schedulers.\n",
    "    \"\"\"\n",
    "    opt_config = copy.deepcopy(config)\n",
    "    self.optimization_cfg = opt_config\n",
    "\n",
    "    self.optimization_cfg[\"steps_per_epoch\"] = len(self._train_dl)\n",
    "\n",
    "    if self._trainer.max_epochs is None and self._trainer.max_steps is None:\n",
    "        msg = \"Either one of max_epochs or max_epochs must be provided in Trainer\"\n",
    "        log_main_process(_logger, logging.ERROR, msg)\n",
    "        raise ValueError\n",
    "\n",
    "    # compute effective num training steps\n",
    "    # fmt: off\n",
    "    if isinstance(self._trainer.limit_train_batches, int) and self._trainer.limit_train_batches != 0:\n",
    "    # fmt: on\n",
    "        dataset_size = self.trainer.limit_train_batches\n",
    "    \n",
    "    elif isinstance(self._trainer.limit_train_batches, float):\n",
    "        # limit_train_batches is a percentage of batches\n",
    "        dataset_size = len(self._train_dl)\n",
    "        dataset_size = int(dataset_size * self._trainer.limit_train_batches)\n",
    "    \n",
    "    else:\n",
    "        dataset_size = len(self._train_dl)\n",
    "\n",
    "    num_devices = max(1, self._trainer.num_gpus, self._trainer.num_processes)\n",
    "\n",
    "    if self._trainer.tpu_cores:\n",
    "        num_devices = max(num_devices, self._trainer.tpu_cores)\n",
    "\n",
    "    effective_batch_size = self._trainer.accumulate_grad_batches * num_devices\n",
    "    max_steps = (dataset_size // effective_batch_size) * self._trainer.max_epochs\n",
    "\n",
    "    if self._trainer.max_steps is None:\n",
    "        self.optimization_cfg[\"max_epochs\"] = self._trainer.max_epochs\n",
    "        self.optimization_cfg[\"max_steps\"] = max_steps\n",
    "\n",
    "    else:\n",
    "        epochs = self._trainer.max_steps * len(self._train_dl)\n",
    "        self.optimization_cfg[\"max_steps\"] = self._trainer.max_steps\n",
    "        self.optimization_cfg[\"max_epochs\"] = epochs\n",
    "\n",
    "    # covert config to Dictionary\n",
    "    # fmt: off\n",
    "    sched_config = OmegaConf.to_container(self.optimization_cfg.scheduler.init_args, resolve=True)\n",
    "\n",
    "    max_steps = self.optimization_cfg[\"max_steps\"]\n",
    "    max_epochs = self.optimization_cfg[\"max_epochs\"]\n",
    "    steps = self.optimization_cfg[\"steps_per_epoch\"]\n",
    "\n",
    "    # populate values in learning rate schedulers\n",
    "    if \"max_iters\" in sched_config:\n",
    "        if sched_config[\"max_iters\"] == -1:\n",
    "            OmegaConf.update(self.optimization_cfg, \"scheduler.init_args.max_iters\", max_steps)\n",
    "            msg = f\"Set the value of 'max_iters' to be {max_steps}.\"\n",
    "            log_main_process(_logger, logging.DEBUG, msg)\n",
    "\n",
    "    if \"epochs\" in sched_config:\n",
    "        if sched_config[\"epochs\"] == -1:\n",
    "            OmegaConf.update(self.optimization_cfg, \"scheduler.init_args.epochs\", max_epochs)\n",
    "            msg = f\"Set the value of 'epochs' to be {max_epochs}.\"\n",
    "            log_main_process(_logger, logging.DEBUG, msg)\n",
    "\n",
    "    if \"steps_per_epoch\" in sched_config:\n",
    "        if sched_config[\"steps_per_epoch\"] is None:\n",
    "            OmegaConf.update(self.optimization_cfg, \"scheduler.init_args.steps_per_epoch\", steps)\n",
    "            msg = f\"Set the value of 'steps_per_epoch' to be {steps}.\"\n",
    "            log_main_process(_logger, logging.DEBUG, msg)\n",
    "    # fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 22;\n",
       "                var nbb_unformatted_code = \"builder.prepare_optimization_config(config=cfg.optimization)\";\n",
       "                var nbb_formatted_code = \"builder.prepare_optimization_config(config=cfg.optimization)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "builder.prepare_optimization_config(config=cfg.optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 23;\n",
       "                var nbb_unformatted_code = \"# export\\n@patch\\ndef build_optimizer(self: OptimSchedBuilder, params: Any) -> torch.optim.Optimizer:\\n    \\\"\\\"\\\"\\n    Builds a single optimizer from `OptimizationConfig`. `params` are the parameter\\n    dict with the weights for the optimizer to optimizer.\\n\\n    Note this method must be called after `prepare_optimization_config()`\\n    \\\"\\\"\\\"\\n    if not isinstance(self.optimization_cfg, DictConfig):\\n        msg = \\\"optimization_cfg not found, did you call `prepare_optimization_config`.\\\"\\n        log_main_process(_logger, logging.WARNING, msg)\\n        raise NameError\\n    else:\\n        if self.optimization_cfg.optimizer.name is None:\\n            msg = \\\"Optimizer is None, so no optimizer will be created.\\\"\\n            log_main_process(_logger, logging.WARNING, msg)\\n            opt = None\\n        else:\\n            opt = self.optimization_cfg.optimizer\\n            opt = OPTIM_REGISTRY.get(opt.name)(params=params, **opt.init_args)\\n            msg = f\\\"Built optimizer, {opt.__class__.__name__} with {len(opt.param_groups)} param group(s).\\\"\\n            log_main_process(_logger, logging.DEBUG, msg)\\n        return opt\";\n",
       "                var nbb_formatted_code = \"# export\\n@patch\\ndef build_optimizer(self: OptimSchedBuilder, params: Any) -> torch.optim.Optimizer:\\n    \\\"\\\"\\\"\\n    Builds a single optimizer from `OptimizationConfig`. `params` are the parameter\\n    dict with the weights for the optimizer to optimizer.\\n\\n    Note this method must be called after `prepare_optimization_config()`\\n    \\\"\\\"\\\"\\n    if not isinstance(self.optimization_cfg, DictConfig):\\n        msg = \\\"optimization_cfg not found, did you call `prepare_optimization_config`.\\\"\\n        log_main_process(_logger, logging.WARNING, msg)\\n        raise NameError\\n    else:\\n        if self.optimization_cfg.optimizer.name is None:\\n            msg = \\\"Optimizer is None, so no optimizer will be created.\\\"\\n            log_main_process(_logger, logging.WARNING, msg)\\n            opt = None\\n        else:\\n            opt = self.optimization_cfg.optimizer\\n            opt = OPTIM_REGISTRY.get(opt.name)(params=params, **opt.init_args)\\n            msg = f\\\"Built optimizer, {opt.__class__.__name__} with {len(opt.param_groups)} param group(s).\\\"\\n            log_main_process(_logger, logging.DEBUG, msg)\\n        return opt\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@patch\n",
    "def build_optimizer(self: OptimSchedBuilder, params: Any) -> torch.optim.Optimizer:\n",
    "    \"\"\"\n",
    "    Builds a single optimizer from `OptimizationConfig`. `params` are the parameter\n",
    "    dict with the weights for the optimizer to optimizer.\n",
    "\n",
    "    Note this method must be called after `prepare_optimization_config()`\n",
    "    \"\"\"\n",
    "    if not isinstance(self.optimization_cfg, DictConfig):\n",
    "        msg = \"optimization_cfg not found, did you call `prepare_optimization_config`.\"\n",
    "        log_main_process(_logger, logging.WARNING, msg)\n",
    "        raise NameError\n",
    "    else:\n",
    "        if self.optimization_cfg.optimizer.name is None:\n",
    "            msg = \"Optimizer is None, so no optimizer will be created.\"\n",
    "            log_main_process(_logger, logging.WARNING, msg)\n",
    "            opt = None\n",
    "        else:\n",
    "            opt = self.optimization_cfg.optimizer\n",
    "            opt = OPTIM_REGISTRY.get(opt.name)(params=params, **opt.init_args)\n",
    "            msg = f\"Built optimizer, {opt.__class__.__name__} with {len(opt.param_groups)} param group(s).\"\n",
    "            log_main_process(_logger, logging.DEBUG, msg)\n",
    "        return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 24;\n",
       "                var nbb_unformatted_code = \"# export\\n@patch\\ndef build_lr_scheduler(\\n    self: OptimSchedBuilder, optimizer: torch.optim.Optimizer\\n) -> Any:\\n    \\\"\\\"\\\"\\n    Builds a LearningRate scheduler from `OptimizationConfig`. Returns an LRScheduler dict\\n    that is required by PyTorch Lightning for LRSchedulers.\\n    Note this method must be called after `prepare_optimization_config()`\\n    \\\"\\\"\\\"\\n    if not isinstance(self.optimization_cfg, DictConfig):\\n        msg = \\\"optimization_cfg not found, did you call `prepare_optimization_config`.\\\"\\n        log_main_process(_logger, logging.WARNING, msg)\\n        raise NameError\\n    else:\\n        if self.optimization_cfg.scheduler.name is None:\\n            msg = \\\"scheduler is None, so no scheduler will be created.\\\"\\n            log_main_process(_logger, logging.WARNING, msg)\\n            sched = None\\n        else:\\n            _c = self.optimization_cfg.scheduler.init_args\\n            _temp = OmegaConf.to_container(_c, resolve=True)\\n            kwds = {}\\n\\n            # if a key value is ListConfig then we convert it to simple list\\n            for key, value in _temp.items():\\n                if isinstance(value, list):\\n                    kwds[key] = list(value)\\n                else:\\n                    kwds[key] = value\\n\\n            instance = SCHEDULER_REGISTRY.get(self.optimization_cfg.scheduler.name)\\n            sch = instance(optimizer=optimizer, **kwds)\\n\\n            # convert the lr_scheduler to pytorch-lightning LRScheduler dictionary format\\n            msg = f\\\"LRScheduler : {sch.__class__.__name__}.\\\"\\n            log_main_process(_logger, logging.DEBUG, msg)\\n            sched = {\\n                \\\"scheduler\\\": sch,\\n                \\\"interval\\\": self.optimization_cfg.scheduler.interval,\\n                \\\"monitor\\\": self.optimization_cfg.scheduler.monitor,\\n            }\\n            return sched\";\n",
       "                var nbb_formatted_code = \"# export\\n@patch\\ndef build_lr_scheduler(\\n    self: OptimSchedBuilder, optimizer: torch.optim.Optimizer\\n) -> Any:\\n    \\\"\\\"\\\"\\n    Builds a LearningRate scheduler from `OptimizationConfig`. Returns an LRScheduler dict\\n    that is required by PyTorch Lightning for LRSchedulers.\\n    Note this method must be called after `prepare_optimization_config()`\\n    \\\"\\\"\\\"\\n    if not isinstance(self.optimization_cfg, DictConfig):\\n        msg = \\\"optimization_cfg not found, did you call `prepare_optimization_config`.\\\"\\n        log_main_process(_logger, logging.WARNING, msg)\\n        raise NameError\\n    else:\\n        if self.optimization_cfg.scheduler.name is None:\\n            msg = \\\"scheduler is None, so no scheduler will be created.\\\"\\n            log_main_process(_logger, logging.WARNING, msg)\\n            sched = None\\n        else:\\n            _c = self.optimization_cfg.scheduler.init_args\\n            _temp = OmegaConf.to_container(_c, resolve=True)\\n            kwds = {}\\n\\n            # if a key value is ListConfig then we convert it to simple list\\n            for key, value in _temp.items():\\n                if isinstance(value, list):\\n                    kwds[key] = list(value)\\n                else:\\n                    kwds[key] = value\\n\\n            instance = SCHEDULER_REGISTRY.get(self.optimization_cfg.scheduler.name)\\n            sch = instance(optimizer=optimizer, **kwds)\\n\\n            # convert the lr_scheduler to pytorch-lightning LRScheduler dictionary format\\n            msg = f\\\"LRScheduler : {sch.__class__.__name__}.\\\"\\n            log_main_process(_logger, logging.DEBUG, msg)\\n            sched = {\\n                \\\"scheduler\\\": sch,\\n                \\\"interval\\\": self.optimization_cfg.scheduler.interval,\\n                \\\"monitor\\\": self.optimization_cfg.scheduler.monitor,\\n            }\\n            return sched\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@patch\n",
    "def build_lr_scheduler(\n",
    "    self: OptimSchedBuilder, optimizer: torch.optim.Optimizer\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Builds a LearningRate scheduler from `OptimizationConfig`. Returns an LRScheduler dict\n",
    "    that is required by PyTorch Lightning for LRSchedulers.\n",
    "    Note this method must be called after `prepare_optimization_config()`\n",
    "    \"\"\"\n",
    "    if not isinstance(self.optimization_cfg, DictConfig):\n",
    "        msg = \"optimization_cfg not found, did you call `prepare_optimization_config`.\"\n",
    "        log_main_process(_logger, logging.WARNING, msg)\n",
    "        raise NameError\n",
    "    else:\n",
    "        if self.optimization_cfg.scheduler.name is None:\n",
    "            msg = \"scheduler is None, so no scheduler will be created.\"\n",
    "            log_main_process(_logger, logging.WARNING, msg)\n",
    "            sched = None\n",
    "        else:\n",
    "            _c = self.optimization_cfg.scheduler.init_args\n",
    "            _temp = OmegaConf.to_container(_c, resolve=True)\n",
    "            kwds = {}\n",
    "\n",
    "            # if a key value is ListConfig then we convert it to simple list\n",
    "            for key, value in _temp.items():\n",
    "                if isinstance(value, list):\n",
    "                    kwds[key] = list(value)\n",
    "                else:\n",
    "                    kwds[key] = value\n",
    "\n",
    "            instance = SCHEDULER_REGISTRY.get(self.optimization_cfg.scheduler.name)\n",
    "            sch = instance(optimizer=optimizer, **kwds)\n",
    "\n",
    "            # convert the lr_scheduler to pytorch-lightning LRScheduler dictionary format\n",
    "            msg = f\"LRScheduler : {sch.__class__.__name__}.\"\n",
    "            log_main_process(_logger, logging.DEBUG, msg)\n",
    "            sched = {\n",
    "                \"scheduler\": sch,\n",
    "                \"interval\": self.optimization_cfg.scheduler.interval,\n",
    "                \"monitor\": self.optimization_cfg.scheduler.monitor,\n",
    "            }\n",
    "            return sched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 25;\n",
       "                var nbb_unformatted_code = \"params = [torch.nn.Parameter(torch.randn(1, 2))]\\noptimizer = builder.build_optimizer(params)\\nassert isinstance(optimizer, torch.optim.AdamW)\\n\\n# for onecycle lrs; we need max_lrs\\nbuilder.optimization_cfg.scheduler.init_args.max_lr = [1e-03]\\nscheduler = builder.build_lr_scheduler(optimizer)\\nassert isinstance(scheduler, Dict)\\nassert isinstance(scheduler[\\\"scheduler\\\"], torch.optim.lr_scheduler.OneCycleLR)\";\n",
       "                var nbb_formatted_code = \"params = [torch.nn.Parameter(torch.randn(1, 2))]\\noptimizer = builder.build_optimizer(params)\\nassert isinstance(optimizer, torch.optim.AdamW)\\n\\n# for onecycle lrs; we need max_lrs\\nbuilder.optimization_cfg.scheduler.init_args.max_lr = [1e-03]\\nscheduler = builder.build_lr_scheduler(optimizer)\\nassert isinstance(scheduler, Dict)\\nassert isinstance(scheduler[\\\"scheduler\\\"], torch.optim.lr_scheduler.OneCycleLR)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = [torch.nn.Parameter(torch.randn(1, 2))]\n",
    "optimizer = builder.build_optimizer(params)\n",
    "assert isinstance(optimizer, torch.optim.AdamW)\n",
    "\n",
    "# for onecycle lrs; we need max_lrs\n",
    "builder.optimization_cfg.scheduler.init_args.max_lr = [1e-03]\n",
    "scheduler = builder.build_lr_scheduler(optimizer)\n",
    "assert isinstance(scheduler, Dict)\n",
    "assert isinstance(scheduler[\"scheduler\"], torch.optim.lr_scheduler.OneCycleLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GaleTask -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 27;\n",
       "                var nbb_unformatted_code = \"# export\\nclass GaleTask(pl.LightningModule, OptimSchedBuilder, metaclass=ABCMeta):\\n    \\\"\\\"\\\"\\n    Interface for Pytorch-lightning based Gale modules\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        cfg: DictConfig,\\n        trainer: Optional[pl.Trainer] = None,\\n        metrics: Union[torchmetrics.Metric, Mapping, Sequence, None] = None,\\n    ):\\n        \\\"\\\"\\\"\\n        Base class from which all PyTorch Lightning Tasks in Gale should inherit.\\n\\n        Arguments:\\n        1. `cfg` `(DictConfig)`:  configuration object. cfg object should be inherited from `BaseGaleConfig`.\\n        2. `trainer` `(Optional, pl.Trainer)`: Pytorch Lightning Trainer instance\\n        3. `metrics` `(Optional)`: Metrics to compute for training and evaluation.\\n        \\\"\\\"\\\"\\n        super().__init__()\\n        self._cfg = OmegaConf.structured(cfg)\\n\\n        self.save_hyperparameters(self._cfg)\\n        self._train_dl = noop\\n        self._validation_dl = noop\\n        self._test_dl = noop\\n        self._optimizer = noop\\n        self._scheduler = noop\\n        self._trainer = trainer\\n        self.metrics = metrics\\n\\n    def train_dataloader(self) -> torch.utils.data.DataLoader:\\n        \\\"Returns the Dataloader used for Training\\\"\\n        if self._train_dl is not None and self._train_dl is not noop:\\n            return self._train_dl\\n\\n    def val_dataloader(self) -> Any:\\n        \\\"Returns the List of Dataloaders or Dataloader used for Validation\\\"\\n        if self._validation_dl is not None and self._validation_dl is not noop:\\n            return self._validation_dl\\n\\n    def test_dataloader(self) -> Any:\\n        \\\"Returns the List of Dataloaders or Dataloader used for Testing\\\"\\n        if self._test_dl is not None and self._test_dl is not noop:\\n            return self._test_dl\\n\\n    @abstractmethod\\n    def forward(self, x: torch.Tensor):\\n        \\\"\\\"\\\"\\n        The Forward method for LightningModule, users should modify this method.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def setup_training_data(self, train_data_config: Union[DictConfig, Dict]):\\n        \\\"\\\"\\\"\\n        Setups data loader to be used in training\\n\\n        Arguments:\\n        1. `train_data_config`: training data loader parameters.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def setup_validation_data(self, val_data_config: Union[DictConfig, Dict]):\\n        \\\"\\\"\\\"\\n        Setups data loader to be used in validation\\n\\n        Arguments:\\n        1. `val_data_config`: validation data loader parameters.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    def setup_test_data(\\n        self, test_data_config: Optional[Union[DictConfig, Dict]] = None\\n    ):\\n        \\\"\\\"\\\"\\n        (Optionally) Setups data loader to be used in test\\n\\n        Arguments:\\n        1. `test_data_config`: test data loader parameters.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @property\\n    def param_dicts(self) -> Union[Iterator, List[Dict]]:\\n        \\\"\\\"\\\"\\n        Property that returns the param dicts for optimization.\\n        Override for custom training behaviour. Currently returns all the trainable paramters.\\n        \\\"\\\"\\\"\\n        return L(self).map(trainable_params)\\n\\n    def shared_step(self, batch: Any, batch_idx: int, stage: str) -> Any:\\n        \\\"\\\"\\\"\\n        The common training/validation/test step. Override for custom behavior. This step\\n        is shared between training/validation/test step. For training/validation/test steps\\n        `stage` is train/val/test respectively. You training logic should go here avoid directly overriding\\n        training/validation/test step methods.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    def training_step(self, batch: Any, batch_idx: int) -> Any:\\n        \\\"\\\"\\\"\\n        The training step of the LightningModule. For common use cases you need\\n        not need to override this method. See `GaleTask.shared_step()`\\n        \\\"\\\"\\\"\\n        return self.shared_step(batch, batch_idx, stage=\\\"train\\\")\\n\\n    def validation_step(self, batch: Any, batch_idx: int) -> None:\\n        \\\"\\\"\\\"\\n        The validation step of the LightningModule. For common use cases you need\\n        not need to override this method. See `GaleTask.shared_step()`\\n        \\\"\\\"\\\"\\n        return self.shared_step(batch, batch_idx, stage=\\\"val\\\")\\n\\n    def test_step(self, batch: Any, batch_idx: int) -> None:\\n        \\\"\\\"\\\"\\n        The test step of the LightningModule. For common use cases you need\\n        not need to override this method. See `GaleTask.shared_step()`\\n        \\\"\\\"\\\"\\n        return self.shared_step(batch, batch_idx, stage=\\\"test\\\")\\n\\n    def setup_optimization(self, optim_config: DictConfig = None):\\n        \\\"\\\"\\\"\\n        Prepares an optimizer from a string name and its optional config parameters.\\n\\n        Args:\\n        1. `optim_config`: A `dictionary`/`DictConfig` or instance of `OptimizationConfig`.\\n        \\\"\\\"\\\"\\n        # If config was not explicitly passed to us\\n        if optim_config is None:\\n            # See if internal config has `optim` namespace\\n            if self._cfg is not None and hasattr(self._cfg, \\\"optimization\\\"):\\n                optim_config = self._cfg.optimization\\n\\n        # If config is still None, or internal config has no Optim, return without instantiation\\n        if optim_config is None:\\n            msg = \\\"No optimizer config provided, therefore no optimizer was created\\\"\\n            log_main_process(_logger, logging.WARNING, msg)\\n            return\\n\\n        else:\\n            # Preserve the configuration\\n            if not isinstance(optim_config, DictConfig):\\n                optim_config = OmegaConf.create(optim_config)\\n\\n            # prepare the optimization config\\n            self.prepare_optimization_config(optim_config)\\n\\n            # Setup optimizer and scheduler\\n            self._optimizer = self.build_optimizer(self.param_dicts)\\n            self._scheduler = self.build_lr_scheduler(self._optimizer)\\n\\n    def configure_optimizers(self):\\n        \\\"\\\"\\\"\\n        Choose what optimizers and learning-rate schedulers to use in your optimization.\\n        See https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html\\n        \\\"\\\"\\\"\\n        # if self.setup_optimization() has been called manually no\\n        # need to call again\\n        if self._optimizer is noop and self._scheduler is noop:\\n            self.setup_optimization()\\n\\n        if self._scheduler is None:\\n            return self._optimizer\\n        else:\\n            return [self._optimizer], [self._scheduler]\";\n",
       "                var nbb_formatted_code = \"# export\\nclass GaleTask(pl.LightningModule, OptimSchedBuilder, metaclass=ABCMeta):\\n    \\\"\\\"\\\"\\n    Interface for Pytorch-lightning based Gale modules\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        cfg: DictConfig,\\n        trainer: Optional[pl.Trainer] = None,\\n        metrics: Union[torchmetrics.Metric, Mapping, Sequence, None] = None,\\n    ):\\n        \\\"\\\"\\\"\\n        Base class from which all PyTorch Lightning Tasks in Gale should inherit.\\n\\n        Arguments:\\n        1. `cfg` `(DictConfig)`:  configuration object. cfg object should be inherited from `BaseGaleConfig`.\\n        2. `trainer` `(Optional, pl.Trainer)`: Pytorch Lightning Trainer instance\\n        3. `metrics` `(Optional)`: Metrics to compute for training and evaluation.\\n        \\\"\\\"\\\"\\n        super().__init__()\\n        self._cfg = OmegaConf.structured(cfg)\\n\\n        self.save_hyperparameters(self._cfg)\\n        self._train_dl = noop\\n        self._validation_dl = noop\\n        self._test_dl = noop\\n        self._optimizer = noop\\n        self._scheduler = noop\\n        self._trainer = trainer\\n        self.metrics = metrics\\n\\n    def train_dataloader(self) -> torch.utils.data.DataLoader:\\n        \\\"Returns the Dataloader used for Training\\\"\\n        if self._train_dl is not None and self._train_dl is not noop:\\n            return self._train_dl\\n\\n    def val_dataloader(self) -> Any:\\n        \\\"Returns the List of Dataloaders or Dataloader used for Validation\\\"\\n        if self._validation_dl is not None and self._validation_dl is not noop:\\n            return self._validation_dl\\n\\n    def test_dataloader(self) -> Any:\\n        \\\"Returns the List of Dataloaders or Dataloader used for Testing\\\"\\n        if self._test_dl is not None and self._test_dl is not noop:\\n            return self._test_dl\\n\\n    @abstractmethod\\n    def forward(self, x: torch.Tensor):\\n        \\\"\\\"\\\"\\n        The Forward method for LightningModule, users should modify this method.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def setup_training_data(self, train_data_config: Union[DictConfig, Dict]):\\n        \\\"\\\"\\\"\\n        Setups data loader to be used in training\\n\\n        Arguments:\\n        1. `train_data_config`: training data loader parameters.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def setup_validation_data(self, val_data_config: Union[DictConfig, Dict]):\\n        \\\"\\\"\\\"\\n        Setups data loader to be used in validation\\n\\n        Arguments:\\n        1. `val_data_config`: validation data loader parameters.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    def setup_test_data(\\n        self, test_data_config: Optional[Union[DictConfig, Dict]] = None\\n    ):\\n        \\\"\\\"\\\"\\n        (Optionally) Setups data loader to be used in test\\n\\n        Arguments:\\n        1. `test_data_config`: test data loader parameters.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @property\\n    def param_dicts(self) -> Union[Iterator, List[Dict]]:\\n        \\\"\\\"\\\"\\n        Property that returns the param dicts for optimization.\\n        Override for custom training behaviour. Currently returns all the trainable paramters.\\n        \\\"\\\"\\\"\\n        return L(self).map(trainable_params)\\n\\n    def shared_step(self, batch: Any, batch_idx: int, stage: str) -> Any:\\n        \\\"\\\"\\\"\\n        The common training/validation/test step. Override for custom behavior. This step\\n        is shared between training/validation/test step. For training/validation/test steps\\n        `stage` is train/val/test respectively. You training logic should go here avoid directly overriding\\n        training/validation/test step methods.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    def training_step(self, batch: Any, batch_idx: int) -> Any:\\n        \\\"\\\"\\\"\\n        The training step of the LightningModule. For common use cases you need\\n        not need to override this method. See `GaleTask.shared_step()`\\n        \\\"\\\"\\\"\\n        return self.shared_step(batch, batch_idx, stage=\\\"train\\\")\\n\\n    def validation_step(self, batch: Any, batch_idx: int) -> None:\\n        \\\"\\\"\\\"\\n        The validation step of the LightningModule. For common use cases you need\\n        not need to override this method. See `GaleTask.shared_step()`\\n        \\\"\\\"\\\"\\n        return self.shared_step(batch, batch_idx, stage=\\\"val\\\")\\n\\n    def test_step(self, batch: Any, batch_idx: int) -> None:\\n        \\\"\\\"\\\"\\n        The test step of the LightningModule. For common use cases you need\\n        not need to override this method. See `GaleTask.shared_step()`\\n        \\\"\\\"\\\"\\n        return self.shared_step(batch, batch_idx, stage=\\\"test\\\")\\n\\n    def setup_optimization(self, optim_config: DictConfig = None):\\n        \\\"\\\"\\\"\\n        Prepares an optimizer from a string name and its optional config parameters.\\n\\n        Args:\\n        1. `optim_config`: A `dictionary`/`DictConfig` or instance of `OptimizationConfig`.\\n        \\\"\\\"\\\"\\n        # If config was not explicitly passed to us\\n        if optim_config is None:\\n            # See if internal config has `optim` namespace\\n            if self._cfg is not None and hasattr(self._cfg, \\\"optimization\\\"):\\n                optim_config = self._cfg.optimization\\n\\n        # If config is still None, or internal config has no Optim, return without instantiation\\n        if optim_config is None:\\n            msg = \\\"No optimizer config provided, therefore no optimizer was created\\\"\\n            log_main_process(_logger, logging.WARNING, msg)\\n            return\\n\\n        else:\\n            # Preserve the configuration\\n            if not isinstance(optim_config, DictConfig):\\n                optim_config = OmegaConf.create(optim_config)\\n\\n            # prepare the optimization config\\n            self.prepare_optimization_config(optim_config)\\n\\n            # Setup optimizer and scheduler\\n            self._optimizer = self.build_optimizer(self.param_dicts)\\n            self._scheduler = self.build_lr_scheduler(self._optimizer)\\n\\n    def configure_optimizers(self):\\n        \\\"\\\"\\\"\\n        Choose what optimizers and learning-rate schedulers to use in your optimization.\\n        See https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html\\n        \\\"\\\"\\\"\\n        # if self.setup_optimization() has been called manually no\\n        # need to call again\\n        if self._optimizer is noop and self._scheduler is noop:\\n            self.setup_optimization()\\n\\n        if self._scheduler is None:\\n            return self._optimizer\\n        else:\\n            return [self._optimizer], [self._scheduler]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class GaleTask(pl.LightningModule, OptimSchedBuilder, metaclass=ABCMeta):\n",
    "    \"\"\"\n",
    "    Interface for Pytorch-lightning based Gale modules\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: DictConfig,\n",
    "        trainer: Optional[pl.Trainer] = None,\n",
    "        metrics: Union[torchmetrics.Metric, Mapping, Sequence, None] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Base class from which all PyTorch Lightning Tasks in Gale should inherit.\n",
    "\n",
    "        Arguments:\n",
    "        1. `cfg` `(DictConfig)`:  configuration object. cfg object should be inherited from `BaseGaleConfig`.\n",
    "        2. `trainer` `(Optional, pl.Trainer)`: Pytorch Lightning Trainer instance\n",
    "        3. `metrics` `(Optional)`: Metrics to compute for training and evaluation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._cfg = OmegaConf.structured(cfg)\n",
    "\n",
    "        self.save_hyperparameters(self._cfg)\n",
    "        self._train_dl = noop\n",
    "        self._validation_dl = noop\n",
    "        self._test_dl = noop\n",
    "        self._optimizer = noop\n",
    "        self._scheduler = noop\n",
    "        self._trainer = trainer\n",
    "        self.metrics = metrics\n",
    "\n",
    "    def train_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        \"Returns the Dataloader used for Training\"\n",
    "        if self._train_dl is not None and self._train_dl is not noop:\n",
    "            return self._train_dl\n",
    "\n",
    "    def val_dataloader(self) -> Any:\n",
    "        \"Returns the List of Dataloaders or Dataloader used for Validation\"\n",
    "        if self._validation_dl is not None and self._validation_dl is not noop:\n",
    "            return self._validation_dl\n",
    "\n",
    "    def test_dataloader(self) -> Any:\n",
    "        \"Returns the List of Dataloaders or Dataloader used for Testing\"\n",
    "        if self._test_dl is not None and self._test_dl is not noop:\n",
    "            return self._test_dl\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        The Forward method for LightningModule, users should modify this method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def setup_training_data(self, train_data_config: Union[DictConfig, Dict]):\n",
    "        \"\"\"\n",
    "        Setups data loader to be used in training\n",
    "\n",
    "        Arguments:\n",
    "        1. `train_data_config`: training data loader parameters.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def setup_validation_data(self, val_data_config: Union[DictConfig, Dict]):\n",
    "        \"\"\"\n",
    "        Setups data loader to be used in validation\n",
    "\n",
    "        Arguments:\n",
    "        1. `val_data_config`: validation data loader parameters.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def setup_test_data(\n",
    "        self, test_data_config: Optional[Union[DictConfig, Dict]] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        (Optionally) Setups data loader to be used in test\n",
    "\n",
    "        Arguments:\n",
    "        1. `test_data_config`: test data loader parameters.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def param_dicts(self) -> Union[Iterator, List[Dict]]:\n",
    "        \"\"\"\n",
    "        Property that returns the param dicts for optimization.\n",
    "        Override for custom training behaviour. Currently returns all the trainable paramters.\n",
    "        \"\"\"\n",
    "        return L(self).map(trainable_params)\n",
    "\n",
    "    def shared_step(self, batch: Any, batch_idx: int, stage: str) -> Any:\n",
    "        \"\"\"\n",
    "        The common training/validation/test step. Override for custom behavior. This step\n",
    "        is shared between training/validation/test step. For training/validation/test steps\n",
    "        `stage` is train/val/test respectively. You training logic should go here avoid directly overriding\n",
    "        training/validation/test step methods.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int) -> Any:\n",
    "        \"\"\"\n",
    "        The training step of the LightningModule. For common use cases you need\n",
    "        not need to override this method. See `GaleTask.shared_step()`\n",
    "        \"\"\"\n",
    "        return self.shared_step(batch, batch_idx, stage=\"train\")\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int) -> None:\n",
    "        \"\"\"\n",
    "        The validation step of the LightningModule. For common use cases you need\n",
    "        not need to override this method. See `GaleTask.shared_step()`\n",
    "        \"\"\"\n",
    "        return self.shared_step(batch, batch_idx, stage=\"val\")\n",
    "\n",
    "    def test_step(self, batch: Any, batch_idx: int) -> None:\n",
    "        \"\"\"\n",
    "        The test step of the LightningModule. For common use cases you need\n",
    "        not need to override this method. See `GaleTask.shared_step()`\n",
    "        \"\"\"\n",
    "        return self.shared_step(batch, batch_idx, stage=\"test\")\n",
    "\n",
    "    def setup_optimization(self, optim_config: DictConfig = None):\n",
    "        \"\"\"\n",
    "        Prepares an optimizer from a string name and its optional config parameters.\n",
    "\n",
    "        Args:\n",
    "        1. `optim_config`: A `dictionary`/`DictConfig` or instance of `OptimizationConfig`.\n",
    "        \"\"\"\n",
    "        # If config was not explicitly passed to us\n",
    "        if optim_config is None:\n",
    "            # See if internal config has `optim` namespace\n",
    "            if self._cfg is not None and hasattr(self._cfg, \"optimization\"):\n",
    "                optim_config = self._cfg.optimization\n",
    "\n",
    "        # If config is still None, or internal config has no Optim, return without instantiation\n",
    "        if optim_config is None:\n",
    "            msg = \"No optimizer config provided, therefore no optimizer was created\"\n",
    "            log_main_process(_logger, logging.WARNING, msg)\n",
    "            return\n",
    "\n",
    "        else:\n",
    "            # Preserve the configuration\n",
    "            if not isinstance(optim_config, DictConfig):\n",
    "                optim_config = OmegaConf.create(optim_config)\n",
    "\n",
    "            # prepare the optimization config\n",
    "            self.prepare_optimization_config(optim_config)\n",
    "\n",
    "            # Setup optimizer and scheduler\n",
    "            self._optimizer = self.build_optimizer(self.param_dicts)\n",
    "            self._scheduler = self.build_lr_scheduler(self._optimizer)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Choose what optimizers and learning-rate schedulers to use in your optimization.\n",
    "        See https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html\n",
    "        \"\"\"\n",
    "        # if self.setup_optimization() has been called manually no\n",
    "        # need to call again\n",
    "        if self._optimizer is noop and self._scheduler is noop:\n",
    "            self.setup_optimization()\n",
    "\n",
    "        if self._scheduler is None:\n",
    "            return self._optimizer\n",
    "        else:\n",
    "            return [self._optimizer], [self._scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods and Properties "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.forward\" class=\"doc_header\"><code>GaleTask.forward</code><a href=\"__main__.py#L48\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.forward</code>(**`x`**:`Tensor`)\n",
       "\n",
       "The Forward method for LightningModule, users should modify this method."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 28;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.forward)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.forward)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.train_dataloader\" class=\"doc_header\"><code>GaleTask.train_dataloader</code><a href=\"__main__.py#L33\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.train_dataloader</code>()\n",
       "\n",
       "Returns the Dataloader used for Training"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 29;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.train_dataloader)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.train_dataloader)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.val_dataloader\" class=\"doc_header\"><code>GaleTask.val_dataloader</code><a href=\"__main__.py#L38\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.val_dataloader</code>()\n",
       "\n",
       "Returns the List of Dataloaders or Dataloader used for Validation"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 31;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.val_dataloader)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.val_dataloader)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.test_dataloader\" class=\"doc_header\"><code>GaleTask.test_dataloader</code><a href=\"__main__.py#L43\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.test_dataloader</code>()\n",
       "\n",
       "Returns the List of Dataloaders or Dataloader used for Testing"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 33;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.test_dataloader)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.test_dataloader)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.setup_training_data\" class=\"doc_header\"><code>GaleTask.setup_training_data</code><a href=\"__main__.py#L55\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.setup_training_data</code>(**`train_data_config`**:`Union`\\[`DictConfig`, `Dict`\\])\n",
       "\n",
       "Setups data loader to be used in training\n",
       "\n",
       "Arguments:\n",
       "1. `train_data_config`: training data loader parameters."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 30;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.setup_training_data)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.setup_training_data)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.setup_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.setup_validation_data\" class=\"doc_header\"><code>GaleTask.setup_validation_data</code><a href=\"__main__.py#L65\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.setup_validation_data</code>(**`val_data_config`**:`Union`\\[`DictConfig`, `Dict`\\])\n",
       "\n",
       "Setups data loader to be used in validation\n",
       "\n",
       "Arguments:\n",
       "1. `val_data_config`: validation data loader parameters."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 32;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.setup_validation_data)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.setup_validation_data)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.setup_validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.setup_test_data\" class=\"doc_header\"><code>GaleTask.setup_test_data</code><a href=\"__main__.py#L75\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.setup_test_data</code>(**`test_data_config`**:`Union`\\[`DictConfig`, `Dict`, `NoneType`\\]=*`None`*)\n",
       "\n",
       "(Optionally) Setups data loader to be used in test\n",
       "\n",
       "Arguments:\n",
       "1. `test_data_config`: test data loader parameters."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 34;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.setup_test_data)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.setup_test_data)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.setup_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.shared_step\" class=\"doc_header\"><code>GaleTask.shared_step</code><a href=\"__main__.py#L94\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.shared_step</code>(**`batch`**:`Any`, **`batch_idx`**:`int`, **`stage`**:`str`)\n",
       "\n",
       "The common training/validation/test step. Override for custom behavior. This step\n",
       "is shared between training/validation/test step. For training/validation/test steps\n",
       "`stage` is train/val/test respectively. You training logic should go here avoid directly overriding\n",
       "training/validation/test step methods."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 36;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.shared_step)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.shared_step)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.shared_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.training_step\" class=\"doc_header\"><code>GaleTask.training_step</code><a href=\"__main__.py#L103\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.training_step</code>(**`batch`**:`Any`, **`batch_idx`**:`int`)\n",
       "\n",
       "The training step of the LightningModule. For common use cases you need\n",
       "not need to override this method. See [`GaleTask.shared_step()`](/gale/core.classes.html#GaleTask.shared_step())"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 37;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.training_step)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.training_step)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.training_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.validation_step\" class=\"doc_header\"><code>GaleTask.validation_step</code><a href=\"__main__.py#L110\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.validation_step</code>(**`batch`**:`Any`, **`batch_idx`**:`int`)\n",
       "\n",
       "The validation step of the LightningModule. For common use cases you need\n",
       "not need to override this method. See [`GaleTask.shared_step()`](/gale/core.classes.html#GaleTask.shared_step())"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 38;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.validation_step)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.validation_step)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.validation_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.test_step\" class=\"doc_header\"><code>GaleTask.test_step</code><a href=\"__main__.py#L117\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.test_step</code>(**`batch`**:`Any`, **`batch_idx`**:`int`)\n",
       "\n",
       "The test step of the LightningModule. For common use cases you need\n",
       "not need to override this method. See [`GaleTask.shared_step()`](/gale/core.classes.html#GaleTask.shared_step())"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 39;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.test_step)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.test_step)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.test_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.param_dicts\" class=\"doc_header\"><code>GaleTask.param_dicts</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "Property that returns the param dicts for optimization.\n",
       "Override for custom training behaviour. Currently returns all the trainable paramters."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 35;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.param_dicts)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.param_dicts)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.param_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.configure_optimizers\" class=\"doc_header\"><code>GaleTask.configure_optimizers</code><a href=\"__main__.py#L155\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.configure_optimizers</code>()\n",
       "\n",
       "Choose what optimizers and learning-rate schedulers to use in your optimization.\n",
       "See https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 41;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.configure_optimizers)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.configure_optimizers)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.configure_optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.setup_optimization\" class=\"doc_header\"><code>GaleTask.setup_optimization</code><a href=\"__main__.py#L124\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.setup_optimization</code>(**`optim_config`**:`DictConfig`=*`None`*)\n",
       "\n",
       "Prepares an optimizer from a string name and its optional config parameters.\n",
       "\n",
       "Args:\n",
       "1. `optim_config`: A `dictionary`/`DictConfig` or instance of `OptimizationConfig`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 40;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.setup_optimization)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.setup_optimization)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.setup_optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.utils.logger.ipynb.\n",
      "Converted 00a_core.utils.visualize.ipynb.\n",
      "Converted 00b_core.utils.structures.ipynb.\n",
      "Converted 01_core.nn.utils.ipynb.\n",
      "Converted 01a_core.nn.losses.ipynb.\n",
      "Converted 02_core.nn.optim.optimizers.ipynb.\n",
      "Converted 02a_core.nn.optim.lr_schedulers.ipynb.\n",
      "Converted 03_core.classes.ipynb.\n",
      "Converted 04_classification.modelling.backbones.ipynb.\n",
      "Converted 04a_classification.modelling.heads.ipynb.\n",
      "Converted 04b_classification.modelling.meta_arch.common.ipynb.\n",
      "Converted 04b_classification.modelling.meta_arch.vit.ipynb.\n",
      "Converted 05_classification.data.common.ipynb.\n",
      "Converted 05a_classification.data.transforms.ipynb.\n",
      "Converted 05b_classification.data.build.ipynb.\n",
      "Converted 06_classification.task.ipynb.\n",
      "Converted 07_collections.pandas.ipynb.\n",
      "Converted 07a_collections.callbacks.notebook.ipynb.\n",
      "Converted 07b_collections.callbacks.ema.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 42;\n",
       "                var nbb_unformatted_code = \"# hide\\nnotebook2script()\";\n",
       "                var nbb_formatted_code = \"# hide\\nnotebook2script()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gale_dev",
   "language": "python",
   "name": "gale_dev"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
