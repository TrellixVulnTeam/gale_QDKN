{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# hide\\n%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\";\n",
       "                var nbb_formatted_code = \"# hide\\n%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "%load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# hide\\nimport warnings\\n\\nfrom nbdev.export import *\\nfrom nbdev.showdoc import *\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\";\n",
       "                var nbb_formatted_code = \"# hide\\nimport warnings\\n\\nfrom nbdev.export import *\\nfrom nbdev.showdoc import *\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "import warnings\n",
    "\n",
    "from nbdev.export import *\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes\n",
    "> Interfaces common to all `Modules` and `Models` in Gale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# export\\nimport copy\\nimport math\\nfrom abc import ABC, ABCMeta, abstractmethod\\nfrom contextlib import contextmanager\\nfrom typing import *\\n\\nimport hydra\\nimport pytorch_lightning as pl\\nimport torch\\nfrom fastcore.all import L, noop, patch\\nfrom omegaconf import DictConfig, OmegaConf\\nfrom torch.nn import Module\\nimport torchmetrics\\n\\nfrom gale.config import BaseGaleConfig, DatasetsConfig, OptimizationConfig\\nfrom gale.config.optimizers import create_optimizer\\nfrom gale.core.logging import setup_logger\\nfrom gale.core.nn.utils import params, trainable_params\\n\\n_logger = setup_logger()\";\n",
       "                var nbb_formatted_code = \"# export\\nimport copy\\nimport math\\nfrom abc import ABC, ABCMeta, abstractmethod\\nfrom contextlib import contextmanager\\nfrom typing import *\\n\\nimport hydra\\nimport pytorch_lightning as pl\\nimport torch\\nfrom fastcore.all import L, noop, patch\\nfrom omegaconf import DictConfig, OmegaConf\\nfrom torch.nn import Module\\nimport torchmetrics\\n\\nfrom gale.config import BaseGaleConfig, DatasetsConfig, OptimizationConfig\\nfrom gale.config.optimizers import create_optimizer\\nfrom gale.core.logging import setup_logger\\nfrom gale.core.nn.utils import params, trainable_params\\n\\n_logger = setup_logger()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "import copy\n",
    "import math\n",
    "from abc import ABC, ABCMeta, abstractmethod\n",
    "from contextlib import contextmanager\n",
    "from typing import *\n",
    "\n",
    "import hydra\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from fastcore.all import L, noop, patch\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from torch.nn import Module\n",
    "import torchmetrics\n",
    "\n",
    "from gale.config import BaseGaleConfig, DatasetsConfig, OptimizationConfig\n",
    "from gale.config.optimizers import create_optimizer\n",
    "from gale.core.logging import setup_logger\n",
    "from gale.core.nn.utils import params, trainable_params\n",
    "\n",
    "_logger = setup_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# export\\nclass Configurable(ABC):\\n    @classmethod\\n    def from_config_dict(cls, config: DictConfig):\\n        \\\"\\\"\\\"Instantiates object using `DictConfig-based` configuration\\\"\\\"\\\"\\n        # Resolve the config dict\\n        if isinstance(config, DictConfig):\\n            config = OmegaConf.to_container(config, resolve=True)\\n            config = OmegaConf.create(config)\\n\\n        if \\\"_target_\\\" in config:\\n            # regular hydra-based instantiation\\n            instance = hydra.utils.instantiate(config=config)\\n        else:\\n            # instantiate directly using kwargs\\n            try:\\n                instance = cls(cfg=config)\\n            except:\\n                instance = cls(**config)\\n\\n        if not hasattr(instance, \\\"_cfg\\\"):\\n            instance._cfg = config\\n        return instance\";\n",
       "                var nbb_formatted_code = \"# export\\nclass Configurable(ABC):\\n    @classmethod\\n    def from_config_dict(cls, config: DictConfig):\\n        \\\"\\\"\\\"Instantiates object using `DictConfig-based` configuration\\\"\\\"\\\"\\n        # Resolve the config dict\\n        if isinstance(config, DictConfig):\\n            config = OmegaConf.to_container(config, resolve=True)\\n            config = OmegaConf.create(config)\\n\\n        if \\\"_target_\\\" in config:\\n            # regular hydra-based instantiation\\n            instance = hydra.utils.instantiate(config=config)\\n        else:\\n            # instantiate directly using kwargs\\n            try:\\n                instance = cls(cfg=config)\\n            except:\\n                instance = cls(**config)\\n\\n        if not hasattr(instance, \\\"_cfg\\\"):\\n            instance._cfg = config\\n        return instance\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class Configurable(ABC):\n",
    "    @classmethod\n",
    "    def from_config_dict(cls, config: DictConfig):\n",
    "        \"\"\"Instantiates object using `DictConfig-based` configuration\"\"\"\n",
    "        # Resolve the config dict\n",
    "        if isinstance(config, DictConfig):\n",
    "            config = OmegaConf.to_container(config, resolve=True)\n",
    "            config = OmegaConf.create(config)\n",
    "\n",
    "        if \"_target_\" in config:\n",
    "            # regular hydra-based instantiation\n",
    "            instance = hydra.utils.instantiate(config=config)\n",
    "        else:\n",
    "            # instantiate directly using kwargs\n",
    "            try:\n",
    "                instance = cls(cfg=config)\n",
    "            except:\n",
    "                instance = cls(**config)\n",
    "\n",
    "        if not hasattr(instance, \"_cfg\"):\n",
    "            instance._cfg = config\n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# export\\nclass GaleModule(Module, Configurable, metaclass=ABCMeta):\\n    \\\"\\\"\\\"\\n    Abstract class offering interface which should be implemented by all `Backbones`,\\n    `Heads` and `Meta Archs` in gale.\\n    \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def forward(self) -> Any:\\n        \\\"\\\"\\\"\\n        The main logic for the model lives here. Can return either features, logits\\n        or loss.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def build_param_dicts(self) -> Union[Iterable, List[Dict], Dict, List]:\\n        \\\"\\\"\\\"\\n        Should return the iterable of parameters to optimize or dicts defining parameter groups\\n        for the Module.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @property\\n    def param_lists(self):\\n        \\\"Returns the list of paramters in the module\\\"\\n        return [p for p in self.parameters()]\\n\\n    def all_params(self, n=slice(None), with_grad=False):\\n        \\\"List of `param_groups` upto n\\\"\\n        res = L(p for p in self.param_lists[n])\\n        return (\\n            L(o for o in res if hasattr(o, \\\"grad\\\") and o.grad is not None)\\n            if with_grad\\n            else res\\n        )\\n\\n    def _set_require_grad(self, rg, p):\\n        p.requires_grad_(rg)\\n\\n    def unfreeze(self) -> None:\\n        \\\"\\\"\\\"\\n        Unfreeze all parameters for training.\\n        \\\"\\\"\\\"\\n        for param in self.parameters():\\n            param.requires_grad = True\\n\\n        self.train()\\n\\n    def freeze(self) -> None:\\n        \\\"\\\"\\\"\\n        Freeze all params for inference & set model to eval\\n        \\\"\\\"\\\"\\n        for param in self.parameters():\\n            param.requires_grad = False\\n        self.eval()\\n\\n    def freeze_to(self, n) -> None:\\n        \\\"Freeze parameter groups up to `n`\\\"\\n        self.frozen_idx = n if n >= 0 else len(self.param_lists) + n\\n        if self.frozen_idx >= len(self.param_lists):\\n            _logger.warning(\\n                f\\\"Freezing {self.frozen_idx} groups; model has {len(self.param_lists)}; whole model is frozen.\\\"\\n            )\\n\\n        for o in self.all_params(slice(n, None)):\\n            self._set_require_grad(True, o)\\n\\n        for o in self.all_params(slice(None, n)):\\n            self._set_require_grad(False, o)\\n\\n    @contextmanager\\n    def as_frozen(self):\\n        \\\"\\\"\\\"\\n        Context manager which temporarily freezes a module, yields control\\n        and finally unfreezes the module.\\n        \\\"\\\"\\\"\\n        self.freeze()\\n\\n        try:\\n            yield\\n        finally:\\n            self.unfreeze()\";\n",
       "                var nbb_formatted_code = \"# export\\nclass GaleModule(Module, Configurable, metaclass=ABCMeta):\\n    \\\"\\\"\\\"\\n    Abstract class offering interface which should be implemented by all `Backbones`,\\n    `Heads` and `Meta Archs` in gale.\\n    \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def forward(self) -> Any:\\n        \\\"\\\"\\\"\\n        The main logic for the model lives here. Can return either features, logits\\n        or loss.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def build_param_dicts(self) -> Union[Iterable, List[Dict], Dict, List]:\\n        \\\"\\\"\\\"\\n        Should return the iterable of parameters to optimize or dicts defining parameter groups\\n        for the Module.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @property\\n    def param_lists(self):\\n        \\\"Returns the list of paramters in the module\\\"\\n        return [p for p in self.parameters()]\\n\\n    def all_params(self, n=slice(None), with_grad=False):\\n        \\\"List of `param_groups` upto n\\\"\\n        res = L(p for p in self.param_lists[n])\\n        return (\\n            L(o for o in res if hasattr(o, \\\"grad\\\") and o.grad is not None)\\n            if with_grad\\n            else res\\n        )\\n\\n    def _set_require_grad(self, rg, p):\\n        p.requires_grad_(rg)\\n\\n    def unfreeze(self) -> None:\\n        \\\"\\\"\\\"\\n        Unfreeze all parameters for training.\\n        \\\"\\\"\\\"\\n        for param in self.parameters():\\n            param.requires_grad = True\\n\\n        self.train()\\n\\n    def freeze(self) -> None:\\n        \\\"\\\"\\\"\\n        Freeze all params for inference & set model to eval\\n        \\\"\\\"\\\"\\n        for param in self.parameters():\\n            param.requires_grad = False\\n        self.eval()\\n\\n    def freeze_to(self, n) -> None:\\n        \\\"Freeze parameter groups up to `n`\\\"\\n        self.frozen_idx = n if n >= 0 else len(self.param_lists) + n\\n        if self.frozen_idx >= len(self.param_lists):\\n            _logger.warning(\\n                f\\\"Freezing {self.frozen_idx} groups; model has {len(self.param_lists)}; whole model is frozen.\\\"\\n            )\\n\\n        for o in self.all_params(slice(n, None)):\\n            self._set_require_grad(True, o)\\n\\n        for o in self.all_params(slice(None, n)):\\n            self._set_require_grad(False, o)\\n\\n    @contextmanager\\n    def as_frozen(self):\\n        \\\"\\\"\\\"\\n        Context manager which temporarily freezes a module, yields control\\n        and finally unfreezes the module.\\n        \\\"\\\"\\\"\\n        self.freeze()\\n\\n        try:\\n            yield\\n        finally:\\n            self.unfreeze()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class GaleModule(Module, Configurable, metaclass=ABCMeta):\n",
    "    \"\"\"\n",
    "    Abstract class offering interface which should be implemented by all `Backbones`,\n",
    "    `Heads` and `Meta Archs` in gale.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self) -> Any:\n",
    "        \"\"\"\n",
    "        The main logic for the model lives here. Can return either features, logits\n",
    "        or loss.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def build_param_dicts(self) -> Union[Iterable, List[Dict], Dict, List]:\n",
    "        \"\"\"\n",
    "        Should return the iterable of parameters to optimize or dicts defining parameter groups\n",
    "        for the Module.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def param_lists(self):\n",
    "        \"Returns the list of paramters in the module\"\n",
    "        return [p for p in self.parameters()]\n",
    "\n",
    "    def all_params(self, n=slice(None), with_grad=False):\n",
    "        \"List of `param_groups` upto n\"\n",
    "        res = L(p for p in self.param_lists[n])\n",
    "        return (\n",
    "            L(o for o in res if hasattr(o, \"grad\") and o.grad is not None)\n",
    "            if with_grad\n",
    "            else res\n",
    "        )\n",
    "\n",
    "    def _set_require_grad(self, rg, p):\n",
    "        p.requires_grad_(rg)\n",
    "\n",
    "    def unfreeze(self) -> None:\n",
    "        \"\"\"\n",
    "        Unfreeze all parameters for training.\n",
    "        \"\"\"\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.train()\n",
    "\n",
    "    def freeze(self) -> None:\n",
    "        \"\"\"\n",
    "        Freeze all params for inference & set model to eval\n",
    "        \"\"\"\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.eval()\n",
    "\n",
    "    def freeze_to(self, n) -> None:\n",
    "        \"Freeze parameter groups up to `n`\"\n",
    "        self.frozen_idx = n if n >= 0 else len(self.param_lists) + n\n",
    "        if self.frozen_idx >= len(self.param_lists):\n",
    "            _logger.warning(\n",
    "                f\"Freezing {self.frozen_idx} groups; model has {len(self.param_lists)}; whole model is frozen.\"\n",
    "            )\n",
    "\n",
    "        for o in self.all_params(slice(n, None)):\n",
    "            self._set_require_grad(True, o)\n",
    "\n",
    "        for o in self.all_params(slice(None, n)):\n",
    "            self._set_require_grad(False, o)\n",
    "\n",
    "    @contextmanager\n",
    "    def as_frozen(self):\n",
    "        \"\"\"\n",
    "        Context manager which temporarily freezes a module, yields control\n",
    "        and finally unfreezes the module.\n",
    "        \"\"\"\n",
    "        self.freeze()\n",
    "\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            self.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"GaleModule\" class=\"doc_header\"><code>class</code> <code>GaleModule</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>GaleModule</code>() :: `Module`\n",
       "\n",
       "Abstract class offering interface which should be implemented by all `Backbones`,\n",
       "`Heads` and `Meta Archs` in gale."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleModule)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleModule)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleModule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Required Methods -*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.forward\" class=\"doc_header\"><code>GaleModule.forward</code><a href=\"__main__.py#L8\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleModule.forward</code>()\n",
       "\n",
       "The main logic for the model lives here. Can return either features, logits\n",
       "or loss."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.build_param_dicts\" class=\"doc_header\"><code>GaleModule.build_param_dicts</code><a href=\"__main__.py#L16\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleModule.build_param_dicts</code>()\n",
       "\n",
       "Should return the iterable of parameters to optimize or dicts defining parameter groups\n",
       "for the Module."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 36;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleModule.forward)\\nshow_doc(GaleModule.build_param_dicts)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleModule.forward)\\nshow_doc(GaleModule.build_param_dicts)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleModule.forward)\n",
    "show_doc(GaleModule.build_param_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Extra functionality -*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Configurable.from_config_dict\" class=\"doc_header\"><code>Configurable.from_config_dict</code><a href=\"__main__.py#L3\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Configurable.from_config_dict</code>(**`config`**:`DictConfig`)\n",
       "\n",
       "Instantiates object using `DictConfig-based` configuration"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.param_lists\" class=\"doc_header\"><code>GaleModule.param_lists</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "Returns the list of paramters in the module"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.all_params\" class=\"doc_header\"><code>GaleModule.all_params</code><a href=\"__main__.py#L29\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleModule.all_params</code>(**`n`**=*`slice(None, None, None)`*, **`with_grad`**=*`False`*)\n",
       "\n",
       "List of `param_groups` upto n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.freeze\" class=\"doc_header\"><code>GaleModule.freeze</code><a href=\"__main__.py#L50\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleModule.freeze</code>()\n",
       "\n",
       "Freeze all params for inference & set model to eval"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.freeze_to\" class=\"doc_header\"><code>GaleModule.freeze_to</code><a href=\"__main__.py#L58\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleModule.freeze_to</code>(**`n`**)\n",
       "\n",
       "Freeze parameter groups up to `n`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.unfreeze\" class=\"doc_header\"><code>GaleModule.unfreeze</code><a href=\"__main__.py#L41\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleModule.unfreeze</code>()\n",
       "\n",
       "Unfreeze all parameters for training."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleModule.as_frozen\" class=\"doc_header\"><code>GaleModule.as_frozen</code><a href=\"__main__.py#L72\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleModule.as_frozen</code>()\n",
       "\n",
       "Context manager which temporarily freezes a module, yields control\n",
       "and finally unfreezes the module."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 37;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleModule.from_config_dict)\\nshow_doc(GaleModule.param_lists)\\nshow_doc(GaleModule.all_params)\\nshow_doc(GaleModule.freeze)\\nshow_doc(GaleModule.freeze_to)\\nshow_doc(GaleModule.unfreeze)\\nshow_doc(GaleModule.as_frozen)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleModule.from_config_dict)\\nshow_doc(GaleModule.param_lists)\\nshow_doc(GaleModule.all_params)\\nshow_doc(GaleModule.freeze)\\nshow_doc(GaleModule.freeze_to)\\nshow_doc(GaleModule.unfreeze)\\nshow_doc(GaleModule.as_frozen)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleModule.from_config_dict)\n",
    "show_doc(GaleModule.param_lists)\n",
    "show_doc(GaleModule.all_params)\n",
    "show_doc(GaleModule.freeze)\n",
    "show_doc(GaleModule.freeze_to)\n",
    "show_doc(GaleModule.unfreeze)\n",
    "show_doc(GaleModule.as_frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"# export\\nclass OptimSchedBuilder:\\n    \\\"\\\"\\\"\\n    Interface that constructs an Optimizer and Scheduler for `GaleTask` from config.\\n    \\\"\\\"\\\"\\n\\n    _train_dl: Callable\\n    _trainer: pl.Trainer\\n    optimization_cfg: DictConfig\";\n",
       "                var nbb_formatted_code = \"# export\\nclass OptimSchedBuilder:\\n    \\\"\\\"\\\"\\n    Interface that constructs an Optimizer and Scheduler for `GaleTask` from config.\\n    \\\"\\\"\\\"\\n\\n    _train_dl: Callable\\n    _trainer: pl.Trainer\\n    optimization_cfg: DictConfig\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class OptimSchedBuilder:\n",
    "    \"\"\"\n",
    "    Interface that constructs an Optimizer and Scheduler for `GaleTask` from config.\n",
    "    \"\"\"\n",
    "\n",
    "    _train_dl: Callable\n",
    "    _trainer: pl.Trainer\n",
    "    optimization_cfg: DictConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Config: \n",
      "optimizer:\n",
      "  _target_: torch.optim.adamw.AdamW\n",
      "  lr: 0.001\n",
      "  weight_decay: 0.01\n",
      "  betas:\n",
      "  - 0.95\n",
      "  - 0.999\n",
      "  eps: 1.0e-05\n",
      "  amsgrad: false\n",
      "scheduler:\n",
      "  _target_: torch.optim.lr_scheduler.OneCycleLR\n",
      "  max_lr: 0.001\n",
      "  total_steps: null\n",
      "  epochs: null\n",
      "  steps_per_epoch: null\n",
      "  pct_start: 0.3\n",
      "  anneal_strategy: cos\n",
      "  cycle_momentum: true\n",
      "  base_momentum: 0.85\n",
      "  max_momentum: 0.95\n",
      "  div_factor: 25.0\n",
      "  final_div_factor: 10000.0\n",
      "  last_epoch: -1\n",
      "interval: epoch\n",
      "monitor: null\n",
      "steps_per_epoch: null\n",
      "max_steps: null\n",
      "max_epochs: null\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"# collapse-output\\nfrom dataclasses import dataclass, field\\n\\nfrom fastcore.all import Path\\nfrom nbdev.export import Config\\nfrom torch.utils.data import DataLoader\\nfrom torchvision.datasets import FashionMNIST\\n\\nfrom gale.config.optimizers import *\\nfrom gale.config.schedulers import *\\n\\n# Create a mock config\\nopt_cfg = OptimizationConfig(AdamWConfig(lr=1e-03), OneCycleLRConfig(max_lr=1e-03))\\ncfg = BaseGaleConfig(optimization=opt_cfg, datasets=DatasetsConfig())\\ncfg = OmegaConf.structured(cfg)\\n\\ndata_path = Path(Config().path(\\\"nbs_path\\\")) / \\\"data\\\"\\ndset = FashionMNIST(root=data_path, download=True)\\n\\nbuilder = OptimSchedBuilder()\\n# create mock dataloaders and trainer for builder\\nbuilder._train_dl = DataLoader(dset, batch_size=32)\\nbuilder._trainer = pl.Trainer(max_epochs=10, accumulate_grad_batches=1)\\n\\nprint(f\\\"Original Config: \\\\n{OmegaConf.to_yaml(cfg.optimization)}\\\")\";\n",
       "                var nbb_formatted_code = \"# collapse-output\\nfrom dataclasses import dataclass, field\\n\\nfrom fastcore.all import Path\\nfrom nbdev.export import Config\\nfrom torch.utils.data import DataLoader\\nfrom torchvision.datasets import FashionMNIST\\n\\nfrom gale.config.optimizers import *\\nfrom gale.config.schedulers import *\\n\\n# Create a mock config\\nopt_cfg = OptimizationConfig(AdamWConfig(lr=1e-03), OneCycleLRConfig(max_lr=1e-03))\\ncfg = BaseGaleConfig(optimization=opt_cfg, datasets=DatasetsConfig())\\ncfg = OmegaConf.structured(cfg)\\n\\ndata_path = Path(Config().path(\\\"nbs_path\\\")) / \\\"data\\\"\\ndset = FashionMNIST(root=data_path, download=True)\\n\\nbuilder = OptimSchedBuilder()\\n# create mock dataloaders and trainer for builder\\nbuilder._train_dl = DataLoader(dset, batch_size=32)\\nbuilder._trainer = pl.Trainer(max_epochs=10, accumulate_grad_batches=1)\\n\\nprint(f\\\"Original Config: \\\\n{OmegaConf.to_yaml(cfg.optimization)}\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# collapse-output\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from fastcore.all import Path\n",
    "from nbdev.export import Config\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "from gale.config.optimizers import *\n",
    "from gale.config.schedulers import *\n",
    "\n",
    "# Create a mock config\n",
    "opt_cfg = OptimizationConfig(AdamWConfig(lr=1e-03), OneCycleLRConfig(max_lr=1e-03))\n",
    "cfg = BaseGaleConfig(optimization=opt_cfg, datasets=DatasetsConfig())\n",
    "cfg = OmegaConf.structured(cfg)\n",
    "\n",
    "data_path = Path(Config().path(\"nbs_path\")) / \"data\"\n",
    "dset = FashionMNIST(root=data_path, download=True)\n",
    "\n",
    "builder = OptimSchedBuilder()\n",
    "# create mock dataloaders and trainer for builder\n",
    "builder._train_dl = DataLoader(dset, batch_size=32)\n",
    "builder._trainer = pl.Trainer(max_epochs=10, accumulate_grad_batches=1)\n",
    "\n",
    "print(f\"Original Config: \\n{OmegaConf.to_yaml(cfg.optimization)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"# export\\n@patch\\ndef prepare_optimization_config(self: OptimSchedBuilder, config: DictConfig):\\n    \\\"\\\"\\\"\\n    Prepares `OptimizationConfig` config and adds some interval\\n    values and infers values like max_steps, max_epochs, etc. This method\\n    takes in a config that is inherited from `BaseGaleConfig`. This method also fills in\\n    the values for `max_iters` & `epochs`, `steps_per_epoch` which are required by some of\\n    the LearningRate Schedulers.\\n    \\\"\\\"\\\"\\n    opt_config = copy.deepcopy(config)\\n    self.optimization_cfg = opt_config\\n\\n    self.optimization_cfg[\\\"steps_per_epoch\\\"] = len(self._train_dl)\\n\\n    if self._trainer.max_epochs is None and self._trainer.max_steps is None:\\n        _logger.error(\\n            \\\"Either one of max_epochs or max_epochs must be provided in Trainer\\\"\\n        )\\n        raise ValueError\\n\\n    if self._trainer.max_steps is None:\\n        # calculate max_steps\\n        self.optimization_cfg[\\\"max_epochs\\\"] = self._trainer.max_epochs\\n        accumulate_grad_batches = self._trainer.accumulate_grad_batches\\n        limit_train_batches = self._trainer.limit_train_batches\\n        steps_per_epoch = self.optimization_cfg[\\\"steps_per_epoch\\\"]\\n\\n        if isinstance(limit_train_batches, int) or limit_train_batches == 0.0:\\n            steps_per_epoch = min(steps_per_epoch, int(limit_train_batches))\\n        elif steps_per_epoch != float(\\\"inf\\\"):\\n            steps_per_epoch = int(steps_per_epoch * limit_train_batches)\\n            if accumulate_grad_batches == 1:\\n                steps_per_epoch = max(steps_per_epoch, 1)\\n        max_steps = (\\n            math.ceil(steps_per_epoch / accumulate_grad_batches)\\n            * self.optimization_cfg[\\\"max_epochs\\\"]\\n        )\\n\\n        self.optimization_cfg[\\\"max_steps\\\"] = max_steps\\n\\n    else:\\n        self.optimization_cfg[\\\"max_steps\\\"] = self._trainer.max_steps\\n        # compute max epochs\\n        self.optimization_cfg[\\\"max_epochs\\\"] = self._trainer.max_steps * len(\\n            self._train_dl\\n        )\\n\\n    # covert config to Dictionary\\n    sched_config = OmegaConf.to_container(self.optimization_cfg.scheduler, resolve=True)\\n\\n    # populate values in learning rate schedulers\\n    if \\\"max_iters\\\" in sched_config:\\n        if sched_config[\\\"max_iters\\\"] == \\\"???\\\" or sched_config[\\\"max_iters\\\"] is None:\\n            OmegaConf.update(\\n                self.optimization_cfg,\\n                \\\"scheduler.max_iters\\\",\\n                self.optimization_cfg[\\\"max_steps\\\"],\\n            )\\n\\n    if \\\"epochs\\\" in sched_config:\\n        if sched_config[\\\"epochs\\\"] == \\\"???\\\" or sched_config[\\\"epochs\\\"] is None:\\n            OmegaConf.update(\\n                self.optimization_cfg,\\n                \\\"scheduler.epochs\\\",\\n                self.optimization_cfg[\\\"max_epochs\\\"],\\n            )\\n\\n    if \\\"steps_per_epoch\\\" in sched_config:\\n        if (\\n            sched_config[\\\"steps_per_epoch\\\"] == \\\"???\\\"\\n            or sched_config[\\\"steps_per_epoch\\\"] is None\\n        ):\\n            OmegaConf.update(\\n                self.optimization_cfg,\\n                \\\"scheduler.steps_per_epoch\\\",\\n                self.optimization_cfg[\\\"steps_per_epoch\\\"],\\n            )\";\n",
       "                var nbb_formatted_code = \"# export\\n@patch\\ndef prepare_optimization_config(self: OptimSchedBuilder, config: DictConfig):\\n    \\\"\\\"\\\"\\n    Prepares `OptimizationConfig` config and adds some interval\\n    values and infers values like max_steps, max_epochs, etc. This method\\n    takes in a config that is inherited from `BaseGaleConfig`. This method also fills in\\n    the values for `max_iters` & `epochs`, `steps_per_epoch` which are required by some of\\n    the LearningRate Schedulers.\\n    \\\"\\\"\\\"\\n    opt_config = copy.deepcopy(config)\\n    self.optimization_cfg = opt_config\\n\\n    self.optimization_cfg[\\\"steps_per_epoch\\\"] = len(self._train_dl)\\n\\n    if self._trainer.max_epochs is None and self._trainer.max_steps is None:\\n        _logger.error(\\n            \\\"Either one of max_epochs or max_epochs must be provided in Trainer\\\"\\n        )\\n        raise ValueError\\n\\n    if self._trainer.max_steps is None:\\n        # calculate max_steps\\n        self.optimization_cfg[\\\"max_epochs\\\"] = self._trainer.max_epochs\\n        accumulate_grad_batches = self._trainer.accumulate_grad_batches\\n        limit_train_batches = self._trainer.limit_train_batches\\n        steps_per_epoch = self.optimization_cfg[\\\"steps_per_epoch\\\"]\\n\\n        if isinstance(limit_train_batches, int) or limit_train_batches == 0.0:\\n            steps_per_epoch = min(steps_per_epoch, int(limit_train_batches))\\n        elif steps_per_epoch != float(\\\"inf\\\"):\\n            steps_per_epoch = int(steps_per_epoch * limit_train_batches)\\n            if accumulate_grad_batches == 1:\\n                steps_per_epoch = max(steps_per_epoch, 1)\\n        max_steps = (\\n            math.ceil(steps_per_epoch / accumulate_grad_batches)\\n            * self.optimization_cfg[\\\"max_epochs\\\"]\\n        )\\n\\n        self.optimization_cfg[\\\"max_steps\\\"] = max_steps\\n\\n    else:\\n        self.optimization_cfg[\\\"max_steps\\\"] = self._trainer.max_steps\\n        # compute max epochs\\n        self.optimization_cfg[\\\"max_epochs\\\"] = self._trainer.max_steps * len(\\n            self._train_dl\\n        )\\n\\n    # covert config to Dictionary\\n    sched_config = OmegaConf.to_container(self.optimization_cfg.scheduler, resolve=True)\\n\\n    # populate values in learning rate schedulers\\n    if \\\"max_iters\\\" in sched_config:\\n        if sched_config[\\\"max_iters\\\"] == \\\"???\\\" or sched_config[\\\"max_iters\\\"] is None:\\n            OmegaConf.update(\\n                self.optimization_cfg,\\n                \\\"scheduler.max_iters\\\",\\n                self.optimization_cfg[\\\"max_steps\\\"],\\n            )\\n\\n    if \\\"epochs\\\" in sched_config:\\n        if sched_config[\\\"epochs\\\"] == \\\"???\\\" or sched_config[\\\"epochs\\\"] is None:\\n            OmegaConf.update(\\n                self.optimization_cfg,\\n                \\\"scheduler.epochs\\\",\\n                self.optimization_cfg[\\\"max_epochs\\\"],\\n            )\\n\\n    if \\\"steps_per_epoch\\\" in sched_config:\\n        if (\\n            sched_config[\\\"steps_per_epoch\\\"] == \\\"???\\\"\\n            or sched_config[\\\"steps_per_epoch\\\"] is None\\n        ):\\n            OmegaConf.update(\\n                self.optimization_cfg,\\n                \\\"scheduler.steps_per_epoch\\\",\\n                self.optimization_cfg[\\\"steps_per_epoch\\\"],\\n            )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@patch\n",
    "def prepare_optimization_config(self: OptimSchedBuilder, config: DictConfig):\n",
    "    \"\"\"\n",
    "    Prepares `OptimizationConfig` config and adds some interval\n",
    "    values and infers values like max_steps, max_epochs, etc. This method\n",
    "    takes in a config that is inherited from `BaseGaleConfig`. This method also fills in\n",
    "    the values for `max_iters` & `epochs`, `steps_per_epoch` which are required by some of\n",
    "    the LearningRate Schedulers.\n",
    "    \"\"\"\n",
    "    opt_config = copy.deepcopy(config)\n",
    "    self.optimization_cfg = opt_config\n",
    "\n",
    "    self.optimization_cfg[\"steps_per_epoch\"] = len(self._train_dl)\n",
    "\n",
    "    if self._trainer.max_epochs is None and self._trainer.max_steps is None:\n",
    "        _logger.error(\n",
    "            \"Either one of max_epochs or max_epochs must be provided in Trainer\"\n",
    "        )\n",
    "        raise ValueError\n",
    "\n",
    "    if self._trainer.max_steps is None:\n",
    "        # calculate max_steps\n",
    "        self.optimization_cfg[\"max_epochs\"] = self._trainer.max_epochs\n",
    "        accumulate_grad_batches = self._trainer.accumulate_grad_batches\n",
    "        limit_train_batches = self._trainer.limit_train_batches\n",
    "        steps_per_epoch = self.optimization_cfg[\"steps_per_epoch\"]\n",
    "\n",
    "        if isinstance(limit_train_batches, int) or limit_train_batches == 0.0:\n",
    "            steps_per_epoch = min(steps_per_epoch, int(limit_train_batches))\n",
    "        elif steps_per_epoch != float(\"inf\"):\n",
    "            steps_per_epoch = int(steps_per_epoch * limit_train_batches)\n",
    "            if accumulate_grad_batches == 1:\n",
    "                steps_per_epoch = max(steps_per_epoch, 1)\n",
    "        max_steps = (\n",
    "            math.ceil(steps_per_epoch / accumulate_grad_batches)\n",
    "            * self.optimization_cfg[\"max_epochs\"]\n",
    "        )\n",
    "\n",
    "        self.optimization_cfg[\"max_steps\"] = max_steps\n",
    "\n",
    "    else:\n",
    "        self.optimization_cfg[\"max_steps\"] = self._trainer.max_steps\n",
    "        # compute max epochs\n",
    "        self.optimization_cfg[\"max_epochs\"] = self._trainer.max_steps * len(\n",
    "            self._train_dl\n",
    "        )\n",
    "\n",
    "    # covert config to Dictionary\n",
    "    sched_config = OmegaConf.to_container(self.optimization_cfg.scheduler, resolve=True)\n",
    "\n",
    "    # populate values in learning rate schedulers\n",
    "    if \"max_iters\" in sched_config:\n",
    "        if sched_config[\"max_iters\"] == \"???\" or sched_config[\"max_iters\"] is None:\n",
    "            OmegaConf.update(\n",
    "                self.optimization_cfg,\n",
    "                \"scheduler.max_iters\",\n",
    "                self.optimization_cfg[\"max_steps\"],\n",
    "            )\n",
    "\n",
    "    if \"epochs\" in sched_config:\n",
    "        if sched_config[\"epochs\"] == \"???\" or sched_config[\"epochs\"] is None:\n",
    "            OmegaConf.update(\n",
    "                self.optimization_cfg,\n",
    "                \"scheduler.epochs\",\n",
    "                self.optimization_cfg[\"max_epochs\"],\n",
    "            )\n",
    "\n",
    "    if \"steps_per_epoch\" in sched_config:\n",
    "        if (\n",
    "            sched_config[\"steps_per_epoch\"] == \"???\"\n",
    "            or sched_config[\"steps_per_epoch\"] is None\n",
    "        ):\n",
    "            OmegaConf.update(\n",
    "                self.optimization_cfg,\n",
    "                \"scheduler.steps_per_epoch\",\n",
    "                self.optimization_cfg[\"steps_per_epoch\"],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified Optimization Config: \n",
      "optimizer:\n",
      "  _target_: torch.optim.adamw.AdamW\n",
      "  lr: 0.001\n",
      "  weight_decay: 0.01\n",
      "  betas:\n",
      "  - 0.95\n",
      "  - 0.999\n",
      "  eps: 1.0e-05\n",
      "  amsgrad: false\n",
      "scheduler:\n",
      "  _target_: torch.optim.lr_scheduler.OneCycleLR\n",
      "  max_lr: 0.001\n",
      "  total_steps: null\n",
      "  epochs: 10\n",
      "  steps_per_epoch: 1875\n",
      "  pct_start: 0.3\n",
      "  anneal_strategy: cos\n",
      "  cycle_momentum: true\n",
      "  base_momentum: 0.85\n",
      "  max_momentum: 0.95\n",
      "  div_factor: 25.0\n",
      "  final_div_factor: 10000.0\n",
      "  last_epoch: -1\n",
      "interval: epoch\n",
      "monitor: null\n",
      "steps_per_epoch: 1875\n",
      "max_steps: 18750\n",
      "max_epochs: 10\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"# collapse-output\\nbuilder.prepare_optimization_config(config=cfg.optimization)\\nprint(f\\\"Modified Optimization Config: \\\\n{OmegaConf.to_yaml(builder.optimization_cfg)}\\\")\";\n",
       "                var nbb_formatted_code = \"# collapse-output\\nbuilder.prepare_optimization_config(config=cfg.optimization)\\nprint(f\\\"Modified Optimization Config: \\\\n{OmegaConf.to_yaml(builder.optimization_cfg)}\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# collapse-output\n",
    "builder.prepare_optimization_config(config=cfg.optimization)\n",
    "print(f\"Modified Optimization Config: \\n{OmegaConf.to_yaml(builder.optimization_cfg)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 21;\n",
       "                var nbb_unformatted_code = \"# export\\n@patch\\ndef build_optimizer(\\n    self: OptimSchedBuilder, params: Union[Iterable, Any]\\n) -> torch.optim.Optimizer:\\n    \\\"\\\"\\\"\\n    Builds a single optimizer from `OptimizationConfig`. `params` are the parameter\\n    dict with the weights for the optimizer to optimizer.\\n\\n    Note this method must be called after `prepare_optimization_config()`\\n    \\\"\\\"\\\"\\n    if not isinstance(self.optimization_cfg, DictConfig):\\n        _logger.warning(\\n            \\\"optimization_cfg not found, did you call `prepare_optimization_config`.\\\"\\n        )\\n        raise NameError\\n    else:\\n        if (\\n            self.optimization_cfg.optimizer is None\\n            and self.optimization_cfg.optimizer._target_ is None\\n        ):\\n            _logger.warning(\\\"Optimizer is None, so no optimizer will be created.\\\")\\n            opt = None\\n        else:\\n            opt = create_optimizer(self.optimization_cfg.optimizer, params=params)\\n        return opt\";\n",
       "                var nbb_formatted_code = \"# export\\n@patch\\ndef build_optimizer(\\n    self: OptimSchedBuilder, params: Union[Iterable, Any]\\n) -> torch.optim.Optimizer:\\n    \\\"\\\"\\\"\\n    Builds a single optimizer from `OptimizationConfig`. `params` are the parameter\\n    dict with the weights for the optimizer to optimizer.\\n\\n    Note this method must be called after `prepare_optimization_config()`\\n    \\\"\\\"\\\"\\n    if not isinstance(self.optimization_cfg, DictConfig):\\n        _logger.warning(\\n            \\\"optimization_cfg not found, did you call `prepare_optimization_config`.\\\"\\n        )\\n        raise NameError\\n    else:\\n        if (\\n            self.optimization_cfg.optimizer is None\\n            and self.optimization_cfg.optimizer._target_ is None\\n        ):\\n            _logger.warning(\\\"Optimizer is None, so no optimizer will be created.\\\")\\n            opt = None\\n        else:\\n            opt = create_optimizer(self.optimization_cfg.optimizer, params=params)\\n        return opt\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@patch\n",
    "def build_optimizer(\n",
    "    self: OptimSchedBuilder, params: Union[Iterable, Any]\n",
    ") -> torch.optim.Optimizer:\n",
    "    \"\"\"\n",
    "    Builds a single optimizer from `OptimizationConfig`. `params` are the parameter\n",
    "    dict with the weights for the optimizer to optimizer.\n",
    "\n",
    "    Note this method must be called after `prepare_optimization_config()`\n",
    "    \"\"\"\n",
    "    if not isinstance(self.optimization_cfg, DictConfig):\n",
    "        _logger.warning(\n",
    "            \"optimization_cfg not found, did you call `prepare_optimization_config`.\"\n",
    "        )\n",
    "        raise NameError\n",
    "    else:\n",
    "        if (\n",
    "            self.optimization_cfg.optimizer is None\n",
    "            and self.optimization_cfg.optimizer._target_ is None\n",
    "        ):\n",
    "            _logger.warning(\"Optimizer is None, so no optimizer will be created.\")\n",
    "            opt = None\n",
    "        else:\n",
    "            opt = create_optimizer(self.optimization_cfg.optimizer, params=params)\n",
    "        return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 22;\n",
       "                var nbb_unformatted_code = \"params = [torch.nn.Parameter(torch.randn(1, 2))]\\noptimizer = builder.build_optimizer(params)\\nassert isinstance(optimizer, torch.optim.AdamW)\";\n",
       "                var nbb_formatted_code = \"params = [torch.nn.Parameter(torch.randn(1, 2))]\\noptimizer = builder.build_optimizer(params)\\nassert isinstance(optimizer, torch.optim.AdamW)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = [torch.nn.Parameter(torch.randn(1, 2))]\n",
    "optimizer = builder.build_optimizer(params)\n",
    "assert isinstance(optimizer, torch.optim.AdamW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 23;\n",
       "                var nbb_unformatted_code = \"# export\\n@patch\\ndef build_lr_scheduler(\\n    self: OptimSchedBuilder, optimizer: torch.optim.Optimizer\\n) -> Union[None, Dict]:\\n    \\\"\\\"\\\"\\n    Builds a LearningRate scheduler from `OptimizationConfig`. Returns an LRScheduler dict\\n    that is required by PyTorch Lightning for LRSchedulers.\\n    Note this method must be called after `prepare_optimization_config()`\\n    \\\"\\\"\\\"\\n    if not isinstance(self.optimization_cfg, DictConfig):\\n        _logger.warning(\\n            \\\"optimization_cfg not found, did you call `prepare_optimization_config`.\\\"\\n        )\\n        raise NameError\\n    else:\\n        if (\\n            self.optimization_cfg.scheduler._target_ is None\\n            and self.optimization_cfg.scheduler is None\\n        ):\\n            _logger.warning(\\\"scheduler is None, so no scheduler will be created.\\\")\\n            sched = None\\n        else:\\n            _temp = OmegaConf.to_container(\\n                self.optimization_cfg.scheduler, resolve=True\\n            )\\n            kwds = {}\\n            c_sch = {}\\n\\n            # if a key value is ListConfig then we convert it to simple list\\n            for key, value in _temp.items():\\n                if isinstance(value, list):\\n                    kwds[key] = list(value)\\n                else:\\n                    c_sch[key] = value\\n\\n            sched = hydra.utils.instantiate(c_sch, optimizer=optimizer, **kwds)\\n\\n            # convert the lr_scheduler to pytorch-lightning LRScheduler dictionary format\\n            sched = dict(\\n                scheduler=sched,\\n                interval=self.optimization_cfg.interval,\\n                monitor=self.optimization_cfg.monitor,\\n            )\\n            return sched\";\n",
       "                var nbb_formatted_code = \"# export\\n@patch\\ndef build_lr_scheduler(\\n    self: OptimSchedBuilder, optimizer: torch.optim.Optimizer\\n) -> Union[None, Dict]:\\n    \\\"\\\"\\\"\\n    Builds a LearningRate scheduler from `OptimizationConfig`. Returns an LRScheduler dict\\n    that is required by PyTorch Lightning for LRSchedulers.\\n    Note this method must be called after `prepare_optimization_config()`\\n    \\\"\\\"\\\"\\n    if not isinstance(self.optimization_cfg, DictConfig):\\n        _logger.warning(\\n            \\\"optimization_cfg not found, did you call `prepare_optimization_config`.\\\"\\n        )\\n        raise NameError\\n    else:\\n        if (\\n            self.optimization_cfg.scheduler._target_ is None\\n            and self.optimization_cfg.scheduler is None\\n        ):\\n            _logger.warning(\\\"scheduler is None, so no scheduler will be created.\\\")\\n            sched = None\\n        else:\\n            _temp = OmegaConf.to_container(\\n                self.optimization_cfg.scheduler, resolve=True\\n            )\\n            kwds = {}\\n            c_sch = {}\\n\\n            # if a key value is ListConfig then we convert it to simple list\\n            for key, value in _temp.items():\\n                if isinstance(value, list):\\n                    kwds[key] = list(value)\\n                else:\\n                    c_sch[key] = value\\n\\n            sched = hydra.utils.instantiate(c_sch, optimizer=optimizer, **kwds)\\n\\n            # convert the lr_scheduler to pytorch-lightning LRScheduler dictionary format\\n            sched = dict(\\n                scheduler=sched,\\n                interval=self.optimization_cfg.interval,\\n                monitor=self.optimization_cfg.monitor,\\n            )\\n            return sched\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@patch\n",
    "def build_lr_scheduler(\n",
    "    self: OptimSchedBuilder, optimizer: torch.optim.Optimizer\n",
    ") -> Union[None, Dict]:\n",
    "    \"\"\"\n",
    "    Builds a LearningRate scheduler from `OptimizationConfig`. Returns an LRScheduler dict\n",
    "    that is required by PyTorch Lightning for LRSchedulers.\n",
    "    Note this method must be called after `prepare_optimization_config()`\n",
    "    \"\"\"\n",
    "    if not isinstance(self.optimization_cfg, DictConfig):\n",
    "        _logger.warning(\n",
    "            \"optimization_cfg not found, did you call `prepare_optimization_config`.\"\n",
    "        )\n",
    "        raise NameError\n",
    "    else:\n",
    "        if (\n",
    "            self.optimization_cfg.scheduler._target_ is None\n",
    "            and self.optimization_cfg.scheduler is None\n",
    "        ):\n",
    "            _logger.warning(\"scheduler is None, so no scheduler will be created.\")\n",
    "            sched = None\n",
    "        else:\n",
    "            _temp = OmegaConf.to_container(\n",
    "                self.optimization_cfg.scheduler, resolve=True\n",
    "            )\n",
    "            kwds = {}\n",
    "            c_sch = {}\n",
    "\n",
    "            # if a key value is ListConfig then we convert it to simple list\n",
    "            for key, value in _temp.items():\n",
    "                if isinstance(value, list):\n",
    "                    kwds[key] = list(value)\n",
    "                else:\n",
    "                    c_sch[key] = value\n",
    "\n",
    "            sched = hydra.utils.instantiate(c_sch, optimizer=optimizer, **kwds)\n",
    "\n",
    "            # convert the lr_scheduler to pytorch-lightning LRScheduler dictionary format\n",
    "            sched = dict(\n",
    "                scheduler=sched,\n",
    "                interval=self.optimization_cfg.interval,\n",
    "                monitor=self.optimization_cfg.monitor,\n",
    "            )\n",
    "            return sched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scheduler': <torch.optim.lr_scheduler.OneCycleLR object at 0x7ffd2c1f4630>, 'interval': 'epoch', 'monitor': None}\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 24;\n",
       "                var nbb_unformatted_code = \"scheduler = builder.build_lr_scheduler(optimizer)\\nassert isinstance(scheduler, Dict)\\nassert isinstance(scheduler[\\\"scheduler\\\"], torch.optim.lr_scheduler.OneCycleLR)\\nprint(scheduler)\";\n",
       "                var nbb_formatted_code = \"scheduler = builder.build_lr_scheduler(optimizer)\\nassert isinstance(scheduler, Dict)\\nassert isinstance(scheduler[\\\"scheduler\\\"], torch.optim.lr_scheduler.OneCycleLR)\\nprint(scheduler)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scheduler = builder.build_lr_scheduler(optimizer)\n",
    "assert isinstance(scheduler, Dict)\n",
    "assert isinstance(scheduler[\"scheduler\"], torch.optim.lr_scheduler.OneCycleLR)\n",
    "print(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 63;\n",
       "                var nbb_unformatted_code = \"# export\\nclass GaleTask(pl.LightningModule, OptimSchedBuilder, metaclass=ABCMeta):\\n    \\\"\\\"\\\"\\n    Interface for Pytorch-lightning based Gale modules\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        cfg: DictConfig,\\n        trainer: Optional[pl.Trainer] = None,\\n        metrics: Union[torchmetrics.Metric, Mapping, Sequence, None] = None,\\n    ):\\n        \\\"\\\"\\\"\\n        Base class from which all NeMo models should inherit.\\n\\n        Arguments:\\n        1. `cfg` `(DictConfig)`:  configuration object. cfg object should be inherited from `BaseGaleConfig`.\\n        2. `trainer` `(Optional, pl.Trainer)`: Pytorch Lightning Trainer instance\\n        3. `metrics` `(Optional)`: Metrics to compute for training and evaluation.\\n        \\\"\\\"\\\"\\n        super().__init__()\\n        self._cfg = OmegaConf.structured(cfg)\\n\\n        self.save_hyperparameters(self._cfg)\\n        self._train_dl = noop\\n        self._validation_dl = noop\\n        self._test_dl = noop\\n        self._optimizer = noop\\n        self._scheduler = noop\\n        self._trainer = trainer\\n        self.metrics = metrics\\n\\n    def train_dataloader(self) -> torch.utils.data.DataLoader:\\n        \\\"Returns the Dataloader used for Training\\\"\\n        if self._train_dl is not None and self._train_dl is not noop:\\n            return self._train_dl\\n\\n    def val_dataloader(self) -> Any:\\n        \\\"Returns the List of Dataloaders or Dataloader used for Validation\\\"\\n        if self._validation_dl is not None and self._validation_dl is not noop:\\n            return self._validation_dl\\n\\n    def test_dataloader(self) -> Any:\\n        \\\"Returns the List of Dataloaders or Dataloader used for Testing\\\"\\n        if self._test_dl is not None and self._test_dl is not noop:\\n            return self._test_dl\\n\\n    @abstractmethod\\n    def forward(self, x: torch.Tensor):\\n        \\\"\\\"\\\"\\n        The Forward method for LightningModule, users should modify this method.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def setup_training_data(self, train_data_config: Union[DictConfig, Dict]):\\n        \\\"\\\"\\\"\\n        Setups data loader to be used in training\\n\\n        Arguments:\\n        1. `train_data_config`: training data loader parameters.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def setup_validation_data(self, val_data_config: Union[DictConfig, Dict]):\\n        \\\"\\\"\\\"\\n        Setups data loader to be used in validation\\n\\n        Arguments:\\n        1. `val_data_config`: validation data loader parameters.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    def setup_test_data(\\n        self, test_data_config: Optional[Union[DictConfig, Dict]] = None\\n    ):\\n        \\\"\\\"\\\"\\n        (Optionally) Setups data loader to be used in test\\n\\n        Arguments:\\n        1. `test_data_config`: test data loader parameters.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @property\\n    def param_dicts(self) -> Union[Iterator, List[Dict]]:\\n        \\\"\\\"\\\"\\n        Property that returns the param dicts for optimization.\\n        Override for custom training behaviour. Currently returns all the trainable paramters.\\n        \\\"\\\"\\\"\\n        return L(self).map(trainable_params)\";\n",
       "                var nbb_formatted_code = \"# export\\nclass GaleTask(pl.LightningModule, OptimSchedBuilder, metaclass=ABCMeta):\\n    \\\"\\\"\\\"\\n    Interface for Pytorch-lightning based Gale modules\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        cfg: DictConfig,\\n        trainer: Optional[pl.Trainer] = None,\\n        metrics: Union[torchmetrics.Metric, Mapping, Sequence, None] = None,\\n    ):\\n        \\\"\\\"\\\"\\n        Base class from which all NeMo models should inherit.\\n\\n        Arguments:\\n        1. `cfg` `(DictConfig)`:  configuration object. cfg object should be inherited from `BaseGaleConfig`.\\n        2. `trainer` `(Optional, pl.Trainer)`: Pytorch Lightning Trainer instance\\n        3. `metrics` `(Optional)`: Metrics to compute for training and evaluation.\\n        \\\"\\\"\\\"\\n        super().__init__()\\n        self._cfg = OmegaConf.structured(cfg)\\n\\n        self.save_hyperparameters(self._cfg)\\n        self._train_dl = noop\\n        self._validation_dl = noop\\n        self._test_dl = noop\\n        self._optimizer = noop\\n        self._scheduler = noop\\n        self._trainer = trainer\\n        self.metrics = metrics\\n\\n    def train_dataloader(self) -> torch.utils.data.DataLoader:\\n        \\\"Returns the Dataloader used for Training\\\"\\n        if self._train_dl is not None and self._train_dl is not noop:\\n            return self._train_dl\\n\\n    def val_dataloader(self) -> Any:\\n        \\\"Returns the List of Dataloaders or Dataloader used for Validation\\\"\\n        if self._validation_dl is not None and self._validation_dl is not noop:\\n            return self._validation_dl\\n\\n    def test_dataloader(self) -> Any:\\n        \\\"Returns the List of Dataloaders or Dataloader used for Testing\\\"\\n        if self._test_dl is not None and self._test_dl is not noop:\\n            return self._test_dl\\n\\n    @abstractmethod\\n    def forward(self, x: torch.Tensor):\\n        \\\"\\\"\\\"\\n        The Forward method for LightningModule, users should modify this method.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def setup_training_data(self, train_data_config: Union[DictConfig, Dict]):\\n        \\\"\\\"\\\"\\n        Setups data loader to be used in training\\n\\n        Arguments:\\n        1. `train_data_config`: training data loader parameters.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def setup_validation_data(self, val_data_config: Union[DictConfig, Dict]):\\n        \\\"\\\"\\\"\\n        Setups data loader to be used in validation\\n\\n        Arguments:\\n        1. `val_data_config`: validation data loader parameters.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    def setup_test_data(\\n        self, test_data_config: Optional[Union[DictConfig, Dict]] = None\\n    ):\\n        \\\"\\\"\\\"\\n        (Optionally) Setups data loader to be used in test\\n\\n        Arguments:\\n        1. `test_data_config`: test data loader parameters.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    @property\\n    def param_dicts(self) -> Union[Iterator, List[Dict]]:\\n        \\\"\\\"\\\"\\n        Property that returns the param dicts for optimization.\\n        Override for custom training behaviour. Currently returns all the trainable paramters.\\n        \\\"\\\"\\\"\\n        return L(self).map(trainable_params)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class GaleTask(pl.LightningModule, OptimSchedBuilder, metaclass=ABCMeta):\n",
    "    \"\"\"\n",
    "    Interface for Pytorch-lightning based Gale modules\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: DictConfig,\n",
    "        trainer: Optional[pl.Trainer] = None,\n",
    "        metrics: Union[torchmetrics.Metric, Mapping, Sequence, None] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Base class from which all NeMo models should inherit.\n",
    "\n",
    "        Arguments:\n",
    "        1. `cfg` `(DictConfig)`:  configuration object. cfg object should be inherited from `BaseGaleConfig`.\n",
    "        2. `trainer` `(Optional, pl.Trainer)`: Pytorch Lightning Trainer instance\n",
    "        3. `metrics` `(Optional)`: Metrics to compute for training and evaluation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._cfg = OmegaConf.structured(cfg)\n",
    "\n",
    "        self.save_hyperparameters(self._cfg)\n",
    "        self._train_dl = noop\n",
    "        self._validation_dl = noop\n",
    "        self._test_dl = noop\n",
    "        self._optimizer = noop\n",
    "        self._scheduler = noop\n",
    "        self._trainer = trainer\n",
    "        self.metrics = metrics\n",
    "\n",
    "    def train_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        \"Returns the Dataloader used for Training\"\n",
    "        if self._train_dl is not None and self._train_dl is not noop:\n",
    "            return self._train_dl\n",
    "\n",
    "    def val_dataloader(self) -> Any:\n",
    "        \"Returns the List of Dataloaders or Dataloader used for Validation\"\n",
    "        if self._validation_dl is not None and self._validation_dl is not noop:\n",
    "            return self._validation_dl\n",
    "\n",
    "    def test_dataloader(self) -> Any:\n",
    "        \"Returns the List of Dataloaders or Dataloader used for Testing\"\n",
    "        if self._test_dl is not None and self._test_dl is not noop:\n",
    "            return self._test_dl\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        The Forward method for LightningModule, users should modify this method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def setup_training_data(self, train_data_config: Union[DictConfig, Dict]):\n",
    "        \"\"\"\n",
    "        Setups data loader to be used in training\n",
    "\n",
    "        Arguments:\n",
    "        1. `train_data_config`: training data loader parameters.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def setup_validation_data(self, val_data_config: Union[DictConfig, Dict]):\n",
    "        \"\"\"\n",
    "        Setups data loader to be used in validation\n",
    "\n",
    "        Arguments:\n",
    "        1. `val_data_config`: validation data loader parameters.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def setup_test_data(\n",
    "        self, test_data_config: Optional[Union[DictConfig, Dict]] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        (Optionally) Setups data loader to be used in test\n",
    "\n",
    "        Arguments:\n",
    "        1. `test_data_config`: test data loader parameters.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def param_dicts(self) -> Union[Iterator, List[Dict]]:\n",
    "        \"\"\"\n",
    "        Property that returns the param dicts for optimization.\n",
    "        Override for custom training behaviour. Currently returns all the trainable paramters.\n",
    "        \"\"\"\n",
    "        return L(self).map(trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"GaleTask\" class=\"doc_header\"><code>class</code> <code>GaleTask</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>GaleTask</code>(**`cfg`**:`DictConfig`, **`trainer`**:`Optional`\\[`Trainer`\\]=*`None`*, **`metrics`**:`Union`\\[`Metric`, `Mapping`, `Sequence`, `NoneType`\\]=*`None`*) :: `LightningModule`\n",
       "\n",
       "Interface for Pytorch-lightning based Gale modules"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.train_dataloader\" class=\"doc_header\"><code>GaleTask.train_dataloader</code><a href=\"__main__.py#L33\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.train_dataloader</code>()\n",
       "\n",
       "Returns the Dataloader used for Training"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.setup_training_data\" class=\"doc_header\"><code>GaleTask.setup_training_data</code><a href=\"__main__.py#L55\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.setup_training_data</code>(**`train_data_config`**:`Union`\\[`DictConfig`, `Dict`\\])\n",
       "\n",
       "Setups data loader to be used in training\n",
       "\n",
       "Arguments:\n",
       "1. `train_data_config`: training data loader parameters."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.val_dataloader\" class=\"doc_header\"><code>GaleTask.val_dataloader</code><a href=\"__main__.py#L38\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.val_dataloader</code>()\n",
       "\n",
       "Returns the List of Dataloaders or Dataloader used for Validation"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.setup_validation_data\" class=\"doc_header\"><code>GaleTask.setup_validation_data</code><a href=\"__main__.py#L65\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.setup_validation_data</code>(**`val_data_config`**:`Union`\\[`DictConfig`, `Dict`\\])\n",
       "\n",
       "Setups data loader to be used in validation\n",
       "\n",
       "Arguments:\n",
       "1. `val_data_config`: validation data loader parameters."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.test_dataloader\" class=\"doc_header\"><code>GaleTask.test_dataloader</code><a href=\"__main__.py#L43\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.test_dataloader</code>()\n",
       "\n",
       "Returns the List of Dataloaders or Dataloader used for Testing"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.setup_test_data\" class=\"doc_header\"><code>GaleTask.setup_test_data</code><a href=\"__main__.py#L75\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.setup_test_data</code>(**`test_data_config`**:`Union`\\[`DictConfig`, `Dict`, `NoneType`\\]=*`None`*)\n",
       "\n",
       "(Optionally) Setups data loader to be used in test\n",
       "\n",
       "Arguments:\n",
       "1. `test_data_config`: test data loader parameters."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 64;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask)\\n\\nshow_doc(GaleTask.train_dataloader)\\nshow_doc(GaleTask.setup_training_data)\\n\\nshow_doc(GaleTask.val_dataloader)\\nshow_doc(GaleTask.setup_validation_data)\\n\\nshow_doc(GaleTask.test_dataloader)\\nshow_doc(GaleTask.setup_test_data)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask)\\n\\nshow_doc(GaleTask.train_dataloader)\\nshow_doc(GaleTask.setup_training_data)\\n\\nshow_doc(GaleTask.val_dataloader)\\nshow_doc(GaleTask.setup_validation_data)\\n\\nshow_doc(GaleTask.test_dataloader)\\nshow_doc(GaleTask.setup_test_data)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask)\n",
    "\n",
    "show_doc(GaleTask.train_dataloader)\n",
    "show_doc(GaleTask.setup_training_data)\n",
    "\n",
    "show_doc(GaleTask.val_dataloader)\n",
    "show_doc(GaleTask.setup_validation_data)\n",
    "\n",
    "show_doc(GaleTask.test_dataloader)\n",
    "show_doc(GaleTask.setup_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.forward\" class=\"doc_header\"><code>GaleTask.forward</code><a href=\"__main__.py#L48\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.forward</code>(**`x`**:`Tensor`)\n",
       "\n",
       "The Forward method for LightningModule, users should modify this method."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.param_dicts\" class=\"doc_header\"><code>GaleTask.param_dicts</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "Property that returns the param dicts for optimization.\n",
       "Override for custom training behaviour. Currently returns all the trainable paramters."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 65;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.forward)\\n\\nshow_doc(GaleTask.param_dicts)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.forward)\\n\\nshow_doc(GaleTask.param_dicts)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.forward)\n",
    "\n",
    "show_doc(GaleTask.param_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 66;\n",
       "                var nbb_unformatted_code = \"# export\\n@patch\\ndef shared_step(self: GaleTask, batch: Any, batch_idx: int, stage: str) -> Any:\\n    \\\"\\\"\\\"\\n    The common training/validation/test step. Override for custom behavior. This step\\n    is shared between training/validation/test step. For training/validation/test steps\\n    `stage` is train/val/test respectively. You training logic should go here avoid directly overriding\\n    training/validation/test step methods.\\n    \\\"\\\"\\\"\\n    raise NotImplementedError\";\n",
       "                var nbb_formatted_code = \"# export\\n@patch\\ndef shared_step(self: GaleTask, batch: Any, batch_idx: int, stage: str) -> Any:\\n    \\\"\\\"\\\"\\n    The common training/validation/test step. Override for custom behavior. This step\\n    is shared between training/validation/test step. For training/validation/test steps\\n    `stage` is train/val/test respectively. You training logic should go here avoid directly overriding\\n    training/validation/test step methods.\\n    \\\"\\\"\\\"\\n    raise NotImplementedError\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@patch\n",
    "def shared_step(self: GaleTask, batch: Any, batch_idx: int, stage: str) -> Any:\n",
    "    \"\"\"\n",
    "    The common training/validation/test step. Override for custom behavior. This step\n",
    "    is shared between training/validation/test step. For training/validation/test steps\n",
    "    `stage` is train/val/test respectively. You training logic should go here avoid directly overriding\n",
    "    training/validation/test step methods.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 67;\n",
       "                var nbb_unformatted_code = \"# export\\n@patch\\ndef training_step(self: GaleTask, batch: Any, batch_idx: int) -> Any:\\n    \\\"\\\"\\\"\\n    The training step of the LightningModule. For common use cases you need\\n    not need to override this method. See `GaleTask.shared_step()`\\n    \\\"\\\"\\\"\\n    return self.shared_step(batch, batch_idx, stage=\\\"train\\\")\";\n",
       "                var nbb_formatted_code = \"# export\\n@patch\\ndef training_step(self: GaleTask, batch: Any, batch_idx: int) -> Any:\\n    \\\"\\\"\\\"\\n    The training step of the LightningModule. For common use cases you need\\n    not need to override this method. See `GaleTask.shared_step()`\\n    \\\"\\\"\\\"\\n    return self.shared_step(batch, batch_idx, stage=\\\"train\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@patch\n",
    "def training_step(self: GaleTask, batch: Any, batch_idx: int) -> Any:\n",
    "    \"\"\"\n",
    "    The training step of the LightningModule. For common use cases you need\n",
    "    not need to override this method. See `GaleTask.shared_step()`\n",
    "    \"\"\"\n",
    "    return self.shared_step(batch, batch_idx, stage=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 68;\n",
       "                var nbb_unformatted_code = \"# export\\n@patch\\ndef validation_step(self: GaleTask, batch: Any, batch_idx: int) -> None:\\n    \\\"\\\"\\\"\\n    The validation step of the LightningModule. For common use cases you need\\n    not need to override this method. See `GaleTask.shared_step()`\\n    \\\"\\\"\\\"\\n    return self.shared_step(batch, batch_idx, stage=\\\"val\\\")\";\n",
       "                var nbb_formatted_code = \"# export\\n@patch\\ndef validation_step(self: GaleTask, batch: Any, batch_idx: int) -> None:\\n    \\\"\\\"\\\"\\n    The validation step of the LightningModule. For common use cases you need\\n    not need to override this method. See `GaleTask.shared_step()`\\n    \\\"\\\"\\\"\\n    return self.shared_step(batch, batch_idx, stage=\\\"val\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@patch\n",
    "def validation_step(self: GaleTask, batch: Any, batch_idx: int) -> None:\n",
    "    \"\"\"\n",
    "    The validation step of the LightningModule. For common use cases you need\n",
    "    not need to override this method. See `GaleTask.shared_step()`\n",
    "    \"\"\"\n",
    "    return self.shared_step(batch, batch_idx, stage=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 69;\n",
       "                var nbb_unformatted_code = \"# export\\n@patch\\ndef test_step(self: GaleTask, batch: Any, batch_idx: int) -> None:\\n    \\\"\\\"\\\"\\n    The test step of the LightningModule. For common use cases you need\\n    not need to override this method. See `GaleTask.shared_step()`\\n    \\\"\\\"\\\"\\n    return self.shared_step(batch, batch_idx, stage=\\\"test\\\")\";\n",
       "                var nbb_formatted_code = \"# export\\n@patch\\ndef test_step(self: GaleTask, batch: Any, batch_idx: int) -> None:\\n    \\\"\\\"\\\"\\n    The test step of the LightningModule. For common use cases you need\\n    not need to override this method. See `GaleTask.shared_step()`\\n    \\\"\\\"\\\"\\n    return self.shared_step(batch, batch_idx, stage=\\\"test\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@patch\n",
    "def test_step(self: GaleTask, batch: Any, batch_idx: int) -> None:\n",
    "    \"\"\"\n",
    "    The test step of the LightningModule. For common use cases you need\n",
    "    not need to override this method. See `GaleTask.shared_step()`\n",
    "    \"\"\"\n",
    "    return self.shared_step(batch, batch_idx, stage=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.shared_step\" class=\"doc_header\"><code>GaleTask.shared_step</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.shared_step</code>(**`batch`**:`Any`, **`batch_idx`**:`int`, **`stage`**:`str`)\n",
       "\n",
       "The common training/validation/test step. Override for custom behavior. This step\n",
       "is shared between training/validation/test step. For training/validation/test steps\n",
       "`stage` is train/val/test respectively. You training logic should go here avoid directly overriding\n",
       "training/validation/test step methods."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.training_step\" class=\"doc_header\"><code>GaleTask.training_step</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.training_step</code>(**`batch`**:`Any`, **`batch_idx`**:`int`)\n",
       "\n",
       "The training step of the LightningModule. For common use cases you need\n",
       "not need to override this method. See [`GaleTask.shared_step()`](/gale/core.classes.html#GaleTask.shared_step())"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.validation_step\" class=\"doc_header\"><code>GaleTask.validation_step</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.validation_step</code>(**`batch`**:`Any`, **`batch_idx`**:`int`)\n",
       "\n",
       "The validation step of the LightningModule. For common use cases you need\n",
       "not need to override this method. See [`GaleTask.shared_step()`](/gale/core.classes.html#GaleTask.shared_step())"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.test_step\" class=\"doc_header\"><code>GaleTask.test_step</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.test_step</code>(**`batch`**:`Any`, **`batch_idx`**:`int`)\n",
       "\n",
       "The test step of the LightningModule. For common use cases you need\n",
       "not need to override this method. See [`GaleTask.shared_step()`](/gale/core.classes.html#GaleTask.shared_step())"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 70;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.shared_step)\\nshow_doc(GaleTask.training_step)\\nshow_doc(GaleTask.validation_step)\\nshow_doc(GaleTask.test_step)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.shared_step)\\nshow_doc(GaleTask.training_step)\\nshow_doc(GaleTask.validation_step)\\nshow_doc(GaleTask.test_step)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.shared_step)\n",
    "show_doc(GaleTask.training_step)\n",
    "show_doc(GaleTask.validation_step)\n",
    "show_doc(GaleTask.test_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 71;\n",
       "                var nbb_unformatted_code = \"# export\\n@patch\\ndef setup_optimization(\\n    self: GaleTask,\\n    optim_config: Optional[Union[DictConfig, Dict, OptimizationConfig]] = None,\\n):\\n    \\\"\\\"\\\"\\n    Prepares an optimizer from a string name and its optional config parameters.\\n\\n    Args:\\n    1. `optim_config`: A `dictionary`/`DictConfig` or instance of `OptimizationConfig`.\\n    \\\"\\\"\\\"\\n    # If config was not explicitly passed to us\\n    if optim_config is None:\\n        # See if internal config has `optim` namespace\\n        if self._cfg is not None and hasattr(self._cfg, \\\"optimization\\\"):\\n            optim_config = self._cfg.optimization\\n\\n    # If config is still None, or internal config has no Optim, return without instantiation\\n    if optim_config is None:\\n        _logger.info(\\\"No optimizer config provided, therefore no optimizer was created\\\")\\n        return\\n\\n    else:\\n        # Preserve the configuration\\n        if not isinstance(optim_config, DictConfig):\\n            optim_config = OmegaConf.create(optim_config)\\n\\n        # prepare the optimization config\\n        self.prepare_optimization_config(optim_config)\\n\\n        # Setup optimizer and scheduler\\n        self._optimizer = self.build_optimizer(self.param_dicts)\\n        self._scheduler = self.build_lr_scheduler(self._optimizer)\\n\\n\\n@patch\\ndef configure_optimizers(self: GaleTask):\\n    \\\"\\\"\\\"\\n    Choose what optimizers and learning-rate schedulers to use in your optimization.\\n    See https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html\\n    \\\"\\\"\\\"\\n    # if self.setup_optimization() has been called manually no\\n    # need to call again\\n    if self._optimizer is noop and self._scheduler is noop:\\n        self.setup_optimization()\\n\\n    if self._scheduler is None:\\n        return self._optimizer\\n    else:\\n        return [self._optimizer], [self._scheduler]\";\n",
       "                var nbb_formatted_code = \"# export\\n@patch\\ndef setup_optimization(\\n    self: GaleTask,\\n    optim_config: Optional[Union[DictConfig, Dict, OptimizationConfig]] = None,\\n):\\n    \\\"\\\"\\\"\\n    Prepares an optimizer from a string name and its optional config parameters.\\n\\n    Args:\\n    1. `optim_config`: A `dictionary`/`DictConfig` or instance of `OptimizationConfig`.\\n    \\\"\\\"\\\"\\n    # If config was not explicitly passed to us\\n    if optim_config is None:\\n        # See if internal config has `optim` namespace\\n        if self._cfg is not None and hasattr(self._cfg, \\\"optimization\\\"):\\n            optim_config = self._cfg.optimization\\n\\n    # If config is still None, or internal config has no Optim, return without instantiation\\n    if optim_config is None:\\n        _logger.info(\\\"No optimizer config provided, therefore no optimizer was created\\\")\\n        return\\n\\n    else:\\n        # Preserve the configuration\\n        if not isinstance(optim_config, DictConfig):\\n            optim_config = OmegaConf.create(optim_config)\\n\\n        # prepare the optimization config\\n        self.prepare_optimization_config(optim_config)\\n\\n        # Setup optimizer and scheduler\\n        self._optimizer = self.build_optimizer(self.param_dicts)\\n        self._scheduler = self.build_lr_scheduler(self._optimizer)\\n\\n\\n@patch\\ndef configure_optimizers(self: GaleTask):\\n    \\\"\\\"\\\"\\n    Choose what optimizers and learning-rate schedulers to use in your optimization.\\n    See https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html\\n    \\\"\\\"\\\"\\n    # if self.setup_optimization() has been called manually no\\n    # need to call again\\n    if self._optimizer is noop and self._scheduler is noop:\\n        self.setup_optimization()\\n\\n    if self._scheduler is None:\\n        return self._optimizer\\n    else:\\n        return [self._optimizer], [self._scheduler]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@patch\n",
    "def setup_optimization(\n",
    "    self: GaleTask,\n",
    "    optim_config: Optional[Union[DictConfig, Dict, OptimizationConfig]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepares an optimizer from a string name and its optional config parameters.\n",
    "\n",
    "    Args:\n",
    "    1. `optim_config`: A `dictionary`/`DictConfig` or instance of `OptimizationConfig`.\n",
    "    \"\"\"\n",
    "    # If config was not explicitly passed to us\n",
    "    if optim_config is None:\n",
    "        # See if internal config has `optim` namespace\n",
    "        if self._cfg is not None and hasattr(self._cfg, \"optimization\"):\n",
    "            optim_config = self._cfg.optimization\n",
    "\n",
    "    # If config is still None, or internal config has no Optim, return without instantiation\n",
    "    if optim_config is None:\n",
    "        _logger.info(\"No optimizer config provided, therefore no optimizer was created\")\n",
    "        return\n",
    "\n",
    "    else:\n",
    "        # Preserve the configuration\n",
    "        if not isinstance(optim_config, DictConfig):\n",
    "            optim_config = OmegaConf.create(optim_config)\n",
    "\n",
    "        # prepare the optimization config\n",
    "        self.prepare_optimization_config(optim_config)\n",
    "\n",
    "        # Setup optimizer and scheduler\n",
    "        self._optimizer = self.build_optimizer(self.param_dicts)\n",
    "        self._scheduler = self.build_lr_scheduler(self._optimizer)\n",
    "\n",
    "\n",
    "@patch\n",
    "def configure_optimizers(self: GaleTask):\n",
    "    \"\"\"\n",
    "    Choose what optimizers and learning-rate schedulers to use in your optimization.\n",
    "    See https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html\n",
    "    \"\"\"\n",
    "    # if self.setup_optimization() has been called manually no\n",
    "    # need to call again\n",
    "    if self._optimizer is noop and self._scheduler is noop:\n",
    "        self.setup_optimization()\n",
    "\n",
    "    if self._scheduler is None:\n",
    "        return self._optimizer\n",
    "    else:\n",
    "        return [self._optimizer], [self._scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.setup_optimization\" class=\"doc_header\"><code>GaleTask.setup_optimization</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.setup_optimization</code>(**`optim_config`**:`Union`\\[`DictConfig`, `Dict`, [`OptimizationConfig`](/gale/config.common.html#OptimizationConfig), `NoneType`\\]=*`None`*)\n",
       "\n",
       "Prepares an optimizer from a string name and its optional config parameters.\n",
       "\n",
       "Args:\n",
       "1. `optim_config`: A `dictionary`/`DictConfig` or instance of [`OptimizationConfig`](/gale/config.common.html#OptimizationConfig)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaleTask.configure_optimizers\" class=\"doc_header\"><code>GaleTask.configure_optimizers</code><a href=\"__main__.py#L37\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaleTask.configure_optimizers</code>()\n",
       "\n",
       "Choose what optimizers and learning-rate schedulers to use in your optimization.\n",
       "See https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 72;\n",
       "                var nbb_unformatted_code = \"show_doc(GaleTask.setup_optimization)\\nshow_doc(GaleTask.configure_optimizers)\";\n",
       "                var nbb_formatted_code = \"show_doc(GaleTask.setup_optimization)\\nshow_doc(GaleTask.configure_optimizers)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaleTask.setup_optimization)\n",
    "show_doc(GaleTask.configure_optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.logging.ipynb.\n",
      "Converted 00a_core.structures.ipynb.\n",
      "Converted 00b_core.visualize.ipynb.\n",
      "Converted 01_core.nn.utils.ipynb.\n",
      "Converted 01a_core.nn.losses.ipynb.\n",
      "Converted 01b_core.nn.optim.optimizers.ipynb.\n",
      "Converted 01c_core.nn.optim.lr_schedulers.ipynb.\n",
      "Converted 02_config.optimizers.ipynb.\n",
      "Converted 02a_config.schedulers.ipynb.\n",
      "Converted 02b_config.common.ipynb.\n",
      "Converted 03_core.classes.ipynb.\n",
      "Converted 05_collections.pandas.ipynb.\n",
      "Converted 06a_collections.callbacks.notebook.ipynb.\n",
      "Converted 06b_collections.callbacks.ema.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 74;\n",
       "                var nbb_unformatted_code = \"# hide\\nnotebook2script()\";\n",
       "                var nbb_formatted_code = \"# hide\\nnotebook2script()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gale",
   "language": "python",
   "name": "gale"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
