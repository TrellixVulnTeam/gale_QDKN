{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# skip\n",
    "!git clone https://github.com/benihime91/gale # install gale on colab\n",
    "!pip install -e \"gale[dev]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp classification.model.meta_arch.vision_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# hide\\n%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\\n%matplotlib inline\";\n",
       "                var nbb_formatted_code = \"# hide\\n%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\\n%matplotlib inline\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "%load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# hide\\nimport warnings\\n\\nfrom nbdev.export import *\\nfrom nbdev.showdoc import *\\nfrom timm.utils import *\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\nsetup_default_logging()\";\n",
       "                var nbb_formatted_code = \"# hide\\nimport warnings\\n\\nfrom nbdev.export import *\\nfrom nbdev.showdoc import *\\nfrom timm.utils import *\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\nsetup_default_logging()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "import warnings\n",
    "\n",
    "from nbdev.export import *\n",
    "from nbdev.showdoc import *\n",
    "from timm.utils import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "setup_default_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Architectures : Vision Transformer (ViT) \n",
    "> Pretrained Vision Transformers modified for use in gale from timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# export\\nimport logging\\nfrom collections import namedtuple\\nfrom dataclasses import dataclass\\nfrom typing import *\\n\\nimport timm\\nimport torch\\nfrom fastcore.all import store_attr, use_kwargs_dict\\nfrom omegaconf import MISSING, DictConfig, OmegaConf\\nfrom pytorch_lightning.core.memory import get_human_readable_count\\nfrom timm.optim.optim_factory import add_weight_decay\\n\\nfrom gale.core_classes import BasicModule\\nfrom gale.torch_utils import trainable_params\\nfrom gale.utils.activs import ACTIVATION_REGISTRY\\nfrom gale.utils.shape_spec import ShapeSpec\\n\\n_logger = logging.getLogger(__name__)\";\n",
       "                var nbb_formatted_code = \"# export\\nimport logging\\nfrom collections import namedtuple\\nfrom dataclasses import dataclass\\nfrom typing import *\\n\\nimport timm\\nimport torch\\nfrom fastcore.all import store_attr, use_kwargs_dict\\nfrom omegaconf import MISSING, DictConfig, OmegaConf\\nfrom pytorch_lightning.core.memory import get_human_readable_count\\nfrom timm.optim.optim_factory import add_weight_decay\\n\\nfrom gale.core_classes import BasicModule\\nfrom gale.torch_utils import trainable_params\\nfrom gale.utils.activs import ACTIVATION_REGISTRY\\nfrom gale.utils.shape_spec import ShapeSpec\\n\\n_logger = logging.getLogger(__name__)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "import logging\n",
    "from collections import namedtuple\n",
    "from dataclasses import dataclass\n",
    "from typing import *\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "from fastcore.all import store_attr, use_kwargs_dict\n",
    "from omegaconf import MISSING, DictConfig, OmegaConf\n",
    "from pytorch_lightning.core.memory import get_human_readable_count\n",
    "from timm.optim.optim_factory import add_weight_decay\n",
    "\n",
    "from gale.core_classes import BasicModule\n",
    "from gale.torch_utils import trainable_params\n",
    "from gale.utils.activs import ACTIVATION_REGISTRY\n",
    "from gale.utils.shape_spec import ShapeSpec\n",
    "\n",
    "_logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"# export\\n# @TODO: Add support for Discriminative Lr's\\nclass VisionTransformer(BasicModule):\\n    _hypers = namedtuple(\\\"hypers\\\", field_names=[\\\"lr\\\", \\\"wd\\\"])\\n    \\\"\\\"\\\"\\n    A interface to create a Vision Transformer from timm. For available model check :\\n    https://github.com/rwightman/pytorch-image-models/timm/models/vision_transformer.py\\n    \\\"\\\"\\\"\\n\\n    @use_kwargs_dict(\\n        keep=True,\\n        num_classes=1000,\\n        drop_rate=0.0,\\n        attn_drop_rate=0.0,\\n        drop_path_rate=0.0,\\n    )\\n    def __init__(\\n        self,\\n        model_name: str,\\n        input_shape: ShapeSpec,\\n        lr: float = 1e-03,\\n        wd: float = 1e-05,\\n        pretrained: bool = True,\\n        freeze_to: Optional[int] = None,\\n        finetune: Optional[bool] = None,\\n        act: Optional[str] = None,\\n        reset_classifier: bool = True,\\n        filter_wd: bool = True,\\n        **kwargs,\\n    ):\\n        \\\"\\\"\\\"\\n        Arguments:\\n        1. `input_shape` (ShapeSpec): input image shape. For ViT `height=width` and check the above link for avilable model shapes.\\n        2. `model_name` (str): name of the ViT model, check the above link for avilable models.\\n        3. `pretrained` (bool): load weights pretrained on imagenet.\\n        4. `act` (str): name of the activation layer. Must be registerd in `ACTIVATION_REGISTRY`\\n        5. `num_classes` (int): num output classes.\\n        6. `drop_rate` (float): dropout rate.\\n        7. `attn_drop_rate` (float): attention dropout rate.\\n        8. `drop_path_rate` (float): stochastic depth rate.\\n        9. `reset_classifier` (bool): resets the weights of the classifier.\\n        10. `freeze_to` (int): Freeze the param meter groups of the model upto n.\\n        11. `finetune` (bool): Freeze all the layers and keep only the `classifier` trainable.\\n        \\\"\\\"\\\"\\n        super(VisionTransformer, self).__init__()\\n        # create model from timm\\n        assert input_shape.height == input_shape.width\\n        in_chans = input_shape.channels\\n\\n        if act is not None:\\n            act = ACTIVATION_REGISTRY.get(act)\\n\\n        self.model = timm.create_model(\\n            model_name, pretrained, in_chans=in_chans, act=act, **kwargs\\n        )\\n\\n        if reset_classifier:\\n            num_cls = kwargs.pop(\\\"num_classes\\\")\\n            self.model.reset_classifier(num_cls)\\n\\n        if freeze_to is not None:\\n            self.freeze_to(freeze_to)\\n\\n        if finetune:\\n            if freeze_to is not None and isinstance(freeze_to, int):\\n                msg = \\\"You have sprecified freeze_to along with finetune\\\"\\n                _logger.warning(msg)\\n            _logger.info(\\\"Freezing all the model parameters except for the classifier\\\")\\n            self.freeze()\\n\\n            classifier = [\\\"head\\\", \\\"head_dist\\\"]\\n\\n            for name, module in self.model.named_children():\\n                if name in classifier:\\n                    for p in module.parameters():\\n                        p.requires_grad_(True)\\n\\n        store_attr(\\\"wd, lr, filter_wd, input_shape\\\")\\n\\n    def forward(self, batched_inputs: torch.Tensor) -> torch.Tensor:\\n        \\\"\\\"\\\"\\n        Runs the batched_inputs through the created model.\\n        \\\"\\\"\\\"\\n        out = self.model(batched_inputs)\\n        return out\\n\\n    @classmethod\\n    def from_config_dict(cls, cfg: DictConfig):\\n        \\\"\\\"\\\"\\n        Instantiate the Meta Architecture from gale config\\n        \\\"\\\"\\\"\\n        # fmt: off\\n        input_shape = ShapeSpec(cfg.input.channels, cfg.input.height, cfg.input.width)\\n        _logger.debug(f\\\"Inputs: {input_shape}\\\")\\n        instance = super().from_config_dict(cfg.model.meta_architecture.init_args, input_shape=input_shape)\\n        param_count = get_human_readable_count(sum([m.numel() for m in instance.parameters()]))\\n        _logger.debug('{} created, param count: {}.'.format(cfg.model.meta_architecture.init_args.model_name, param_count))\\n        # fmt: on\\n        return instance\\n\\n    def build_param_dicts(self):\\n        \\\"\\\"\\\"\\n        Builds up the Paramters dicts for optimization.\\n        \\\"\\\"\\\"\\n        if self.filter_wd:\\n            param_lists = add_weight_decay(\\n                self.model,\\n                weight_decay=self.wd,\\n                skip_list=self.model.no_weight_decay(),\\n            )\\n            param_lists[0][\\\"lr\\\"] = self.lr\\n            param_lists[1][\\\"lr\\\"] = self.lr\\n        else:\\n            ps = trainable_params(self.model)\\n            param_lists = dict(params=ps, lr=self.lr, wd=self.wd)\\n        return param_lists\\n\\n    @property\\n    def hypers(self) -> Tuple:\\n        \\\"\\\"\\\"\\n        Returns list of parameters like `lr` and `wd`\\n        for each param group\\n        \\\"\\\"\\\"\\n        lrs = []\\n        wds = []\\n\\n        for p in self.build_param_dicts():\\n            lrs.append(p[\\\"lr\\\"])\\n            wds.append(p[\\\"weight_decay\\\"])\\n        return self._hypers(lrs, wds)\";\n",
       "                var nbb_formatted_code = \"# export\\n# @TODO: Add support for Discriminative Lr's\\nclass VisionTransformer(BasicModule):\\n    _hypers = namedtuple(\\\"hypers\\\", field_names=[\\\"lr\\\", \\\"wd\\\"])\\n    \\\"\\\"\\\"\\n    A interface to create a Vision Transformer from timm. For available model check :\\n    https://github.com/rwightman/pytorch-image-models/timm/models/vision_transformer.py\\n    \\\"\\\"\\\"\\n\\n    @use_kwargs_dict(\\n        keep=True,\\n        num_classes=1000,\\n        drop_rate=0.0,\\n        attn_drop_rate=0.0,\\n        drop_path_rate=0.0,\\n    )\\n    def __init__(\\n        self,\\n        model_name: str,\\n        input_shape: ShapeSpec,\\n        lr: float = 1e-03,\\n        wd: float = 1e-05,\\n        pretrained: bool = True,\\n        freeze_to: Optional[int] = None,\\n        finetune: Optional[bool] = None,\\n        act: Optional[str] = None,\\n        reset_classifier: bool = True,\\n        filter_wd: bool = True,\\n        **kwargs,\\n    ):\\n        \\\"\\\"\\\"\\n        Arguments:\\n        1. `input_shape` (ShapeSpec): input image shape. For ViT `height=width` and check the above link for avilable model shapes.\\n        2. `model_name` (str): name of the ViT model, check the above link for avilable models.\\n        3. `pretrained` (bool): load weights pretrained on imagenet.\\n        4. `act` (str): name of the activation layer. Must be registerd in `ACTIVATION_REGISTRY`\\n        5. `num_classes` (int): num output classes.\\n        6. `drop_rate` (float): dropout rate.\\n        7. `attn_drop_rate` (float): attention dropout rate.\\n        8. `drop_path_rate` (float): stochastic depth rate.\\n        9. `reset_classifier` (bool): resets the weights of the classifier.\\n        10. `freeze_to` (int): Freeze the param meter groups of the model upto n.\\n        11. `finetune` (bool): Freeze all the layers and keep only the `classifier` trainable.\\n        \\\"\\\"\\\"\\n        super(VisionTransformer, self).__init__()\\n        # create model from timm\\n        assert input_shape.height == input_shape.width\\n        in_chans = input_shape.channels\\n\\n        if act is not None:\\n            act = ACTIVATION_REGISTRY.get(act)\\n\\n        self.model = timm.create_model(\\n            model_name, pretrained, in_chans=in_chans, act=act, **kwargs\\n        )\\n\\n        if reset_classifier:\\n            num_cls = kwargs.pop(\\\"num_classes\\\")\\n            self.model.reset_classifier(num_cls)\\n\\n        if freeze_to is not None:\\n            self.freeze_to(freeze_to)\\n\\n        if finetune:\\n            if freeze_to is not None and isinstance(freeze_to, int):\\n                msg = \\\"You have sprecified freeze_to along with finetune\\\"\\n                _logger.warning(msg)\\n            _logger.info(\\\"Freezing all the model parameters except for the classifier\\\")\\n            self.freeze()\\n\\n            classifier = [\\\"head\\\", \\\"head_dist\\\"]\\n\\n            for name, module in self.model.named_children():\\n                if name in classifier:\\n                    for p in module.parameters():\\n                        p.requires_grad_(True)\\n\\n        store_attr(\\\"wd, lr, filter_wd, input_shape\\\")\\n\\n    def forward(self, batched_inputs: torch.Tensor) -> torch.Tensor:\\n        \\\"\\\"\\\"\\n        Runs the batched_inputs through the created model.\\n        \\\"\\\"\\\"\\n        out = self.model(batched_inputs)\\n        return out\\n\\n    @classmethod\\n    def from_config_dict(cls, cfg: DictConfig):\\n        \\\"\\\"\\\"\\n        Instantiate the Meta Architecture from gale config\\n        \\\"\\\"\\\"\\n        # fmt: off\\n        input_shape = ShapeSpec(cfg.input.channels, cfg.input.height, cfg.input.width)\\n        _logger.debug(f\\\"Inputs: {input_shape}\\\")\\n        instance = super().from_config_dict(cfg.model.meta_architecture.init_args, input_shape=input_shape)\\n        param_count = get_human_readable_count(sum([m.numel() for m in instance.parameters()]))\\n        _logger.debug('{} created, param count: {}.'.format(cfg.model.meta_architecture.init_args.model_name, param_count))\\n        # fmt: on\\n        return instance\\n\\n    def build_param_dicts(self):\\n        \\\"\\\"\\\"\\n        Builds up the Paramters dicts for optimization.\\n        \\\"\\\"\\\"\\n        if self.filter_wd:\\n            param_lists = add_weight_decay(\\n                self.model,\\n                weight_decay=self.wd,\\n                skip_list=self.model.no_weight_decay(),\\n            )\\n            param_lists[0][\\\"lr\\\"] = self.lr\\n            param_lists[1][\\\"lr\\\"] = self.lr\\n        else:\\n            ps = trainable_params(self.model)\\n            param_lists = dict(params=ps, lr=self.lr, wd=self.wd)\\n        return param_lists\\n\\n    @property\\n    def hypers(self) -> Tuple:\\n        \\\"\\\"\\\"\\n        Returns list of parameters like `lr` and `wd`\\n        for each param group\\n        \\\"\\\"\\\"\\n        lrs = []\\n        wds = []\\n\\n        for p in self.build_param_dicts():\\n            lrs.append(p[\\\"lr\\\"])\\n            wds.append(p[\\\"weight_decay\\\"])\\n        return self._hypers(lrs, wds)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "# @TODO: Add support for Discriminative Lr's\n",
    "class VisionTransformer(BasicModule):\n",
    "    _hypers = namedtuple(\"hypers\", field_names=[\"lr\", \"wd\"])\n",
    "    \"\"\"\n",
    "    A interface to create a Vision Transformer from timm. For available model check :\n",
    "    https://github.com/rwightman/pytorch-image-models/timm/models/vision_transformer.py\n",
    "    \"\"\"\n",
    "\n",
    "    @use_kwargs_dict(\n",
    "        keep=True,\n",
    "        num_classes=1000,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "    )\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        input_shape: ShapeSpec,\n",
    "        lr: float = 1e-03,\n",
    "        wd: float = 1e-05,\n",
    "        pretrained: bool = True,\n",
    "        freeze_to: Optional[int] = None,\n",
    "        finetune: Optional[bool] = None,\n",
    "        act: Optional[str] = None,\n",
    "        reset_classifier: bool = True,\n",
    "        filter_wd: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        1. `input_shape` (ShapeSpec): input image shape. For ViT `height=width` and check the above link for avilable model shapes.\n",
    "        2. `model_name` (str): name of the ViT model, check the above link for avilable models.\n",
    "        3. `pretrained` (bool): load weights pretrained on imagenet.\n",
    "        4. `act` (str): name of the activation layer. Must be registerd in `ACTIVATION_REGISTRY`\n",
    "        5. `num_classes` (int): num output classes.\n",
    "        6. `drop_rate` (float): dropout rate.\n",
    "        7. `attn_drop_rate` (float): attention dropout rate.\n",
    "        8. `drop_path_rate` (float): stochastic depth rate.\n",
    "        9. `reset_classifier` (bool): resets the weights of the classifier.\n",
    "        10. `freeze_to` (int): Freeze the param meter groups of the model upto n.\n",
    "        11. `finetune` (bool): Freeze all the layers and keep only the `classifier` trainable.\n",
    "        \"\"\"\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        # create model from timm\n",
    "        assert input_shape.height == input_shape.width\n",
    "        in_chans = input_shape.channels\n",
    "\n",
    "        if act is not None:\n",
    "            act = ACTIVATION_REGISTRY.get(act)\n",
    "\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained, in_chans=in_chans, act=act, **kwargs\n",
    "        )\n",
    "\n",
    "        if reset_classifier:\n",
    "            num_cls = kwargs.pop(\"num_classes\")\n",
    "            self.model.reset_classifier(num_cls)\n",
    "\n",
    "        if freeze_to is not None:\n",
    "            self.freeze_to(freeze_to)\n",
    "\n",
    "        if finetune:\n",
    "            if freeze_to is not None and isinstance(freeze_to, int):\n",
    "                msg = \"You have sprecified freeze_to along with finetune\"\n",
    "                _logger.warning(msg)\n",
    "            _logger.info(\"Freezing all the model parameters except for the classifier\")\n",
    "            self.freeze()\n",
    "\n",
    "            classifier = [\"head\", \"head_dist\"]\n",
    "\n",
    "            for name, module in self.model.named_children():\n",
    "                if name in classifier:\n",
    "                    for p in module.parameters():\n",
    "                        p.requires_grad_(True)\n",
    "\n",
    "        store_attr(\"wd, lr, filter_wd, input_shape\")\n",
    "\n",
    "    def forward(self, batched_inputs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Runs the batched_inputs through the created model.\n",
    "        \"\"\"\n",
    "        out = self.model(batched_inputs)\n",
    "        return out\n",
    "\n",
    "    @classmethod\n",
    "    def from_config_dict(cls, cfg: DictConfig):\n",
    "        \"\"\"\n",
    "        Instantiate the Meta Architecture from gale config\n",
    "        \"\"\"\n",
    "        # fmt: off\n",
    "        input_shape = ShapeSpec(cfg.input.channels, cfg.input.height, cfg.input.width)\n",
    "        _logger.debug(f\"Inputs: {input_shape}\")\n",
    "        instance = super().from_config_dict(cfg.model.meta_architecture.init_args, input_shape=input_shape)\n",
    "        param_count = get_human_readable_count(sum([m.numel() for m in instance.parameters()]))\n",
    "        _logger.debug('{} created, param count: {}.'.format(cfg.model.meta_architecture.init_args.model_name, param_count))\n",
    "        # fmt: on\n",
    "        return instance\n",
    "\n",
    "    def build_param_dicts(self):\n",
    "        \"\"\"\n",
    "        Builds up the Paramters dicts for optimization.\n",
    "        \"\"\"\n",
    "        if self.filter_wd:\n",
    "            param_lists = add_weight_decay(\n",
    "                self.model,\n",
    "                weight_decay=self.wd,\n",
    "                skip_list=self.model.no_weight_decay(),\n",
    "            )\n",
    "            param_lists[0][\"lr\"] = self.lr\n",
    "            param_lists[1][\"lr\"] = self.lr\n",
    "        else:\n",
    "            ps = trainable_params(self.model)\n",
    "            param_lists = dict(params=ps, lr=self.lr, wd=self.wd)\n",
    "        return param_lists\n",
    "\n",
    "    @property\n",
    "    def hypers(self) -> Tuple:\n",
    "        \"\"\"\n",
    "        Returns list of parameters like `lr` and `wd`\n",
    "        for each param group\n",
    "        \"\"\"\n",
    "        lrs = []\n",
    "        wds = []\n",
    "\n",
    "        for p in self.build_param_dicts():\n",
    "            lrs.append(p[\"lr\"])\n",
    "            wds.append(p[\"weight_decay\"])\n",
    "        return self._hypers(lrs, wds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"VisionTransformer\" class=\"doc_header\"><code>class</code> <code>VisionTransformer</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>VisionTransformer</code>(**`model_name`**:`str`, **`input_shape`**:`ShapeSpec`, **`lr`**:`float`=*`0.001`*, **`wd`**:`float`=*`1e-05`*, **`pretrained`**:`bool`=*`True`*, **`freeze_to`**:`Optional`\\[`int`\\]=*`None`*, **`finetune`**:`Optional`\\[`bool`\\]=*`None`*, **`act`**:`Optional`\\[`str`\\]=*`None`*, **`reset_classifier`**:`bool`=*`True`*, **`filter_wd`**:`bool`=*`True`*, **`num_classes`**=*`1000`*, **`drop_rate`**=*`0.0`*, **`attn_drop_rate`**=*`0.0`*, **`drop_path_rate`**=*`0.0`*, **\\*\\*`kwargs`**) :: [`BasicModule`](/gale/core-classes.html#BasicModule)\n",
       "\n",
       "Abstract class offering interface which should be implemented by all `Backbones`,\n",
       "`Heads` and `Meta Archs` in gale."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"show_doc(VisionTransformer)\";\n",
       "                var nbb_formatted_code = \"show_doc(VisionTransformer)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(VisionTransformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Arguments :**\n",
    "\n",
    "1. `input_shape` (ShapeSpec): input image shape. For ViT `height=width` and check the above link for avilable model shapes.\n",
    "2. `model_name` (str): name of the ViT model, check the above link for avilable models.\n",
    "3. `pretrained` (bool): load weights pretrained on imagenet.\n",
    "4. `act` (str): name of the activation layer. Must be registerd in `ACTIVATION_REGISTRY`\n",
    "5. `num_classes` (int): num output classes.\n",
    "6. `drop_rate` (float): dropout rate.\n",
    "7. `attn_drop_rate` (float): attention dropout rate.\n",
    "8. `drop_path_rate` (float): stochastic depth rate.\n",
    "9. `reset_classifier` (bool): resets the weights of the classifier.\n",
    "10. `freeze_to` (int): Freeze the param meter groups of the model upto n.\n",
    "11. `finetune` (bool): Freeze all the layers and keep only the `classifier` trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Freezing all the model parameters except for the classifier\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"inp = ShapeSpec(3, 224, 224)\\n\\nm = VisionTransformer(\\n    model_name=\\\"vit_small_patch16_224\\\",\\n    pretrained=False,\\n    input_shape=inp,\\n    finetune=True,\\n    reset_classifier=True,\\n    num_classes=10,\\n)\";\n",
       "                var nbb_formatted_code = \"inp = ShapeSpec(3, 224, 224)\\n\\nm = VisionTransformer(\\n    model_name=\\\"vit_small_patch16_224\\\",\\n    pretrained=False,\\n    input_shape=inp,\\n    finetune=True,\\n    reset_classifier=True,\\n    num_classes=10,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inp = ShapeSpec(3, 224, 224)\n",
    "\n",
    "m = VisionTransformer(\n",
    "    model_name=\"vit_small_patch16_224\",\n",
    "    pretrained=False,\n",
    "    input_shape=inp,\n",
    "    finetune=True,\n",
    "    reset_classifier=True,\n",
    "    num_classes=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"i = torch.randn(2, inp.channels, inp.height, inp.width)\\no = m(i)\";\n",
       "                var nbb_formatted_code = \"i = torch.randn(2, inp.channels, inp.height, inp.width)\\no = m(i)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = torch.randn(2, inp.channels, inp.height, inp.width)\n",
    "o = m(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to `GeneralizedImageClassifier` we can also instantiate `ViT` from a config. `ViT` does not require neither a `backbone` nor a `head` configuration. We just need the particular initialization arguments for the vit model defined in `model_name`.\n",
    "\n",
    "> Note: You input shape must match the dimensions that the Vision Transformer model supports. Unlike `GeneralizedImageClassifier`, `ViT` is dependent on the shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 21;\n",
       "                var nbb_unformatted_code = \"from dataclasses import dataclass, field\\nfrom omegaconf import MISSING, OmegaConf\";\n",
       "                var nbb_formatted_code = \"from dataclasses import dataclass, field\\nfrom omegaconf import MISSING, OmegaConf\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from omegaconf import MISSING, OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 22;\n",
       "                var nbb_unformatted_code = \"# export\\n@dataclass\\nclass VisionTransformerDataClass:\\n    model_name: str = MISSING\\n    lr: float = 1e-03\\n    wd: float = 1e-05\\n    pretrained: bool = False\\n    freeze_to: Optional[int] = None\\n    finetune: Optional[bool] = True\\n    reset_classifier: bool = True\\n    filter_wd: bool = True\\n    drop_rate: float = 0.0\\n    attn_drop_rate: float = 0.0\\n    drop_path_rate: float = 0.0\\n    num_classes: int = MISSING\";\n",
       "                var nbb_formatted_code = \"# export\\n@dataclass\\nclass VisionTransformerDataClass:\\n    model_name: str = MISSING\\n    lr: float = 1e-03\\n    wd: float = 1e-05\\n    pretrained: bool = False\\n    freeze_to: Optional[int] = None\\n    finetune: Optional[bool] = True\\n    reset_classifier: bool = True\\n    filter_wd: bool = True\\n    drop_rate: float = 0.0\\n    attn_drop_rate: float = 0.0\\n    drop_path_rate: float = 0.0\\n    num_classes: int = MISSING\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@dataclass\n",
    "class VisionTransformerDataClass:\n",
    "    model_name: str = MISSING\n",
    "    lr: float = 1e-03\n",
    "    wd: float = 1e-05\n",
    "    pretrained: bool = False\n",
    "    freeze_to: Optional[int] = None\n",
    "    finetune: Optional[bool] = True\n",
    "    reset_classifier: bool = True\n",
    "    filter_wd: bool = True\n",
    "    drop_rate: float = 0.0\n",
    "    attn_drop_rate: float = 0.0\n",
    "    drop_path_rate: float = 0.0\n",
    "    num_classes: int = MISSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here how a `VisionTransformer` can be instantiated via the config ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "  channels: 3\n",
      "  height: 224\n",
      "  width: 224\n",
      "model:\n",
      "  meta_architecture:\n",
      "    name: ViT\n",
      "    init_args:\n",
      "      model_name: vit_small_patch16_224\n",
      "      lr: 0.001\n",
      "      wd: 1.0e-05\n",
      "      pretrained: false\n",
      "      freeze_to: null\n",
      "      finetune: true\n",
      "      reset_classifier: true\n",
      "      filter_wd: true\n",
      "      drop_rate: 0.0\n",
      "      attn_drop_rate: 0.0\n",
      "      drop_path_rate: 0.0\n",
      "      num_classes: 2\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 29;\n",
       "                var nbb_unformatted_code = \"# collapse-output\\ninp = ShapeSpec(3, 224, 224)\\n\\nmeta_args = OmegaConf.structured(\\n    VisionTransformerDataClass(model_name=\\\"vit_small_patch16_224\\\", num_classes=2)\\n)\\n\\nmeta = OmegaConf.create()\\nmeta.name = \\\"ViT\\\"\\nmeta.init_args = meta_args\\n\\ni = OmegaConf.create()\\ni.channels = inp.channels\\ni.height = inp.height\\ni.width = inp.width\\n\\nC = OmegaConf.create()\\nC.input = i\\nC.model = OmegaConf.create()\\nC.model.meta_architecture = meta\\n\\nprint(OmegaConf.to_yaml(C, resolve=True))\";\n",
       "                var nbb_formatted_code = \"# collapse-output\\ninp = ShapeSpec(3, 224, 224)\\n\\nmeta_args = OmegaConf.structured(\\n    VisionTransformerDataClass(model_name=\\\"vit_small_patch16_224\\\", num_classes=2)\\n)\\n\\nmeta = OmegaConf.create()\\nmeta.name = \\\"ViT\\\"\\nmeta.init_args = meta_args\\n\\ni = OmegaConf.create()\\ni.channels = inp.channels\\ni.height = inp.height\\ni.width = inp.width\\n\\nC = OmegaConf.create()\\nC.input = i\\nC.model = OmegaConf.create()\\nC.model.meta_architecture = meta\\n\\nprint(OmegaConf.to_yaml(C, resolve=True))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# collapse-output\n",
    "inp = ShapeSpec(3, 224, 224)\n",
    "\n",
    "meta_args = OmegaConf.structured(\n",
    "    VisionTransformerDataClass(model_name=\"vit_small_patch16_224\", num_classes=2)\n",
    ")\n",
    "\n",
    "meta = OmegaConf.create()\n",
    "meta.name = \"ViT\"\n",
    "meta.init_args = meta_args\n",
    "\n",
    "i = OmegaConf.create()\n",
    "i.channels = inp.channels\n",
    "i.height = inp.height\n",
    "i.width = inp.width\n",
    "\n",
    "C = OmegaConf.create()\n",
    "C.input = i\n",
    "C.model = OmegaConf.create()\n",
    "C.model.meta_architecture = meta\n",
    "\n",
    "print(OmegaConf.to_yaml(C, resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Freezing all the model parameters except for the classifier\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0844,  0.1789],\n",
       "        [-0.0287, -0.1308]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 30;\n",
       "                var nbb_unformatted_code = \"m = VisionTransformer.from_config_dict(C)\\nshape = (m.input_shape.channels, m.input_shape.height, m.input_shape.width)\\ninp = torch.randn(2, *shape)\\no = m(inp)\\no\";\n",
       "                var nbb_formatted_code = \"m = VisionTransformer.from_config_dict(C)\\nshape = (m.input_shape.channels, m.input_shape.height, m.input_shape.width)\\ninp = torch.randn(2, *shape)\\no = m(inp)\\no\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = VisionTransformer.from_config_dict(C)\n",
    "shape = (m.input_shape.channels, m.input_shape.height, m.input_shape.width)\n",
    "inp = torch.randn(2, *shape)\n",
    "o = m(inp)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/ayushman/Desktop/gale/nbs/data/hymenoptera_data.zip\n",
      "Extracting /Users/ayushman/Desktop/gale/nbs/data/hymenoptera_data.zip to /Users/ayushman/Desktop/gale/nbs/data\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 31;\n",
       "                var nbb_unformatted_code = \"# hide\\n# cuda\\nimport pytorch_lightning as pl\\nimport torchmetrics\\nimport torchvision.transforms as T\\nfrom fastcore.all import Path\\nfrom nbdev.export import Config\\nfrom torch import optim\\nfrom torch.utils.data import DataLoader\\nfrom torchvision.datasets import ImageFolder\\n\\nfrom gale.collections.callbacks.notebook import NotebookProgressCallback\\nfrom gale.collections.download import download_and_extract_archive\\nfrom gale.schedules import WarmupStepLR\\nfrom gale.utils.display import show_images\\n\\nURL = \\\"https://download.pytorch.org/tutorial/hymenoptera_data.zip\\\"\\ndata_path = Path(Config().path(\\\"nbs_path\\\")) / \\\"data\\\"\\n\\n# download a toy dataset\\ndownload_and_extract_archive(url=URL, download_root=data_path)\";\n",
       "                var nbb_formatted_code = \"# hide\\n# cuda\\nimport pytorch_lightning as pl\\nimport torchmetrics\\nimport torchvision.transforms as T\\nfrom fastcore.all import Path\\nfrom nbdev.export import Config\\nfrom torch import optim\\nfrom torch.utils.data import DataLoader\\nfrom torchvision.datasets import ImageFolder\\n\\nfrom gale.collections.callbacks.notebook import NotebookProgressCallback\\nfrom gale.collections.download import download_and_extract_archive\\nfrom gale.schedules import WarmupStepLR\\nfrom gale.utils.display import show_images\\n\\nURL = \\\"https://download.pytorch.org/tutorial/hymenoptera_data.zip\\\"\\ndata_path = Path(Config().path(\\\"nbs_path\\\")) / \\\"data\\\"\\n\\n# download a toy dataset\\ndownload_and_extract_archive(url=URL, download_root=data_path)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "# cuda\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "import torchvision.transforms as T\n",
    "from fastcore.all import Path\n",
    "from nbdev.export import Config\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from gale.collections.callbacks.notebook import NotebookProgressCallback\n",
    "from gale.collections.download import download_and_extract_archive\n",
    "from gale.schedules import WarmupStepLR\n",
    "from gale.utils.display import show_images\n",
    "\n",
    "URL = \"https://download.pytorch.org/tutorial/hymenoptera_data.zip\"\n",
    "data_path = Path(Config().path(\"nbs_path\")) / \"data\"\n",
    "\n",
    "# download a toy dataset\n",
    "download_and_extract_archive(url=URL, download_root=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 32;\n",
       "                var nbb_unformatted_code = \"# hide\\n# cuda\\ndata_transforms = {\\n    \\\"train\\\": T.Compose(\\n        [\\n            T.RandomResizedCrop(224),\\n            T.RandomHorizontalFlip(),\\n            T.ToTensor(),\\n            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\\n        ]\\n    ),\\n    \\\"val\\\": T.Compose(\\n        [\\n            T.Resize(256),\\n            T.CenterCrop(224),\\n            T.ToTensor(),\\n            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\\n        ]\\n    ),\\n}\\n\\ntraining_data = ImageFolder(\\n    data_path / \\\"hymenoptera_data/train\\\", transform=data_transforms[\\\"train\\\"]\\n)\\nvalidation_data = ImageFolder(\\n    data_path / \\\"hymenoptera_data/val\\\", transform=data_transforms[\\\"val\\\"]\\n)\\n\\ntrain_dl = DataLoader(training_data, batch_size=32, shuffle=True)\\nvalid_dl = DataLoader(validation_data, batch_size=32, shuffle=False)\";\n",
       "                var nbb_formatted_code = \"# hide\\n# cuda\\ndata_transforms = {\\n    \\\"train\\\": T.Compose(\\n        [\\n            T.RandomResizedCrop(224),\\n            T.RandomHorizontalFlip(),\\n            T.ToTensor(),\\n            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\\n        ]\\n    ),\\n    \\\"val\\\": T.Compose(\\n        [\\n            T.Resize(256),\\n            T.CenterCrop(224),\\n            T.ToTensor(),\\n            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\\n        ]\\n    ),\\n}\\n\\ntraining_data = ImageFolder(\\n    data_path / \\\"hymenoptera_data/train\\\", transform=data_transforms[\\\"train\\\"]\\n)\\nvalidation_data = ImageFolder(\\n    data_path / \\\"hymenoptera_data/val\\\", transform=data_transforms[\\\"val\\\"]\\n)\\n\\ntrain_dl = DataLoader(training_data, batch_size=32, shuffle=True)\\nvalid_dl = DataLoader(validation_data, batch_size=32, shuffle=False)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "# cuda\n",
    "data_transforms = {\n",
    "    \"train\": T.Compose(\n",
    "        [\n",
    "            T.RandomResizedCrop(224),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "    \"val\": T.Compose(\n",
    "        [\n",
    "            T.Resize(256),\n",
    "            T.CenterCrop(224),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "training_data = ImageFolder(\n",
    "    data_path / \"hymenoptera_data/train\", transform=data_transforms[\"train\"]\n",
    ")\n",
    "validation_data = ImageFolder(\n",
    "    data_path / \"hymenoptera_data/val\", transform=data_transforms[\"val\"]\n",
    ")\n",
    "\n",
    "train_dl = DataLoader(training_data, batch_size=32, shuffle=True)\n",
    "valid_dl = DataLoader(validation_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 33;\n",
       "                var nbb_unformatted_code = \"# hide\\n# cuda\\nclass Learner(pl.LightningModule):\\n    def __init__(self, model):\\n        super().__init__()\\n        self.model = model\\n        self.train_metric = torchmetrics.Accuracy()\\n        self.valid_metric = torchmetrics.Accuracy()\\n        self.loss_fn = torch.nn.CrossEntropyLoss()\\n\\n    def forward(self, xb):\\n        return self.model(xb)\\n\\n    def training_step(self, batch: Any, batch_idx: int):\\n        x, y = batch\\n        y_hat = self(x)\\n        loss = self.loss_fn(y_hat, y)\\n        acc = self.train_metric(torch.nn.functional.softmax(y_hat), y)\\n        self.log_dict(dict(loss=loss, acc=acc))\\n        return loss\\n\\n    def validation_step(self, batch: Any, batch_idx: int):\\n        x, y = batch\\n        y_hat = self(x)\\n        loss = self.loss_fn(y_hat, y)\\n        acc = self.valid_metric(torch.nn.functional.softmax(y_hat), y)\\n        self.log_dict(dict(val_loss=loss, val_acc=acc))\\n\\n    def configure_optimizers(self):\\n        paramters = self.model.build_param_dicts()\\n        opt = optim.AdamW(paramters)\\n        sch = WarmupStepLR(\\n            opt,\\n            num_decays=2,\\n            warmup_epochs=1,\\n            decay_rate=0.1,\\n            epochs=self.trainer.max_epochs,\\n        )\\n        return [opt], [sch]\";\n",
       "                var nbb_formatted_code = \"# hide\\n# cuda\\nclass Learner(pl.LightningModule):\\n    def __init__(self, model):\\n        super().__init__()\\n        self.model = model\\n        self.train_metric = torchmetrics.Accuracy()\\n        self.valid_metric = torchmetrics.Accuracy()\\n        self.loss_fn = torch.nn.CrossEntropyLoss()\\n\\n    def forward(self, xb):\\n        return self.model(xb)\\n\\n    def training_step(self, batch: Any, batch_idx: int):\\n        x, y = batch\\n        y_hat = self(x)\\n        loss = self.loss_fn(y_hat, y)\\n        acc = self.train_metric(torch.nn.functional.softmax(y_hat), y)\\n        self.log_dict(dict(loss=loss, acc=acc))\\n        return loss\\n\\n    def validation_step(self, batch: Any, batch_idx: int):\\n        x, y = batch\\n        y_hat = self(x)\\n        loss = self.loss_fn(y_hat, y)\\n        acc = self.valid_metric(torch.nn.functional.softmax(y_hat), y)\\n        self.log_dict(dict(val_loss=loss, val_acc=acc))\\n\\n    def configure_optimizers(self):\\n        paramters = self.model.build_param_dicts()\\n        opt = optim.AdamW(paramters)\\n        sch = WarmupStepLR(\\n            opt,\\n            num_decays=2,\\n            warmup_epochs=1,\\n            decay_rate=0.1,\\n            epochs=self.trainer.max_epochs,\\n        )\\n        return [opt], [sch]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "# cuda\n",
    "class Learner(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.train_metric = torchmetrics.Accuracy()\n",
    "        self.valid_metric = torchmetrics.Accuracy()\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.model(xb)\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        acc = self.train_metric(torch.nn.functional.softmax(y_hat), y)\n",
    "        self.log_dict(dict(loss=loss, acc=acc))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        acc = self.valid_metric(torch.nn.functional.softmax(y_hat), y)\n",
    "        self.log_dict(dict(val_loss=loss, val_acc=acc))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        paramters = self.model.build_param_dicts()\n",
    "        opt = optim.AdamW(paramters)\n",
    "        sch = WarmupStepLR(\n",
    "            opt,\n",
    "            num_decays=2,\n",
    "            warmup_epochs=1,\n",
    "            decay_rate=0.1,\n",
    "            epochs=self.trainer.max_epochs,\n",
    "        )\n",
    "        return [opt], [sch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Freezing all the model parameters except for the classifier\n",
      "\n",
      "  | Name         | Type              | Params\n",
      "---------------------------------------------------\n",
      "0 | model        | VisionTransformer | 48.0 M\n",
      "1 | train_metric | Accuracy          | 0     \n",
      "2 | valid_metric | Accuracy          | 0     \n",
      "3 | loss_fn      | CrossEntropyLoss  | 0     \n",
      "---------------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "48.0 M    Non-trainable params\n",
      "48.0 M    Total params\n",
      "191.948   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            progress {\n",
       "                border: none;\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      Training\n",
       "      <progress value='25' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/56 04:56 < 06:23, 0.08 it/s, Epoch 3 {'loss': '0.818', 'v_num': 3}]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>time</th>\n",
       "      <th>samples/s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.902881</td>\n",
       "      <td>0.483660</td>\n",
       "      <td>0.931735</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>94.035300</td>\n",
       "      <td>0.138200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.863190</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.902018</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>94.278800</td>\n",
       "      <td>0.137900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.778777</td>\n",
       "      <td>0.522876</td>\n",
       "      <td>0.761685</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>99.144800</td>\n",
       "      <td>0.131100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 35;\n",
       "                var nbb_unformatted_code = \"# hide\\n# cuda\\ncbs = [\\n    NotebookProgressCallback(),\\n    pl.callbacks.LearningRateMonitor(logging_interval=\\\"epoch\\\", log_momentum=True),\\n]\\n\\nlogger = pl.loggers.TensorBoardLogger(save_dir=\\\"lightning_logs/\\\", name=\\\"my_model\\\", default_hp_metric=False)\\n\\ntrainer = pl.Trainer(max_epochs=7, callbacks=cbs, log_every_n_steps=1, logger=logger)\\n\\nmodel = VisionTransformer.from_config_dict(C)\\nlearn = Learner(model)\\n\\ntrainer.fit(learn, train_dataloader=train_dl, val_dataloaders=valid_dl)\";\n",
       "                var nbb_formatted_code = \"# hide\\n# cuda\\ncbs = [\\n    NotebookProgressCallback(),\\n    pl.callbacks.LearningRateMonitor(logging_interval=\\\"epoch\\\", log_momentum=True),\\n]\\n\\nlogger = pl.loggers.TensorBoardLogger(\\n    save_dir=\\\"lightning_logs/\\\", name=\\\"my_model\\\", default_hp_metric=False\\n)\\n\\ntrainer = pl.Trainer(max_epochs=7, callbacks=cbs, log_every_n_steps=1, logger=logger)\\n\\nmodel = VisionTransformer.from_config_dict(C)\\nlearn = Learner(model)\\n\\ntrainer.fit(learn, train_dataloader=train_dl, val_dataloaders=valid_dl)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "# cuda\n",
    "cbs = [\n",
    "    NotebookProgressCallback(),\n",
    "    pl.callbacks.LearningRateMonitor(logging_interval=\"epoch\", log_momentum=True),\n",
    "]\n",
    "\n",
    "logger = pl.loggers.TensorBoardLogger(\n",
    "    save_dir=\"lightning_logs/\", name=\"my_model\", default_hp_metric=False\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=7, callbacks=cbs, log_every_n_steps=1, logger=logger)\n",
    "\n",
    "model = VisionTransformer.from_config_dict(C)\n",
    "learn = Learner(model)\n",
    "\n",
    "trainer.fit(learn, train_dataloader=train_dl, val_dataloaders=valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 04b_classification.model.meta_arch.vit.ipynb.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 36;\n",
       "                var nbb_unformatted_code = \"# hide\\nnotebook2script('04b_classification.model.meta_arch.vit.ipynb')\";\n",
       "                var nbb_formatted_code = \"# hide\\nnotebook2script(\\\"04b_classification.model.meta_arch.vit.ipynb\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "notebook2script(\"04b_classification.model.meta_arch.vit.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gale_dev",
   "language": "python",
   "name": "gale_dev"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
