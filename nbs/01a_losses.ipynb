{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee65e183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# skip\n",
    "!git clone https://github.com/benihime91/gale # install gale on colab\n",
    "!pip install -e \"gale[dev]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99a2d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc86fd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# hide\\n%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\\n%matplotlib inline\";\n",
       "                var nbb_formatted_code = \"# hide\\n%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\\n%matplotlib inline\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "%load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57345d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# hide\\nimport warnings\\n\\nfrom nbdev.export import *\\nfrom nbdev.showdoc import *\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\";\n",
       "                var nbb_formatted_code = \"# hide\\nimport warnings\\n\\nfrom nbdev.export import *\\nfrom nbdev.showdoc import *\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "import warnings\n",
    "\n",
    "from nbdev.export import *\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ab5f0c",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "> Custom loss functions in `Gale`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c11644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# export\\nimport logging\\nfrom typing import *\\n\\nimport torch\\nimport torch.nn.functional as F\\nimport torch.nn.modules.loss as torch_losses\\nfrom fastcore.all import store_attr\\nfrom fvcore.nn import sigmoid_focal_loss\\nfrom omegaconf import DictConfig, OmegaConf\\nfrom timm.loss import SoftTargetCrossEntropy\\nfrom torch import Tensor, nn\\n\\nfrom gale.torch_utils import maybe_convert_to_onehot\\nfrom gale.utils.structures import LOSS_REGISTRY\\n\\n_logger = logging.getLogger(__name__)\";\n",
       "                var nbb_formatted_code = \"# export\\nimport logging\\nfrom typing import *\\n\\nimport torch\\nimport torch.nn.functional as F\\nimport torch.nn.modules.loss as torch_losses\\nfrom fastcore.all import store_attr\\nfrom fvcore.nn import sigmoid_focal_loss\\nfrom omegaconf import DictConfig, OmegaConf\\nfrom timm.loss import SoftTargetCrossEntropy\\nfrom torch import Tensor, nn\\n\\nfrom gale.torch_utils import maybe_convert_to_onehot\\nfrom gale.utils.structures import LOSS_REGISTRY\\n\\n_logger = logging.getLogger(__name__)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "import logging\n",
    "from typing import *\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.modules.loss as torch_losses\n",
    "from fastcore.all import store_attr\n",
    "from fvcore.nn import sigmoid_focal_loss\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from timm.loss import SoftTargetCrossEntropy\n",
    "from torch import Tensor, nn\n",
    "\n",
    "from gale.torch_utils import maybe_convert_to_onehot\n",
    "from gale.utils.structures import LOSS_REGISTRY\n",
    "\n",
    "_logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8a697c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"from fastcore.all import *\";\n",
       "                var nbb_formatted_code = \"from fastcore.all import *\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastcore.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0698e037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# export\\n_all_ = [\\\"SoftTargetCrossEntropy\\\", \\\"LOSS_REGISTRY\\\"]\";\n",
       "                var nbb_formatted_code = \"# export\\n_all_ = [\\\"SoftTargetCrossEntropy\\\", \\\"LOSS_REGISTRY\\\"]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "_all_ = [\"SoftTargetCrossEntropy\", \"LOSS_REGISTRY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42badad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"# export\\nLOSS_REGISTRY.register(SoftTargetCrossEntropy)\";\n",
       "                var nbb_formatted_code = \"# export\\nLOSS_REGISTRY.register(SoftTargetCrossEntropy)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "LOSS_REGISTRY.register(SoftTargetCrossEntropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679b993f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"# export\\n@LOSS_REGISTRY.register()\\nclass LabelSmoothingCrossEntropy(nn.Module):\\n    \\\"Cross Entropy Loss with Label Smoothing\\\"\\n\\n    def __init__(\\n        self,\\n        eps: float = 0.1,\\n        reduction: str = \\\"mean\\\",\\n        weight: Optional[Tensor] = None,\\n    ):\\n        super(LabelSmoothingCrossEntropy, self).__init__()\\n        store_attr(\\\"eps, reduction, weight\\\")\\n\\n    def forward(self, input: Tensor, target: Tensor):\\n        \\\"\\\"\\\"\\n        Shape:\\n        - Input  : $(N,C)$ where $N$ is the mini-batch size and $C$ is the total number of classes\\n        - Target : $(N)$ where each value is $0 \\\\leq {targets}[i] \\\\leq C-10\\u2264targets[i]\\u2264C\\u22121$\\n        - Output: scalar. If `reduction` is `none`, then $(N, *)$ , same shape as input.\\n        \\\"\\\"\\\"\\n        c = input.size()[1]\\n        log_preds = F.log_softmax(input, dim=1)\\n        if self.reduction == \\\"sum\\\":\\n            loss = -log_preds.sum()\\n        else:\\n            loss = -log_preds.sum(dim=1)\\n            if self.reduction == \\\"mean\\\":\\n                loss = loss.mean()\\n        loss = loss * self.eps / c + (1 - self.eps) * F.nll_loss(\\n            log_preds, target.long(), weight=self.weight, reduction=self.reduction\\n        )\\n        return loss\";\n",
       "                var nbb_formatted_code = \"# export\\n@LOSS_REGISTRY.register()\\nclass LabelSmoothingCrossEntropy(nn.Module):\\n    \\\"Cross Entropy Loss with Label Smoothing\\\"\\n\\n    def __init__(\\n        self,\\n        eps: float = 0.1,\\n        reduction: str = \\\"mean\\\",\\n        weight: Optional[Tensor] = None,\\n    ):\\n        super(LabelSmoothingCrossEntropy, self).__init__()\\n        store_attr(\\\"eps, reduction, weight\\\")\\n\\n    def forward(self, input: Tensor, target: Tensor):\\n        \\\"\\\"\\\"\\n        Shape:\\n        - Input  : $(N,C)$ where $N$ is the mini-batch size and $C$ is the total number of classes\\n        - Target : $(N)$ where each value is $0 \\\\leq {targets}[i] \\\\leq C-10\\u2264targets[i]\\u2264C\\u22121$\\n        - Output: scalar. If `reduction` is `none`, then $(N, *)$ , same shape as input.\\n        \\\"\\\"\\\"\\n        c = input.size()[1]\\n        log_preds = F.log_softmax(input, dim=1)\\n        if self.reduction == \\\"sum\\\":\\n            loss = -log_preds.sum()\\n        else:\\n            loss = -log_preds.sum(dim=1)\\n            if self.reduction == \\\"mean\\\":\\n                loss = loss.mean()\\n        loss = loss * self.eps / c + (1 - self.eps) * F.nll_loss(\\n            log_preds, target.long(), weight=self.weight, reduction=self.reduction\\n        )\\n        return loss\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@LOSS_REGISTRY.register()\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"Cross Entropy Loss with Label Smoothing\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eps: float = 0.1,\n",
    "        reduction: str = \"mean\",\n",
    "        weight: Optional[Tensor] = None,\n",
    "    ):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        store_attr(\"eps, reduction, weight\")\n",
    "\n",
    "    def forward(self, input: Tensor, target: Tensor):\n",
    "        \"\"\"\n",
    "        Shape:\n",
    "        - Input  : $(N,C)$ where $N$ is the mini-batch size and $C$ is the total number of classes\n",
    "        - Target : $(N)$ where each value is $0 \\leq {targets}[i] \\leq C-10≤targets[i]≤C−1$\n",
    "        - Output: scalar. If `reduction` is `none`, then $(N, *)$ , same shape as input.\n",
    "        \"\"\"\n",
    "        c = input.size()[1]\n",
    "        log_preds = F.log_softmax(input, dim=1)\n",
    "        if self.reduction == \"sum\":\n",
    "            loss = -log_preds.sum()\n",
    "        else:\n",
    "            loss = -log_preds.sum(dim=1)\n",
    "            if self.reduction == \"mean\":\n",
    "                loss = loss.mean()\n",
    "        loss = loss * self.eps / c + (1 - self.eps) * F.nll_loss(\n",
    "            log_preds, target.long(), weight=self.weight, reduction=self.reduction\n",
    "        )\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6acdbd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"criterion = LabelSmoothingCrossEntropy(reduction=\\\"mean\\\")\\n\\noutput = torch.randn(32, 5, requires_grad=True)\\ntarget = torch.empty(32, dtype=torch.long).random_(5)\\n\\nloss = criterion(output, target)\";\n",
       "                var nbb_formatted_code = \"criterion = LabelSmoothingCrossEntropy(reduction=\\\"mean\\\")\\n\\noutput = torch.randn(32, 5, requires_grad=True)\\ntarget = torch.empty(32, dtype=torch.long).random_(5)\\n\\nloss = criterion(output, target)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "criterion = LabelSmoothingCrossEntropy(reduction=\"mean\")\n",
    "\n",
    "output = torch.randn(32, 5, requires_grad=True)\n",
    "target = torch.empty(32, dtype=torch.long).random_(5)\n",
    "\n",
    "loss = criterion(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea2df89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"# export\\n@LOSS_REGISTRY.register()\\nclass BinarySigmoidFocalLoss(nn.Module):\\n    \\\"\\\"\\\"\\n    Creates a criterion that computes the focal loss between binary `input` and `target`.\\n    Focal Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\\n\\n    Source: https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/focal_loss.py\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        alpha: float = -1,\\n        gamma: float = 2,\\n        reduction: str = \\\"mean\\\",\\n    ):\\n        super(BinarySigmoidFocalLoss, self).__init__()\\n        store_attr(\\\"alpha, gamma, reduction\\\")\\n\\n    def forward(self, input: Tensor, target: Tensor):\\n        \\\"\\\"\\\"\\n        Shape:\\n        - Input: : $(N, *)$ where $*$ means, any number of additional dimensions.\\n        - Target: : $(N, *)$, same shape as the input.\\n        - Output: scalar. If `reduction` is 'none', then $(N, *)$ , same shape as input.\\n        \\\"\\\"\\\"\\n        loss = sigmoid_focal_loss(input, target, self.gamma, self.alpha, self.reduction)\\n        return loss\";\n",
       "                var nbb_formatted_code = \"# export\\n@LOSS_REGISTRY.register()\\nclass BinarySigmoidFocalLoss(nn.Module):\\n    \\\"\\\"\\\"\\n    Creates a criterion that computes the focal loss between binary `input` and `target`.\\n    Focal Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\\n\\n    Source: https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/focal_loss.py\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        alpha: float = -1,\\n        gamma: float = 2,\\n        reduction: str = \\\"mean\\\",\\n    ):\\n        super(BinarySigmoidFocalLoss, self).__init__()\\n        store_attr(\\\"alpha, gamma, reduction\\\")\\n\\n    def forward(self, input: Tensor, target: Tensor):\\n        \\\"\\\"\\\"\\n        Shape:\\n        - Input: : $(N, *)$ where $*$ means, any number of additional dimensions.\\n        - Target: : $(N, *)$, same shape as the input.\\n        - Output: scalar. If `reduction` is 'none', then $(N, *)$ , same shape as input.\\n        \\\"\\\"\\\"\\n        loss = sigmoid_focal_loss(input, target, self.gamma, self.alpha, self.reduction)\\n        return loss\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@LOSS_REGISTRY.register()\n",
    "class BinarySigmoidFocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates a criterion that computes the focal loss between binary `input` and `target`.\n",
    "    Focal Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n",
    "\n",
    "    Source: https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/focal_loss.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha: float = -1,\n",
    "        gamma: float = 2,\n",
    "        reduction: str = \"mean\",\n",
    "    ):\n",
    "        super(BinarySigmoidFocalLoss, self).__init__()\n",
    "        store_attr(\"alpha, gamma, reduction\")\n",
    "\n",
    "    def forward(self, input: Tensor, target: Tensor):\n",
    "        \"\"\"\n",
    "        Shape:\n",
    "        - Input: : $(N, *)$ where $*$ means, any number of additional dimensions.\n",
    "        - Target: : $(N, *)$, same shape as the input.\n",
    "        - Output: scalar. If `reduction` is 'none', then $(N, *)$ , same shape as input.\n",
    "        \"\"\"\n",
    "        loss = sigmoid_focal_loss(input, target, self.gamma, self.alpha, self.reduction)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abdbf29",
   "metadata": {},
   "source": [
    "[Focal Loss](https://arxiv.org/pdf/1708.02002.pdf) is the same as cross entropy except easy-to-classify observations are down-weighted in the loss calculation. The strength of down-weighting is proportional to the size of the gamma parameter. Put another way, the larger gamma the less the easy-to-classify observations contribute to the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15542318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"criterion = BinarySigmoidFocalLoss(reduction=\\\"mean\\\")\\n\\ntarget = torch.ones([10, 64], dtype=torch.float32)\\noutput = torch.full([10, 64], 1.5)\\n\\nloss = criterion(output, target)\";\n",
       "                var nbb_formatted_code = \"criterion = BinarySigmoidFocalLoss(reduction=\\\"mean\\\")\\n\\ntarget = torch.ones([10, 64], dtype=torch.float32)\\noutput = torch.full([10, 64], 1.5)\\n\\nloss = criterion(output, target)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "criterion = BinarySigmoidFocalLoss(reduction=\"mean\")\n",
    "\n",
    "target = torch.ones([10, 64], dtype=torch.float32)\n",
    "output = torch.full([10, 64], 1.5)\n",
    "\n",
    "loss = criterion(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58f0520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"# export\\n@LOSS_REGISTRY.register()\\nclass FocalLoss(nn.Module):\\n    \\\"\\\"\\\"\\n    Same as `nn.CrossEntropyLoss` but with focal paramter, `gamma`.\\n    Focal Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\\n    Focal loss is computed as follows :\\n    ${FL}(p_t)$ = $\\\\alpha(1 - p_t)^{\\\\gamma}{log}(p_t)$\\n\\n    Source: https://kornia.readthedocs.io/en/latest/_modules/kornia/losses/focal.html\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        alpha: float = 1,\\n        gamma: float = 2,\\n        reduction: str = \\\"mean\\\",\\n        eps: float = 1e-8,\\n    ):\\n\\n        super(FocalLoss, self).__init__()\\n        store_attr(\\\"alpha, gamma, reduction, eps\\\")\\n\\n    def forward(self, input: Tensor, target: Tensor):\\n        \\\"\\\"\\\"\\n        Shape:\\n        - Input  : $(N,C)$ where $N$ is the mini-batch size and $C$ is the total number of classes\\n        - Target : $(N)$ where each value is $0 \\\\leq {targets}[i] \\\\leq C-10\\u2264targets[i]\\u2264C\\u22121$\\n        \\\"\\\"\\\"\\n        if not len(input.shape) >= 2:\\n            raise ValueError(\\n                \\\"Invalid input shape, we expect BxCx*. Got: {}\\\".format(input.shape)\\n            )\\n\\n        if input.size(0) != target.size(0):\\n            raise ValueError(\\n                \\\"Expected input batch_size ({}) to match target batch_size ({}).\\\".format(\\n                    input.size(0), target.size(0)\\n                )\\n            )\\n\\n        n = input.size(0)\\n\\n        # compute softmax over the classes axis\\n        softmax_inputs: Tensor = F.softmax(input, dim=1) + self.eps\\n\\n        # create the labels one hot tensor\\n        one_hot_targs: Tensor = maybe_convert_to_onehot(target, softmax_inputs)\\n\\n        # compute the actual focal loss\\n        focal_weight = torch.pow(-softmax_inputs + 1.0, self.gamma)\\n\\n        focal_factor = -self.alpha * focal_weight * torch.log(softmax_inputs)\\n\\n        loss = torch.sum(one_hot_targs * focal_factor, dim=1)\\n\\n        if self.reduction == \\\"none\\\":\\n            loss = loss\\n        elif self.reduction == \\\"mean\\\":\\n            loss = torch.mean(loss)\\n        elif self.reduction == \\\"sum\\\":\\n            loss = torch.sum(loss)\\n        else:\\n            raise NotImplementedError(\\n                \\\"Invalid reduction mode: {}\\\".format(self.reduction)\\n            )\\n        return loss\";\n",
       "                var nbb_formatted_code = \"# export\\n@LOSS_REGISTRY.register()\\nclass FocalLoss(nn.Module):\\n    \\\"\\\"\\\"\\n    Same as `nn.CrossEntropyLoss` but with focal paramter, `gamma`.\\n    Focal Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\\n    Focal loss is computed as follows :\\n    ${FL}(p_t)$ = $\\\\alpha(1 - p_t)^{\\\\gamma}{log}(p_t)$\\n\\n    Source: https://kornia.readthedocs.io/en/latest/_modules/kornia/losses/focal.html\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        alpha: float = 1,\\n        gamma: float = 2,\\n        reduction: str = \\\"mean\\\",\\n        eps: float = 1e-8,\\n    ):\\n\\n        super(FocalLoss, self).__init__()\\n        store_attr(\\\"alpha, gamma, reduction, eps\\\")\\n\\n    def forward(self, input: Tensor, target: Tensor):\\n        \\\"\\\"\\\"\\n        Shape:\\n        - Input  : $(N,C)$ where $N$ is the mini-batch size and $C$ is the total number of classes\\n        - Target : $(N)$ where each value is $0 \\\\leq {targets}[i] \\\\leq C-10\\u2264targets[i]\\u2264C\\u22121$\\n        \\\"\\\"\\\"\\n        if not len(input.shape) >= 2:\\n            raise ValueError(\\n                \\\"Invalid input shape, we expect BxCx*. Got: {}\\\".format(input.shape)\\n            )\\n\\n        if input.size(0) != target.size(0):\\n            raise ValueError(\\n                \\\"Expected input batch_size ({}) to match target batch_size ({}).\\\".format(\\n                    input.size(0), target.size(0)\\n                )\\n            )\\n\\n        n = input.size(0)\\n\\n        # compute softmax over the classes axis\\n        softmax_inputs: Tensor = F.softmax(input, dim=1) + self.eps\\n\\n        # create the labels one hot tensor\\n        one_hot_targs: Tensor = maybe_convert_to_onehot(target, softmax_inputs)\\n\\n        # compute the actual focal loss\\n        focal_weight = torch.pow(-softmax_inputs + 1.0, self.gamma)\\n\\n        focal_factor = -self.alpha * focal_weight * torch.log(softmax_inputs)\\n\\n        loss = torch.sum(one_hot_targs * focal_factor, dim=1)\\n\\n        if self.reduction == \\\"none\\\":\\n            loss = loss\\n        elif self.reduction == \\\"mean\\\":\\n            loss = torch.mean(loss)\\n        elif self.reduction == \\\"sum\\\":\\n            loss = torch.sum(loss)\\n        else:\\n            raise NotImplementedError(\\n                \\\"Invalid reduction mode: {}\\\".format(self.reduction)\\n            )\\n        return loss\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@LOSS_REGISTRY.register()\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Same as `nn.CrossEntropyLoss` but with focal paramter, `gamma`.\n",
    "    Focal Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n",
    "    Focal loss is computed as follows :\n",
    "    ${FL}(p_t)$ = $\\alpha(1 - p_t)^{\\gamma}{log}(p_t)$\n",
    "\n",
    "    Source: https://kornia.readthedocs.io/en/latest/_modules/kornia/losses/focal.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha: float = 1,\n",
    "        gamma: float = 2,\n",
    "        reduction: str = \"mean\",\n",
    "        eps: float = 1e-8,\n",
    "    ):\n",
    "\n",
    "        super(FocalLoss, self).__init__()\n",
    "        store_attr(\"alpha, gamma, reduction, eps\")\n",
    "\n",
    "    def forward(self, input: Tensor, target: Tensor):\n",
    "        \"\"\"\n",
    "        Shape:\n",
    "        - Input  : $(N,C)$ where $N$ is the mini-batch size and $C$ is the total number of classes\n",
    "        - Target : $(N)$ where each value is $0 \\leq {targets}[i] \\leq C-10≤targets[i]≤C−1$\n",
    "        \"\"\"\n",
    "        if not len(input.shape) >= 2:\n",
    "            raise ValueError(\n",
    "                \"Invalid input shape, we expect BxCx*. Got: {}\".format(input.shape)\n",
    "            )\n",
    "\n",
    "        if input.size(0) != target.size(0):\n",
    "            raise ValueError(\n",
    "                \"Expected input batch_size ({}) to match target batch_size ({}).\".format(\n",
    "                    input.size(0), target.size(0)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        n = input.size(0)\n",
    "\n",
    "        # compute softmax over the classes axis\n",
    "        softmax_inputs: Tensor = F.softmax(input, dim=1) + self.eps\n",
    "\n",
    "        # create the labels one hot tensor\n",
    "        one_hot_targs: Tensor = maybe_convert_to_onehot(target, softmax_inputs)\n",
    "\n",
    "        # compute the actual focal loss\n",
    "        focal_weight = torch.pow(-softmax_inputs + 1.0, self.gamma)\n",
    "\n",
    "        focal_factor = -self.alpha * focal_weight * torch.log(softmax_inputs)\n",
    "\n",
    "        loss = torch.sum(one_hot_targs * focal_factor, dim=1)\n",
    "\n",
    "        if self.reduction == \"none\":\n",
    "            loss = loss\n",
    "        elif self.reduction == \"mean\":\n",
    "            loss = torch.mean(loss)\n",
    "        elif self.reduction == \"sum\":\n",
    "            loss = torch.sum(loss)\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"Invalid reduction mode: {}\".format(self.reduction)\n",
    "            )\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228a4620",
   "metadata": {},
   "source": [
    "Arguments to `FocalLoss`:\n",
    "- `alpha` (float): Weighting factor $\\alpha$ in `[0, 1]`.\n",
    "- `gamma` (float, optional): Focusing parameter $\\gamma$ >= 0. Default 2.\n",
    "- `reduction` (str, optional): Specifies the reduction to apply to the\n",
    "- `output`: `none` | `mean` | `sum`. \n",
    "  * `none`: no reduction will be applied,\n",
    "  * `mean`: the sum of the output will be divided by the number of elements in the output \n",
    "  * `sum`: the output will be summed. \n",
    "  * Default: `none`.\n",
    "- `eps` (float, optional): Scalar to enforce numerical stabiliy. Default: 1e-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775921ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"criterion = FocalLoss(alpha=0.5, gamma=2.0, reduction=\\\"mean\\\")\\n\\nN = 5  # num_classes\\ninput = torch.randn(32, N, requires_grad=True)\\ntarget = torch.empty(32, dtype=torch.long).random_(N)\\nloss = criterion(input, target)\\n\\n\\n# Compare focal loss with gamma = 0 ,cross entropy\\nfl = FocalLoss(alpha=1, gamma=0, reduction=\\\"mean\\\")\\nce = nn.CrossEntropyLoss(reduction=\\\"mean\\\")\\noutput = torch.randn(32, N, requires_grad=True)\\ntarget = torch.empty(32, dtype=torch.long).random_(N)\\ntest_close(fl(output, target), ce(output, target))\\n\\n# Test focal loss with gamma > 0 is different than cross entropy\\nfl = FocalLoss(gamma=2)\\nwith torch.no_grad():\\n    test_ne(fl(output, target), ce(output, target))\";\n",
       "                var nbb_formatted_code = \"criterion = FocalLoss(alpha=0.5, gamma=2.0, reduction=\\\"mean\\\")\\n\\nN = 5  # num_classes\\ninput = torch.randn(32, N, requires_grad=True)\\ntarget = torch.empty(32, dtype=torch.long).random_(N)\\nloss = criterion(input, target)\\n\\n\\n# Compare focal loss with gamma = 0 ,cross entropy\\nfl = FocalLoss(alpha=1, gamma=0, reduction=\\\"mean\\\")\\nce = nn.CrossEntropyLoss(reduction=\\\"mean\\\")\\noutput = torch.randn(32, N, requires_grad=True)\\ntarget = torch.empty(32, dtype=torch.long).random_(N)\\ntest_close(fl(output, target), ce(output, target))\\n\\n# Test focal loss with gamma > 0 is different than cross entropy\\nfl = FocalLoss(gamma=2)\\nwith torch.no_grad():\\n    test_ne(fl(output, target), ce(output, target))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "criterion = FocalLoss(alpha=0.5, gamma=2.0, reduction=\"mean\")\n",
    "\n",
    "N = 5  # num_classes\n",
    "input = torch.randn(32, N, requires_grad=True)\n",
    "target = torch.empty(32, dtype=torch.long).random_(N)\n",
    "loss = criterion(input, target)\n",
    "\n",
    "\n",
    "# Compare focal loss with gamma = 0 ,cross entropy\n",
    "fl = FocalLoss(alpha=1, gamma=0, reduction=\"mean\")\n",
    "ce = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "output = torch.randn(32, N, requires_grad=True)\n",
    "target = torch.empty(32, dtype=torch.long).random_(N)\n",
    "test_close(fl(output, target), ce(output, target))\n",
    "\n",
    "# Test focal loss with gamma > 0 is different than cross entropy\n",
    "fl = FocalLoss(gamma=2)\n",
    "with torch.no_grad():\n",
    "    test_ne(fl(output, target), ce(output, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09016d74",
   "metadata": {},
   "source": [
    "## Build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0476b7f4",
   "metadata": {},
   "source": [
    "Losses are created by the Lightning-Tasks in Gale using the Config. To load a loss via gale config the loss must be present in either `LOSS_REGISTRY` or losses available in the `torch.nn.modules.loss`[_module](https://pytorch.org/docs/stable/nn.html#loss-functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9449bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"# export\\ndef build_loss(config: DictConfig):\\n    \\\"\\\"\\\"\\n    Builds a loss from a config.\\n    This assumes a 'name' key in the config which is used to determine what\\n    model class to instantiate. For instance, a config `{\\\"name\\\": \\\"my_loss\\\",\\n    \\\"foo\\\": \\\"bar\\\"}` will find a class that was registered as \\\"my_loss\\\". A custom\\n    loss must first be registerd into `LOSS_REGISTRY`.\\n    \\\"\\\"\\\"\\n\\n    assert \\\"name\\\" in config, f\\\"name not provided for loss: {config}\\\"\\n    config = OmegaConf.to_container(config, resolve=True)\\n\\n    name = config[\\\"name\\\"]\\n    args = config[\\\"init_args\\\"]\\n\\n    # if we are passing weights, we need to change the weights from a list to a tensor\\n    if args is not None:\\n        if \\\"weight\\\" in args and args[\\\"weight\\\"] is not None:\\n            args[\\\"weight\\\"] = torch.tensor(args[\\\"weight\\\"], dtype=torch.float)\\n\\n    if name in LOSS_REGISTRY:\\n        instance = LOSS_REGISTRY.get(name)\\n\\n    # the name should be available in torch.nn.modules.loss\\n    else:\\n        assert hasattr(torch_losses, name), (\\n            f\\\"{name} isn't a registered loss\\\"\\n            \\\", nor is it available in torch.nn.modules.loss\\\"\\n        )\\n        instance = getattr(torch_losses, name)\\n\\n    if args is not None:\\n        loss = instance(**args)\\n    else:\\n        loss = instance()\\n    _logger.debug(\\\"Built loss function: {}\\\".format(loss.__class__.__name__))\\n    return loss\";\n",
       "                var nbb_formatted_code = \"# export\\ndef build_loss(config: DictConfig):\\n    \\\"\\\"\\\"\\n    Builds a loss from a config.\\n    This assumes a 'name' key in the config which is used to determine what\\n    model class to instantiate. For instance, a config `{\\\"name\\\": \\\"my_loss\\\",\\n    \\\"foo\\\": \\\"bar\\\"}` will find a class that was registered as \\\"my_loss\\\". A custom\\n    loss must first be registerd into `LOSS_REGISTRY`.\\n    \\\"\\\"\\\"\\n\\n    assert \\\"name\\\" in config, f\\\"name not provided for loss: {config}\\\"\\n    config = OmegaConf.to_container(config, resolve=True)\\n\\n    name = config[\\\"name\\\"]\\n    args = config[\\\"init_args\\\"]\\n\\n    # if we are passing weights, we need to change the weights from a list to a tensor\\n    if args is not None:\\n        if \\\"weight\\\" in args and args[\\\"weight\\\"] is not None:\\n            args[\\\"weight\\\"] = torch.tensor(args[\\\"weight\\\"], dtype=torch.float)\\n\\n    if name in LOSS_REGISTRY:\\n        instance = LOSS_REGISTRY.get(name)\\n\\n    # the name should be available in torch.nn.modules.loss\\n    else:\\n        assert hasattr(torch_losses, name), (\\n            f\\\"{name} isn't a registered loss\\\"\\n            \\\", nor is it available in torch.nn.modules.loss\\\"\\n        )\\n        instance = getattr(torch_losses, name)\\n\\n    if args is not None:\\n        loss = instance(**args)\\n    else:\\n        loss = instance()\\n    _logger.debug(\\\"Built loss function: {}\\\".format(loss.__class__.__name__))\\n    return loss\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "def build_loss(config: DictConfig):\n",
    "    \"\"\"\n",
    "    Builds a loss from a config.\n",
    "    This assumes a 'name' key in the config which is used to determine what\n",
    "    model class to instantiate. For instance, a config `{\"name\": \"my_loss\",\n",
    "    \"foo\": \"bar\"}` will find a class that was registered as \"my_loss\". A custom\n",
    "    loss must first be registerd into `LOSS_REGISTRY`.\n",
    "    \"\"\"\n",
    "\n",
    "    assert \"name\" in config, f\"name not provided for loss: {config}\"\n",
    "    config = OmegaConf.to_container(config, resolve=True)\n",
    "\n",
    "    name = config[\"name\"]\n",
    "    args = config[\"init_args\"]\n",
    "\n",
    "    # if we are passing weights, we need to change the weights from a list to a tensor\n",
    "    if args is not None:\n",
    "        if \"weight\" in args and args[\"weight\"] is not None:\n",
    "            args[\"weight\"] = torch.tensor(args[\"weight\"], dtype=torch.float)\n",
    "\n",
    "    if name in LOSS_REGISTRY:\n",
    "        instance = LOSS_REGISTRY.get(name)\n",
    "\n",
    "    # the name should be available in torch.nn.modules.loss\n",
    "    else:\n",
    "        assert hasattr(torch_losses, name), (\n",
    "            f\"{name} isn't a registered loss\"\n",
    "            \", nor is it available in torch.nn.modules.loss\"\n",
    "        )\n",
    "        instance = getattr(torch_losses, name)\n",
    "\n",
    "    if args is not None:\n",
    "        loss = instance(**args)\n",
    "    else:\n",
    "        loss = instance()\n",
    "    _logger.debug(\"Built loss function: {}\".format(loss.__class__.__name__))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eb05d0",
   "metadata": {},
   "source": [
    "For Image Classification a loss is created like so ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067d224e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"# hide-output\\nfrom gale.config import get_config\\n\\ncfg = get_config(config_name=\\\"classification\\\")\\n\\n# grab the config for the Loss Function\\nloss_cfg = cfg.training.train_loss_fn\\n\\n# print(OmegaConf.to_yaml(loss_cfg))\\nloss = build_loss(loss_cfg)\";\n",
       "                var nbb_formatted_code = \"# hide-output\\nfrom gale.config import get_config\\n\\ncfg = get_config(config_name=\\\"classification\\\")\\n\\n# grab the config for the Loss Function\\nloss_cfg = cfg.training.train_loss_fn\\n\\n# print(OmegaConf.to_yaml(loss_cfg))\\nloss = build_loss(loss_cfg)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide-output\n",
    "from gale.config import get_config\n",
    "\n",
    "cfg = get_config(config_name=\"classification\")\n",
    "\n",
    "# grab the config for the Loss Function\n",
    "loss_cfg = cfg.training.train_loss_fn\n",
    "\n",
    "# print(OmegaConf.to_yaml(loss_cfg))\n",
    "loss = build_loss(loss_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4871aceb",
   "metadata": {},
   "source": [
    "## Export-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d397540d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01a_losses.ipynb.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"# hide\\nnotebook2script(\\\"01a_losses.ipynb\\\")\";\n",
       "                var nbb_formatted_code = \"# hide\\nnotebook2script(\\\"01a_losses.ipynb\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "notebook2script(\"01a_losses.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3dbb16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gale_dev",
   "language": "python",
   "name": "gale_dev"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
