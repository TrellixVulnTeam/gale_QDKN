# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02b_config.common.ipynb (unless otherwise specified).

__all__ = ['OptimizationConfig', 'DataLoaderConfig', 'DatasetsConfig', 'BaseGaleConfig']

# Cell
from dataclasses import dataclass, field
from typing import *

from hydra.utils import instantiate
from omegaconf import MISSING, DictConfig, OmegaConf
from torch.utils.data._utils.collate import default_collate

from .optimizers import OptimizerParams
from .schedulers import SchedulerParams

# Cell
@dataclass
class OptimizationConfig:
    """
    Config for Optimizer and LR Scheduler for use in Gale.

    *Arguments*:
    1. `optimizer` `(OptimizerParams)`: Config for the Optimizer; must be an instance of OptimizerParams.
    2. `scheduler` `(SchedulerParams)`: Config for the LR Scheduler; must be an instance of SchedulerParams
    3. `interval` `(str)`: either `epoch` (default) for stepping after each epoch ends or `step` for stepping after each optimization step
    4. `monitor` `(optional[str])`: metric to condition for ReduceLROnPlateau scheduler.
    5. `steps_per_epoch` `(int)`: total number of steps per epochs, this value is computed at runtime and mandatory to have.
    6. `max_steps` `(int)`: total number of training steps, computed at runtime or explicitly set here; mandatory to have.
    7. `max_epochs` `(int)`: total number of training epochs, computed at runtime or explicitly set here; mandatory to have.
    """

    optimizer: Optional[OptimizerParams] = MISSING
    scheduler: Optional[SchedulerParams] = MISSING

    # pytorch lightning args for lr scheduler
    interval: str = "epoch"
    monitor: Optional[str] = None
    steps_per_epoch: Optional[int] = None
    max_steps: Optional[int] = None
    max_epochs: Optional[int] = None

# Cell
@dataclass
class DataLoaderConfig:
    """
    Configuration of PyTorch DataLoader.
    For the details on the function/meanings of the arguments, please refer to:
    https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader
    """

    batch_size: int = MISSING
    shuffle: bool = False
    sampler: Optional[Any] = None
    batch_sampler: Optional[Any] = None
    num_workers: int = 0
    collate_fn: Optional[Any] = None
    pin_memory: bool = False
    drop_last: bool = False
    timeout: int = 0
    worker_init_fn: Optional[Any] = None
    multiprocessing_context: Optional[Any] = None

# Cell
@dataclass
class DatasetsConfig:
    """
    Configuration of `Datasets`.

    *Arguments*
    1. `train_ds` `(str)`: name of the training dataset. (must be registered in `DatasetCatalog`)
    2. `valid_ds` `(str)`: name of the validation dataset. (must be registered in `DatasetCatalog`)
    3. `test_ds` `(optional, str)`: name of the test dataset. (must be registered in `DatasetCatalog`)
    4. `train_loader` `(DataLoaderConfig)`: configuration for the train dataloader.
    5. `valid_loader` `(DataLoaderConfig)`: configuration for the valid dataloader.
    6. `valid_loader` `(optional, DataLoaderConfig)`: configuration for the test dataloader.
    """

    train_ds: str = MISSING
    valid_ds: str = MISSING
    train_loader: DataLoaderConfig = MISSING
    valid_loader: DataLoaderConfig = MISSING
    test_ds: Optional[str] = None
    test_loader: Optional[DataLoaderConfig] = None

# Cell
@dataclass
class BaseGaleConfig:
    """
    Default Gale configuration. All other configuration for
    specific tasks are inherited from this base config.

    *Arguments*
    1. `optimization` `(OptimizationConfig)` : Config contraining optimization information
    2. `datasets` `DatasetsConfig`: Config containing information about datasets, dataloaders and how to obtain them.
    """

    optimization: Optional[OptimizationConfig] = MISSING
    datasets: DatasetsConfig = MISSING