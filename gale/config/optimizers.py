# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02_config.optimizers.ipynb (unless otherwise specified).

__all__ = ['create_optimizer', 'OptimizerParams', 'SGDConfig', 'AdamConfig', 'AdamWConfig', 'RMSpropTFConfig',
           'RangerConfig', 'RangerGCConfig', 'SGDPConfig', 'AdamPConfig']

# Cell
from dataclasses import dataclass
from functools import partial
from typing import *

from hydra.utils import instantiate
from omegaconf import MISSING, DictConfig, OmegaConf

from .utility import get_class_path
from ..core.nn.optim.optimizers import *

# Cell
def create_optimizer(
    config: DictConfig, params: Iterable, **kwargs: Optional[Dict[str, Any]]
):
    """
    Convenience method to obtain a Optimizer from config given params.

    Args:
        name: config for the Optimizer. Config can be created via `OptimizerParams`.
        kwargs: Optional kwargs of the optimizer used during instantiation.
    """
    instance = instantiate(config, params=params, **kwargs)
    return instance

# Cell
@dataclass
class OptimizerParams:
    """
    Base Optimizer params with no values.
    """

    _target_: str = MISSING
    lr: Optional[float] = 2e-03
    weight_decay: Optional[float] = 0.0

# Cell
@dataclass
class SGDConfig(OptimizerParams):
    """
    Default configuration for `SGD` optimizer.
    """

    _target_: str = get_class_path(SGD)
    momentum: float = 0
    dampening: float = 0
    nesterov: bool = False

# Cell
@dataclass
class AdamConfig(OptimizerParams):
    """
    Default configuration for Adam optimizer.
    """

    _target_: str = get_class_path(Adam)
    betas: Tuple[float, float] = (0.9, 0.999)
    eps: float = 1e-08
    amsgrad: bool = False

# Cell
@dataclass
class AdamWConfig(OptimizerParams):
    """
    Default configuration for AdamW optimizer.
    """

    _target_: str = get_class_path(AdamW)
    betas: Tuple[float, float] = (0.95, 0.999)
    eps: float = 1e-05
    weight_decay: float = 1e-02
    amsgrad: bool = False

# Cell
@dataclass
class RMSpropTFConfig(OptimizerParams):
    """
    Default configuration for RMSpropTF optimizer from `timm`.
    """

    _target_: str = get_class_path(RMSpropTF)
    alpha: float = 0.9
    eps: float = 1e-10
    weight_decay: float = 0
    momentum: float = 0.0
    centered: bool = False
    decoupled_decay: bool = False
    lr_in_momentum: bool = True

# Cell
@dataclass
class RangerConfig(OptimizerParams):
    """
    Default configuration for Ranger (RAdom + LookAhead) optimizer.
    """

    _target_: str = get_class_path(Ranger)
    betas: Tuple[float, float] = (0.95, 0.999)
    eps: float = 1e-05
    k: int = 6
    alpha: float = 0.5

# Cell
@dataclass
class RangerGCConfig(OptimizerParams):
    """
    Default configuration for RangerGC
    (RAdom + LookAhead + Gradient Centralization) optimizer.
    """

    _target_: str = get_class_path(RangerGC)
    alpha: float = 0.5
    k: int = 6
    N_sma_threshhold: int = 5
    betas: Tuple[float, float] = (0.95, 0.999)
    eps: float = 1e-05
    use_gc: bool = True
    gc_conv_only: bool = False

# Cell
@dataclass
class SGDPConfig(OptimizerParams):
    """
    Default configuration for SGDP Optimizer.
    """

    _target_: str = get_class_path(SGDP)
    momentum: float = 0
    dampening: float = 0
    nesterov: bool = False
    eps: float = 1e-08
    delta: float = 0.1
    wd_ratio: float = 0.1

# Cell
@dataclass
class AdamPConfig(OptimizerParams):
    """
    Default configuration for AdamP Optimizer.
    """

    _target_: str = get_class_path(AdamP)
    betas: Tuple[float, float] = (0.9, 0.999)
    eps: float = 1e-08
    delta: float = 0.1
    wd_ratio: float = 0.1
    nesterov: bool = False