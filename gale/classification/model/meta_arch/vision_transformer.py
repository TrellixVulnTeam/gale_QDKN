# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/04b_classification.model.meta_arch.vit.ipynb (unless otherwise specified).

__all__ = ['VisionTransformer', 'VisionTransformerDataClass']

# Cell
import logging
from collections import namedtuple
from dataclasses import dataclass
from typing import *

import timm
import torch
from fastcore.all import store_attr, use_kwargs_dict
from omegaconf import MISSING, DictConfig, OmegaConf
from pytorch_lightning.core.memory import get_human_readable_count
from timm.optim.optim_factory import add_weight_decay

from ....core_classes import BasicModule
from ....torch_utils import trainable_params
from ....utils.activs import ACTIVATION_REGISTRY
from ....utils.shape_spec import ShapeSpec

_logger = logging.getLogger(__name__)

# Cell
# @TODO: Add support for Discriminative Lr's
class VisionTransformer(BasicModule):
    _hypers = namedtuple("hypers", field_names=["lr", "wd"])
    """
    A interface to create a Vision Transformer from timm. For available model check :
    https://github.com/rwightman/pytorch-image-models/timm/models/vision_transformer.py
    """

    @use_kwargs_dict(
        keep=True,
        num_classes=1000,
        drop_rate=0.0,
        attn_drop_rate=0.0,
        drop_path_rate=0.0,
    )
    def __init__(
        self,
        model_name: str,
        input_shape: ShapeSpec,
        lr: float = 1e-03,
        wd: float = 1e-05,
        pretrained: bool = True,
        freeze_to: Optional[int] = None,
        finetune: Optional[bool] = None,
        act: Optional[str] = None,
        reset_classifier: bool = True,
        filter_wd: bool = True,
        **kwargs,
    ):
        """
        Arguments:
        1. `input_shape` (ShapeSpec): input image shape. For ViT `height=width` and check the above link for avilable model shapes.
        2. `model_name` (str): name of the ViT model, check the above link for avilable models.
        3. `pretrained` (bool): load weights pretrained on imagenet.
        4. `act` (str): name of the activation layer. Must be registerd in `ACTIVATION_REGISTRY`
        5. `num_classes` (int): num output classes.
        6. `drop_rate` (float): dropout rate.
        7. `attn_drop_rate` (float): attention dropout rate.
        8. `drop_path_rate` (float): stochastic depth rate.
        9. `reset_classifier` (bool): resets the weights of the classifier.
        10. `freeze_to` (int): Freeze the param meter groups of the model upto n.
        11. `finetune` (bool): Freeze all the layers and keep only the `classifier` trainable.
        """
        super(VisionTransformer, self).__init__()
        # create model from timm
        assert input_shape.height == input_shape.width
        in_chans = input_shape.channels

        if act is not None:
            act = ACTIVATION_REGISTRY.get(act)

        self.model = timm.create_model(
            model_name, pretrained, in_chans=in_chans, act=act, **kwargs
        )

        if reset_classifier:
            num_cls = kwargs.pop("num_classes")
            self.model.reset_classifier(num_cls)

        if freeze_to is not None:
            self.freeze_to(freeze_to)

        if finetune:
            if freeze_to is not None and isinstance(freeze_to, int):
                msg = "You have sprecified freeze_to along with finetune"
                _logger.warning(msg)
            _logger.info("Freezing all the model parameters except for the classifier")
            self.freeze()

            classifier = ["head", "head_dist"]

            for name, module in self.model.named_children():
                if name in classifier:
                    for p in module.parameters():
                        p.requires_grad_(True)

        store_attr("wd, lr, filter_wd, input_shape")

    def forward(self, batched_inputs: torch.Tensor) -> torch.Tensor:
        """
        Runs the batched_inputs through the created model.
        """
        out = self.model(batched_inputs)
        return out

    @classmethod
    def from_config_dict(cls, cfg: DictConfig):
        """
        Instantiate the Meta Architecture from gale config
        """
        # fmt: off
        input_shape = ShapeSpec(cfg.input.channels, cfg.input.height, cfg.input.width)
        _logger.debug(f"Inputs: {input_shape}")
        instance = super().from_config_dict(cfg.model.meta_architecture.init_args, input_shape=input_shape)
        param_count = get_human_readable_count(sum([m.numel() for m in instance.parameters()]))
        _logger.debug('{} created, param count: {}.'.format(cfg.model.meta_architecture.init_args.model_name, param_count))
        # fmt: on
        return instance

    def build_param_dicts(self):
        """
        Builds up the Paramters dicts for optimization.
        """
        if self.filter_wd:
            param_lists = add_weight_decay(
                self.model,
                weight_decay=self.wd,
                skip_list=self.model.no_weight_decay(),
            )
            param_lists[0]["lr"] = self.lr
            param_lists[1]["lr"] = self.lr
        else:
            ps = trainable_params(self.model)
            param_lists = dict(params=ps, lr=self.lr, wd=self.wd)
        return param_lists

    @property
    def hypers(self) -> Tuple:
        """
        Returns list of parameters like `lr` and `wd`
        for each param group
        """
        lrs = []
        wds = []

        for p in self.build_param_dicts():
            lrs.append(p["lr"])
            wds.append(p["weight_decay"])
        return self._hypers(lrs, wds)

# Cell
@dataclass
class VisionTransformerDataClass:
    model_name: str = MISSING
    lr: float = 1e-03
    wd: float = 1e-05
    pretrained: bool = False
    freeze_to: Optional[int] = None
    finetune: Optional[bool] = True
    reset_classifier: bool = True
    filter_wd: bool = True
    drop_rate: float = 0.0
    attn_drop_rate: float = 0.0
    drop_path_rate: float = 0.0
    num_classes: int = MISSING