# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/04_classification.models.backbones.ipynb (unless otherwise specified).

__all__ = ['IMAGE_CLASSIFIER_BACKBONES', 'has_pool_type', 'prepare_backbone', 'filter_weight_decay',
           'ImageClassificationBackbone', 'TimmBackboneBase', 'TimmBackboneDataClass', 'ResNetBackbone',
           'ResNetBackboneDataClass']

# Cell
import logging
import re
from collections import namedtuple
from dataclasses import dataclass
from typing import *

import timm
import torch
from fastcore.all import store_attr, use_kwargs_dict
from omegaconf import MISSING
from timm.models import ResNet
from torch import nn

from ...core_classes import BasicModule
from ...torch_utils import build_discriminative_lrs, set_bn_eval, trainable_params
from ...utils.activs import ACTIVATION_REGISTRY
from ...utils.shape_spec import ShapeSpec
from ...utils.structures import IMAGE_CLASSIFIER_BACKBONES

_logger = logging.getLogger(__name__)

# Cell
#nbdev_comment _all_ = ["IMAGE_CLASSIFIER_BACKBONES"]

# Cell
def _is_pool_type(l: nn.Module) -> bool:
    """
    True if `l` is a pooling layer.
    From: https://github.com/fastai/fastai/blob/master/fastai/vision/learner.py#L76
    """
    return re.search(r"Pool[123]d$", l.__class__.__name__)


def has_pool_type(m: nn.Module) -> bool:
    """
    Return `True` if `m` is a pooling layer or has one in its children
    From: https://github.com/fastai/fastai/blob/master/fastai/vision/learner.py#L76
    """
    if _is_pool_type(m):
        return True
    for l in m.children():
        if has_pool_type(l):
            return True
    return False

# Cell
def prepare_backbone(model: nn.Module, cut=None):
    "Cut off the body of a typically pretrained `model` as determined by `cut`"
    if cut is None:
        ll = list(enumerate(model.children()))
        cut = next(i for i, o in reversed(ll) if has_pool_type(o))
    if isinstance(cut, int):
        return nn.Sequential(*list(model.children())[:cut])
    elif callable(cut):
        return cut(model)
    else:
        raise NamedError("cut must be either integer or a function")

# Cell
def filter_weight_decay(
    model: nn.Module,
    lr: float,
    weight_decay: float = 1e-5,
    skip_list=(),
) -> List[Dict]:
    """
    Filter out bias, bn and other 1d params from weight decay.
    Modified from: https://github.com/rwightman/pytorch-image-models/timm/optim/optim_factory.py
    """
    decay = []
    no_decay = []
    for name, param in model.named_parameters():
        if not param.requires_grad:
            continue  # frozen weights
        if len(param.shape) == 1 or name.endswith(".bias") or name in skip_list:
            no_decay.append(param)
        else:
            decay.append(param)
    return [
        {"params": no_decay, "weight_decay": 0.0, "lr": lr},
        {"params": decay, "weight_decay": weight_decay, "lr": lr},
    ]

# Cell
class ImageClassificationBackbone(BasicModule):
    """
    Abstract class for ImageClassification BackBones
    """

    _hypers = namedtuple("hypers", field_names=["lr", "wd"])

    def filter_params(self, parameters: List[Dict]):
        """Filters any empty paramter groups in `p`"""
        pgs_filterd = []

        for group in parameters:
            if group["params"] == []:
                pass
            else:
                pgs_filterd += [group]
        return pgs_filterd

    @property
    def hypers(self) -> Tuple:
        """
        Returns list of parameters like `lr` and `wd`
        for each param group
        """
        lrs = []
        wds = []

        for p in self.build_param_dicts():
            lrs.append(p["lr"])
            wds.append(p["weight_decay"])
        return self._hypers(lrs, wds)

    def output_shape(self) -> ShapeSpec:
        """
        Returns the output shape. For most backbones
        this means it will contain the channels in the
        output layer.
        """
        pass

# Cell
class TimmBackboneBase(ImageClassificationBackbone):
    "Create a model from `timm` and converts it into a Image Classification Backbone"

    @use_kwargs_dict(
        keep=True,
        pretrained=True,
        drop_block_rate=None,
        drop_path_rate=None,
        bn_tf=False,
    )
    def __init__(
        self,
        model_name: str,
        input_shape: ShapeSpec,
        act: str = None,
        lr: float = 1e-03,
        wd: float = 0,
        freeze_bn: bool = False,
        freeze_at: int = False,
        filter_wd: bool = False,
        **kwargs,
    ):
        super(TimmBackboneBase, self).__init__()

        store_attr("lr, wd, filter_wd, input_shape")

        if act is not None:
            act = ACTIVATION_REGISTRY.get(act)

        model = timm.create_model(
            model_name,
            act_layer=act,
            global_pool="",
            num_classes=0,
            in_chans=input_shape.channels,
            **kwargs,
        )

        # save some of information from timm models
        self.num_features = model.num_features
        self.timm_model_cfg = model.default_cfg
        self.model = prepare_backbone(model)

        if not freeze_at:
            self.unfreeze()
        else:
            self.freeze_to(freeze_at)

        if freeze_bn:
            set_bn_eval(self.model)

    def forward(self, xb: torch.Tensor) -> torch.Tensor:
        return self.model(xb)

    def build_param_dicts(self) -> List:
        if self.filter_wd:
            ps = filter_weight_decay(self.model, lr=self.lr, weight_decay=self.wd)
        else:
            ps = {
                "params": trainable_params(self.model),
                "lr": self.lr,
                "weight_decay": self.wd,
            }
            ps = [ps]

        return self.filter_params(ps)

    def output_shape(self) -> ShapeSpec:
        return ShapeSpec(self.num_features, None, None)

# Cell
@dataclass
class TimmBackboneDataClass:
    """
    Base config file for `TimmBackboneBase`. You need to pass in a
    `model_name` the opter parameters are optional.
    """

    model_name: str = MISSING
    act: Optional[str] = None
    lr: Any = 1e-03
    wd: Any = 0.0
    freeze_bn: bool = False
    freeze_at: Any = False
    filter_wd: bool = False
    pretrained: bool = True
    drop_block_rate: Optional[float] = None
    drop_path_rate: Optional[float] = None
    bn_tf: bool = False

# Cell
class ResNetBackbone(ImageClassificationBackbone):
    """
    A Backbone for ResNet based models from timm. Note: this class
    does supports all the models listed
    [here](https://github.com/rwightman/pytorch-image-models/blob/e8a64fb88108b592da192e98054095b1ee25e96e/timm/models/resnet.py)
    """

    @use_kwargs_dict(
        keep=True,
        pretrained=True,
        drop_block_rate=0.0,
        drop_path_rate=0.0,
    )
    def __init__(
        self,
        model_name: str,
        input_shape: ShapeSpec,
        act: str = None,
        lr: float = 1e-03,
        wd: float = 1e-02,
        lr_div: float = 100,
        freeze_at: int = 0,
        freeze_bn: bool = False,
        **kwargs,
    ):
        super(ResNetBackbone, self).__init__()
        store_attr("freeze_at, wd, lr, lr_div, input_shape, freeze_bn")

        if act is not None:
            act = ACTIVATION_REGISTRY.get(act)

        model = timm.create_model(
            model_name,
            act_layer=act,
            global_pool="",
            num_classes=0,
            in_chans=input_shape.channels,
            **kwargs,
        )

        assert isinstance(model, ResNet), "ResNetBackbone supports only ResNet models"
        # save some of the input information from timm models
        self.num_features = model.num_features
        self.timm_model_cfg = model.default_cfg

        # break up the model
        # the stem for the resnet model consists of a convolutional block, norm, act, pool
        stem = nn.Sequential(model.conv1, model.bn1, model.act1, model.maxpool)

        # stages will consisit of the remaining 4 layers
        stages = [model.layer1, model.layer2, model.layer3, model.layer4]
        stages = nn.Sequential(*stages)

        # creat the module
        self.resnet = nn.Sequential(stem, stages)
        self.prepare_model(self.resnet)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.resnet(x)

    def build_param_dicts(self) -> Any:
        # model split according to https://github.com/fastai/fastai/blob/master/fastai/vision/learner.py
        p0 = {
            "params": trainable_params(self.resnet[0])
            + trainable_params(self.resnet[1][:3]),
            "weight_decay": self.wd,
        }
        p1 = {"params": trainable_params(self.resnet[1][3:]), "weight_decay": self.wd}
        ps = [p0, p1]
        ps, _ = build_discriminative_lrs(ps, self.lr, self.lr / self.lr_div)
        return self.filter_params(ps)

    def freeze_block(self, m: nn.Module):
        """
        Make this block `m` not trainable.
        """
        for p in m.parameters():
            p.requires_grad = False
        m.eval()

    def prepare_model(self, m: nn.Module):
        """
        Freeze the first several stages of the `ResNet`. Commonly used in fine-tuning.
        """
        if self.freeze_at >= 1:
            _logger.debug("Freezing stem")
            # freeze the stem of the model
            self.freeze_block(m[0])

        # freeze the blocks of the model according to
        # freeze_at
        for idx, stage in enumerate(m[1], start=2):
            if self.freeze_at >= idx:
                _logger.debug(f"Freezing ResBlock {idx - 2 }")
                for block in stage.children():
                    self.freeze_block(block)

        if self.freeze_bn:
            set_bn_eval(m)

    def output_shape(self) -> ShapeSpec:
        return ShapeSpec(self.num_features, None, None)

# Cell
@dataclass
class ResNetBackboneDataClass:
    """
    Base config file for `ResNetBackbone`
    """

    model_name: str = MISSING
    act: Optional[str] = None
    lr: Any = 1e-03
    lr_div: Any = 10
    wd: Any = 0.0
    freeze_at: int = 0
    pretrained: bool = True
    drop_block_rate: Optional[float] = None
    drop_path_rate: Optional[float] = None
    bn_tf: bool = False