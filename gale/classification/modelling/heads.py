# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/04a_classification.modelling.heads.ipynb (unless otherwise specified).

__all__ = ['ImageClassificationHead', 'FullyConnectedHead', 'FastaiHead']

# Cell
from typing import *

import torch
from fastcore.all import L, delegates, ifnone, store_attr, use_kwargs_dict
from timm.models.layers import SelectAdaptivePool2d, create_classifier
from timm.models.layers.classifier import _create_fc, _create_pool
from torch import nn

from .backbones import filter_weight_decay
from ...core.classes import GaleModule
from ...core.nn import ACTIVATION_REGISTRY
from ...core.nn.utils import params, trainable_params
from ...core.structures import IMAGE_CLASSIFIER_HEADS

# Cell
class ImageClassificationHead(GaleModule):
    """
    Abstract class for ImageClassification Heads
    """

    def __init__(self):
        """
        The `__init__` method of any subclass can specify its own set of arguments.
        """
        super().__init__()

    @property
    def get_lrs(self) -> List:
        """
        Returns a List containining the Lrs' for
        each parameter group. This is required to build schedulers
        like `torch.optim.lr_scheduler.OneCycleScheduler` which needs
        the max lrs' for all the Param Groups.
        """
        lrs = []

        for p in self.build_param_dicts():
            lrs.append(p["lr"])
        return lrs

# Cell
@IMAGE_CLASSIFIER_HEADS.register()
class FullyConnectedHead(ImageClassificationHead):
    """
    Classifier head w/ configurable global pooling and dropout.
    From - https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/classifier.py
    """

    def __init__(
        self,
        in_planes: int,
        num_classes: int,
        pool_type: str = "avg",
        drop_rate: float = 0.0,
        use_conv: bool = False,
        lr: float = 2e-03,
        wd: float = 0,
        filter_wd: bool = False,
    ):
        super(FullyConnectedHead, self).__init__()
        self.drop_rate = drop_rate
        # fmt: off
        self.global_pool, num_pooled_features = _create_pool(in_planes, num_classes, pool_type, use_conv=use_conv)
        # fmt: on
        self.fc = _create_fc(num_pooled_features, num_classes, use_conv=use_conv)
        self.flatten_after_fc = use_conv and pool_type
        self.lr, self.wd, self.filter_wd = lr, wd, filter_wd

    def forward(self, x):
        x = self.global_pool(x)
        if self.drop_rate:
            x = F.dropout(x, p=float(self.drop_rate), training=self.training)
        x = self.fc(x)
        return x

    def build_param_dicts(self) -> Any:
        if self.filter_wd:
            ps = filter_weight_decay(self, lr=self.lr, weight_decay=self.wd)
        else:
            # fmt: off
            ps = [{"params": trainable_params(self),"lr": self.lr,"weight_decay": self.wd}]
            # fmt: on
        return ps

# Cell
@IMAGE_CLASSIFIER_HEADS.register()
class FastaiHead(ImageClassificationHead):
    """
    Model head that takes `in_planes` features, runs through `lin_ftrs`, and out `num_classes` classes.


    From -
    https://github.com/fastai/fastai/blob/8b1da8765fc07f1232c20fa8dc5e909d2835640c/fastai/vision/learner.py#L76
    """

    def __init__(
        self,
        in_planes: int,
        num_classes: int,
        act: str = "ReLU",
        lin_ftrs: Optional[List] = None,
        ps: Union[List, int] = 0.5,
        concat_pool: bool = True,
        first_bn: bool = True,
        bn_final: bool = False,
        lr: float = 2e-03,
        wd: float = 0,
        filter_wd: bool = False,
    ):
        super(FastaiHead, self).__init__()
        pool = "catavgmax" if concat_pool else "avg"
        pool, nf = _create_pool(in_planes, num_classes, pool, use_conv=False)

        # fmt: off
        lin_ftrs = [nf, 512, num_classes] if lin_ftrs is None else [nf] + lin_ftrs + [num_classes]
        # fmt: on

        bns = [first_bn] + [True] * len(lin_ftrs[1:])

        ps = L(ps)

        if len(ps) == 1:
            ps = [ps[0] / 2] * (len(lin_ftrs) - 2) + ps

        act = ifnone(act, "ReLU")
        # fmt: off
        actns = [ACTIVATION_REGISTRY.get(act)(inplace=True)] * (len(lin_ftrs) - 2) + [None]
        if bn_final:
            actns[-1] = ACTIVATION_REGISTRY.get(act)(inplace=True)
        # fmt: on

        self.layers = [pool]

        for ni, no, bn, p, actn in zip(lin_ftrs[:-1], lin_ftrs[1:], bns, ps, actns):
            self.layers += nn.Sequential(
                nn.BatchNorm1d(ni), nn.Dropout(p), nn.Linear(ni, no, bias=not bns), actn
            )

        if bn_final:
            self.layers.append(nn.BatchNorm1d(lin_ftrs[-1], momentum=0.01))
        self.layers = nn.Sequential(*[l for l in self.layers if l is not None])

        self.lr, self.wd, self.filter_wd = lr, wd, filter_wd

    def forward(self, xb: torch.Tensor) -> Any:
        return self.layers(xb)

    def build_param_dicts(self) -> Any:
        if self.filter_wd:
            ps = filter_weight_decay(self.layers, lr=self.lr, weight_decay=self.wd)
        else:
            # fmt: off
            ps = [{"params": trainable_params(self.layers),"lr": self.lr,"weight_decay": self.wd}]
            # fmt: on
        return ps