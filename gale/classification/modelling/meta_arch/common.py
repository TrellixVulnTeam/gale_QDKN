# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/04b_classification.modelling.meta_arch.common.ipynb (unless otherwise specified).

__all__ = ['GeneralizedImageClassifier']

# Cell
import logging
from typing import *

import torch
from omegaconf import DictConfig, OmegaConf
from pytorch_lightning.core.memory import get_human_readable_count
from torch.nn import Module

from ..backbones import ImageClassificationBackbone
from ..build import build_backbone, build_head
from ..heads import ImageClassificationHead
from ....core.classes import GaleModule
from ....core.nn.shape_spec import ShapeSpec

_logger = logging.getLogger(__name__)

# Cell
class GeneralizedImageClassifier(GaleModule):
    """
    A General Image Classifier. Any models that contains the following 2 components:
    1. Feature extractor (aka backbone)
    2. Image Classification head (Pooling + Classifier)
    """

    def __init__(
        self,
        backbone: ImageClassificationBackbone,
        head: ImageClassificationHead,
    ):
        """
        Arguments:
        1. `backbone`: a `ImageClassificationBackbone` module, must follow gale's backbone interface
        2. `head`: a head containg the classifier. and the pooling layer, must be an instance of
        `ImageClassificationHead`.
        """
        super(GeneralizedImageClassifier, self).__init__()
        self.backbone = backbone
        assert isinstance(backbone, ImageClassificationBackbone)
        self.head = head
        assert isinstance(head, ImageClassificationHead)

    def forward(self, batched_inputs: torch.Tensor) -> torch.Tensor:
        """
        Runs the batched_inputs through `backbone` followed by the `head`.
        Returns a Tensor which contains the logits for the batched_inputs.
        """
        out = self.backbone(batched_inputs)
        out = self.head(out)
        return out

    @classmethod
    def from_config_dict(cls, cfg: DictConfig):
        """
        Instantiate the Meta Architecture from gale config
        """
        # fmt: off

        if not hasattr(cfg.model, "backbone"):
            _logger.error("Configuration for model backbone not found")
            raise ValueError

        if not hasattr(cfg.model, "head"):
            _logger.error("Configuration for model head not found")
            raise ValueError

        input_shape = ShapeSpec(cfg.input.channels, cfg.input.height, cfg.input.width)
        _logger.debug(f"Inputs: {input_shape}")

        backbone = build_backbone(cfg, input_shape=input_shape)
        param_count = get_human_readable_count(sum([m.numel() for m in backbone.parameters()]))
        _logger.debug('Backbone {} created, param count: {}.'.format(cfg.model.backbone.name, param_count))

        head = build_head(cfg, backbone.output_shape())
        param_count = get_human_readable_count(sum([m.numel() for m in head.parameters()]))
        _logger.debug('Head {} created, param count: {}.'.format(cfg.model.head.name, param_count))

        kwds = {"backbone": backbone, "head": head}

        instance = cls(**kwds)
        instance._cfg = OmegaConf.to_container(cfg.model, resolve=True)
        instance.input_shape = input_shape
        param_count = get_human_readable_count(sum([m.numel() for m in instance.parameters()]))
        _logger.info("Model created, param count: {}.".format(param_count))
        # fmt: on
        return instance

    def build_param_dicts(self):
        """
        Builds up the Paramters dicts for optimization.
        """
        backbone_params = self.backbone.build_param_dicts()
        head_params = self.head.build_param_dicts()
        return backbone_params + head_params

    def get_lrs(self) -> List:
        """
        Returns a List containining the Lrs' for
        each parameter group. This is required to build schedulers
        like `torch.optim.lr_scheduler.OneCycleScheduler` which needs
        the max lrs' for all the Param Groups.
        """
        lrs = []

        for p in self.build_param_dicts():
            lrs.append(p["lr"])
        return lrs