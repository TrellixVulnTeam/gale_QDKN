# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/04_classification.modelling.backbones.ipynb (unless otherwise specified).

__all__ = ['has_pool_type', 'prepare_backbone', 'filter_weight_decay', 'TimmBackboneBase', 'TimmBackboneBaseConfig']

# Cell
import re
from dataclasses import dataclass, field
from typing import *

import torch
from fastcore.all import L, delegates, ifnone, use_kwargs_dict
from omegaconf import MISSING, DictConfig, OmegaConf
from timm import create_model, list_models
from torch import nn

from ...core.classes import GaleModule
from ...core.nn import ACTIVATION_REGISTRY
from ...core.nn.utils import params, set_bn_eval, trainable_params
from ...core.structures import IMAGE_CLASSIFIER_BACKBONES

# Cell
# funtions taken from: https://github.com/fastai/fastai/blob/master/fastai/vision/learner.py#L76
def _is_pool_type(l: nn.Module):
    return re.search(r"Pool[123]d$", l.__class__.__name__)


def has_pool_type(m: nn.Module):
    "Return `True` if `m` is a pooling layer or has one in its children"
    if _is_pool_type(m):
        return True
    for l in m.children():
        if has_pool_type(l):
            return True
    return False

# Cell
def prepare_backbone(model: nn.Module, cut=None):
    "Cut off the body of a typically pretrained `model` as determined by `cut`"
    if cut is None:
        ll = list(enumerate(model.children()))
        cut = next(i for i, o in reversed(ll) if has_pool_type(o))
    if isinstance(cut, int):
        return nn.Sequential(*list(model.children())[:cut])
    elif callable(cut):
        return cut(model)
    else:
        raise NamedError("cut must be either integer or a function")

# Cell
def filter_weight_decay(
    model: nn.Module, lr: float, weight_decay: float = 1e-5, skip_list=()
):
    """
    Filter out bias, bn and other 1d params from weight decay.
    Modified from: [timm](https://github.com/rwightman/pytorch-image-models/blob/e8a64fb88108b592da192e98054095b1ee25e96e/timm/optim/optim_factory.py)
    """
    decay = []
    no_decay = []
    for name, param in model.named_parameters():
        if not param.requires_grad:
            continue  # frozen weights
        if len(param.shape) == 1 or name.endswith(".bias") or name in skip_list:
            no_decay.append(param)
        else:
            decay.append(param)
    return [
        {"params": no_decay, "weight_decay": 0.0, "lr": lr},
        {"params": decay, "weight_decay": weight_decay, "lr": lr},
    ]

# Cell
@IMAGE_CLASSIFIER_BACKBONES.register()
class TimmBackboneBase(GaleModule):
    "Create a model from `timm` and converts it into a Image Classification Backbone"

    @use_kwargs_dict(
        keep=True,
        pretrained=True,  # pretrained model
        in_chans=3,  # number of channels for the first layer
        drop_block_rate=None,  # Drop block rate
        drop_path_rate=None,  # Drop path rate
        bn_tf=False,  # Use Tensorflow BatchNorm defaults for models that support it
    )
    def __init__(
        self,
        model_name: str,
        act: str = None,  # name of the activation layer
        lr: float = 1e-03,  # learning rate for the backbone
        wd: float = 0,  # weight decay for the backbone paramters
        freeze_bn: bool = False,  # wether to freeze the batchnorm layers of the model
        freeze_at: int = False,  # freeze the layers of the backbone, false means train all
        filter_wd: bool = False,  # Filter out bias, bn from weight_decay
        **kwargs
    ):
        super(TimmBackboneBase, self).__init__()
        if act is not None:
            act = ACTIVATION_REGISTRY.get(act)

        model = create_model(
            model_name, act_layer=act, global_pool="", num_classes=0, **kwargs
        )
        self._default_cfg = model.default_cfg
        self._model = prepare_backbone(model)

        if not freeze_at:
            self.unfreeze()
        else:
            self.freeze_to(freeze_at)

        if freeze_bn:
            set_bn_eval(self._model)

        self.lr, self.wd = lr, wd
        self.filter_wd = filter_wd

    def forward(self, xb: torch.Tensor) -> torch.Tensor:
        return self._model(xb)

    def build_param_dicts(self):
        if self.filter_wd:
            ps = filter_weight_decay(self._model, lr=self.lr, weight_decay=self.wd)
        else:
            pd = {
                "params": trainable_params(self._model),
                "lr": self._lr,
                "weight_decay": self._wd,
            }
            ps = [pd]
        return ps

# Cell
@dataclass
class TimmBackboneBaseConfig:
    """
    Base config file for `TimmBackboneBase`
    """

    model_name: str = MISSING
    act: Optional[str] = None
    lr: Any = 1e-03
    wd: Any = 0.0
    freeze_bn: bool = False
    freeze_at: Any = False
    filter_wd: bool = False
    pretrained: bool = True
    in_chans: int = 3
    drop_block_rate: Optional[float] = None
    drop_path_rate: Optional[float] = None
    bn_tf: bool = False