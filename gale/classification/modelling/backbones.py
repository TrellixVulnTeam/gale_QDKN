# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/04_classification.modelling.backbones.ipynb (unless otherwise specified).

__all__ = ['IMAGE_CLASSIFIER_BACKBONES', 'has_pool_type', 'prepare_backbone', 'filter_weight_decay',
           'ImageClassificationBackbone', 'TimmBackboneBase', 'ResNetBackbone']

# Cell
import abc
import logging
import re
from typing import *

import torch
from fastcore.all import store_attr, use_kwargs_dict
from timm import create_model
from timm.models import ResNet
from torch import nn

from ...core.classes import GaleModule
from ...core.nn import ACTIVATION_REGISTRY
from ...core.nn.shape_spec import ShapeSpec
from ...core.nn.utils import set_bn_eval, trainable_params
from ...core.utils.structures import IMAGE_CLASSIFIER_BACKBONES

_logger = logging.getLogger(__name__)

# Cell
#nbdev_comment _all_ = ["IMAGE_CLASSIFIER_BACKBONES"]

# Cell
# funtions taken from: https://github.com/fastai/fastai/blob/master/fastai/vision/learner.py#L76
def _is_pool_type(l: nn.Module):
    return re.search(r"Pool[123]d$", l.__class__.__name__)


def has_pool_type(m: nn.Module):
    "Return `True` if `m` is a pooling layer or has one in its children"
    if _is_pool_type(m):
        return True
    for l in m.children():
        if has_pool_type(l):
            return True
    return False

# Cell
def prepare_backbone(model: nn.Module, cut=None):
    "Cut off the body of a typically pretrained `model` as determined by `cut`"
    if cut is None:
        ll = list(enumerate(model.children()))
        cut = next(i for i, o in reversed(ll) if has_pool_type(o))
    if isinstance(cut, int):
        return nn.Sequential(*list(model.children())[:cut])
    elif callable(cut):
        return cut(model)
    else:
        raise NamedError("cut must be either integer or a function")

# Cell
def filter_weight_decay(
    model: nn.Module,
    lr: float,
    weight_decay: float = 1e-5,
    skip_list=(),
) -> List[Dict]:
    """
    Filter out bias, bn and other 1d params from weight decay.
    Modified from: [timm](https://github.com/rwightman/pytorch-image-models/blob/e8a64fb88108b592da192e98054095b1ee25e96e/timm/optim/optim_factory.py)
    """
    decay = []
    no_decay = []
    for name, param in model.named_parameters():
        if not param.requires_grad:
            continue  # frozen weights
        if len(param.shape) == 1 or name.endswith(".bias") or name in skip_list:
            no_decay.append(param)
        else:
            decay.append(param)
    return [
        {"params": no_decay, "weight_decay": 0.0, "lr": lr},
        {"params": decay, "weight_decay": weight_decay, "lr": lr},
    ]

# Cell
class ImageClassificationBackbone(GaleModule, metaclass=abc.ABCMeta):
    """
    Abstract class for ImageClassification BackBones
    """

    def __init__(self):
        """
        The `__init__` method of any subclass can specify its own set of arguments.
        """
        super().__init__()

    def get_lrs(self) -> List:
        """
        Returns a List containining the Lrs' for
        each parameter group. This is required to build schedulers
        like `torch.optim.lr_scheduler.OneCycleScheduler` which needs
        the max lrs' for all the Param Groups.
        """
        lrs = []

        for p in self.build_param_dicts():
            lrs.append(p["lr"])
        return lrs

    @abc.abstractmethod
    def output_shape(self) -> ShapeSpec:
        """
        Returns the output shape. For most backbones
        this means it will contain the channels in the
        output layer.
        """
        pass

# Cell
class TimmBackboneBase(ImageClassificationBackbone):
    "Create a model from `timm` and converts it into a Image Classification Backbone"

    @use_kwargs_dict(
        keep=True,
        pretrained=True,
        drop_block_rate=None,
        drop_path_rate=None,
        bn_tf=False,
    )
    def __init__(
        self,
        model_name: str,
        input_shape: ShapeSpec,
        act: str = None,
        lr: float = 1e-03,
        wd: float = 0,
        freeze_bn: bool = False,
        freeze_at: int = False,
        filter_wd: bool = False,
        **kwargs,
    ):
        super(TimmBackboneBase, self).__init__()

        store_attr("lr, wd, filter_wd, input_shape")

        if act is not None:
            act = ACTIVATION_REGISTRY.get(act)

        # fmt: off
        model = create_model(model_name, act_layer=act, global_pool="", num_classes=0,
                             in_chans=input_shape.channels, **kwargs)
        # fmt: on

        # save some of the input information from timm models
        self.num_features = model.num_features
        self.timm_model_cfg = model.default_cfg
        self.model = prepare_backbone(model)

        if not freeze_at:
            self.unfreeze()
        else:
            self.freeze_to(freeze_at)

        if freeze_bn:
            set_bn_eval(self.model)

    def forward(self, xb: torch.Tensor) -> torch.Tensor:
        return self.model(xb)

    def build_param_dicts(self):
        if self.filter_wd:
            ps = filter_weight_decay(self.model, lr=self.lr, weight_decay=self.wd)
        else:
            # fmt: off
            ps = [{"params": trainable_params(self.model),"lr": self.lr,"weight_decay": self.wd}]
            # fmt: on
        return ps

    def output_shape(self) -> ShapeSpec:
        return ShapeSpec(self.num_features, None, None)

# Cell
# fmt: off
class ResNetBackbone(ImageClassificationBackbone):
    """
    A Backbone for ResNet based models from timm. Note: this class
    does supports all the models listed [here](https://github.com/rwightman/pytorch-image-models/blob/e8a64fb88108b592da192e98054095b1ee25e96e/timm/models/resnet.py)
    """

    @use_kwargs_dict(
        keep=True,
        pretrained=True,
        drop_block_rate=None,
        drop_path_rate=None,
        bn_tf=False,
    )

    def __init__(
        self,
        model_name: str,
        input_shape: ShapeSpec,
        act: str = None,
        lr: float = 1e-03,
        lr_div: float = 10,
        wd: float = 0,
        freeze_at: int = 0,
        **kwargs
    ):
        super(ResNetBackbone, self).__init__()
        store_attr("freeze_at, wd, lr, lr_div, input_shape", self)

        if act is not None:
            act = ACTIVATION_REGISTRY.get(act)

        model = create_model(model_name, act_layer=act, global_pool="", num_classes=0,
                             in_chans=input_shape.channels, **kwargs)

        assert isinstance(model, ResNet), "ResNetBackbone supports only ResNet models"
        # save some of the input information from timm models
        self.num_features = model.num_features
        self.timm_model_cfg = model.default_cfg

        # break up the model
        self.stem = nn.Sequential(model.conv1, model.bn1, model.act1, model.maxpool)
        self.stages = nn.Sequential(model.layer1, model.layer2, model.layer3, model.layer4)

        self.prepare_model()


    def forward(self, xb: torch.Tensor) -> torch.Tensor:
        out = self.stem(xb)
        return self.stages(out)

    def build_param_dicts(self) -> Any:
        p0 = {"params": trainable_params(self.stem), "lr": self.lr/self.lr_div, "weight_decay": self.wd}
        p1 = {"params": trainable_params(self.stages[0:2]), "lr": self.lr/self.lr_div, "weight_decay": self.wd}
        p2 = {"params": trainable_params(self.stages[2:]), "lr": self.lr, "weight_decay": self.wd}
        return [p0, p1, p2]


    def freeze_block(self, m: nn.Module):
        """
        Make this block `m` not trainable.
        This method sets all parameters to `requires_grad=False`,
        and convert all BatchNorm Layers in eval mode
        """
        for p in m.parameters():
            p.requires_grad = False
        set_bn_eval(m)

    def prepare_model(self):
        """
        Freeze the first several stages of the ResNet. Commonly used in fine-tuning.
        """
        if self.freeze_at >= 1:
            self.freeze_block(self.stem)
        for idx, stage in enumerate(self.stages, start=2):
            if self.freeze_at >= idx:
                for block in stage.children():
                    self.freeze_block(block)

    def output_shape(self) -> ShapeSpec:
        return ShapeSpec(self.num_features, None, None)
# fmt: on