# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/06_classification.task.ipynb (unless otherwise specified).

__all__ = ['logger', 'Mixup', 'ClassificationTask']

# Cell
import logging
from typing import *

import numpy as np
import pytorch_lightning as pl
import torch
import torch.nn.functional as F
import torchmetrics
from fastcore.all import *
from omegaconf import DictConfig, ListConfig, OmegaConf
from timm.data.mixup import Mixup, mixup_target
from torch import nn

from .data import (
    build_classification_loader_from_config,
    cifar_stats,
    imagenet_stats,
    mnist_stats,
    show_image_batch,
)
from .modelling import build_model
from ..core.classes import GaleTask, GaleModule
from ..core.nn.losses import build_loss
from ..core.nn.utils import trainable_params

logger = logging.getLogger(__name__)

# Cell
# hide
# fmt: off
class Mixup(Mixup):
    """
    CPU friendly Mixup from timm
    """
    def __call__(self, x, target):
        assert len(x) % 2 == 0, 'Batch size should be even when using this'
        if self.mode == 'elem':
            lam = self._mix_elem(x)
        elif self.mode == 'pair':
            lam = self._mix_pair(x)
        else:
            lam = self._mix_batch(x)
        target = mixup_target(target, self.num_classes, lam, self.label_smoothing, device=x.device)
        return x, target
# fmt: on

# Cell
class ClassificationTask(GaleTask):
    is_restored = True
    """
    A General PyTorch Lightning Task for Image Classification

    Arguments:
    1. cfg: gale default config.
    2. trainer (Optional): Pytorch Lightning Trainer instance
    2. metrics (Optional): A List of `torchmetrics` used during training.
    """

    def __init__(
        self,
        cfg: DictConfig,
        trainer: pl.Trainer = None,
        metrics: Union[torchmetrics.Metric, Mapping, Sequence, None] = None,
    ):
        super(ClassificationTask, self).__init__(
            cfg=cfg, trainer=trainer, metrics=metrics
        )
        # Train Loss is used for the Training Dataset
        self.train_loss = noop
        # Eval Loss is used for Validation / Test Datasets
        self.eval_loss = noop

    def setup(self, stage: Optional[str] = None):
        # that means model has not been build manually
        # so we need to build it
        if self._model is noop:
            self.setup_model()

        # if the trainer is passed, that means we are in training
        # so we need to setup the train_dataloaders, valid_dataloaders (Optional)
        # test_dataloaders (Optional) and the optimziation for ht emodel
        if not self.is_restored:
            if self._train_dl is noop:
                self.setup_training_data()
            if self._validation_dl is noop:
                self.setup_validation_data()
            if self._test_dl is noop:
                self.setup_test_data()
            if self._optimizer is noop and self._scheduler is noop:
                optim = self.process_optim_config(self._cfg.optimization)
                self.setup_optimization(optim)

            # setup mixup/cutmix for the model
            mixup_args = self._cfg.training.mixup.init_args
            self.mixup_off_epoch = self._cfg.training.mixup.off_epoch
            self.mixup_fn = Mixup(**mixup_args)

            # build up the loss functions
            self.train_loss = build_loss(self._cfg.training.train_loss_fn)
            self.eval_loss = build_loss(self._cfg.training.eval_loss_fn)

            shapes = (
                self._cfg.input.channels,
                self._cfg.input.height,
                self._cfg.input.width,
            )
            self.example_input_array = torch.randn(1, *shapes)
        else:
            pass

    def forward(self, x):
        """
        Forward method: we pass in the input through the meta_arch
        to get the predictions for the current image batch
        """
        return self._model(x)

    def shared_step(self, batch: Any, batch_idx: int, stage: str) -> Dict:
        stages = ["train", "validation", "test"]
        assert stage in stages

        # Check wether Mixup Threshold is reached and stop mixup
        # makes no sense to check in other stages; so check in
        # the training stage
        if stage == "train":
            if self.mixup_off_epoch and self.current_epoch >= self.mixup_off_epoch:
                self.mixup_fn.mixup_enabled = False

        # Unpack Batch
        x, y = batch

        # Apply mixup in the training stage
        if stage == "train":
            # NOTE: This converts the targets into 1 hot vectores
            x, y_mix = self.mixup_fn(x, y)

        # calculate the logits
        y_hat = self(x)

        # Comput Loss
        if stage == "train":
            loss = self.train_loss(y_hat, y_mix)
        else:
            loss = self.eval_loss(y_hat, y)

        # compute probas
        y_hat = F.softmax(y_hat)

        logs = {}
        logs["loss"] = loss

        for name, metric in self.metrics.items():
            if isinstance(metric, torchmetrics.metric.Metric):
                metric(y_hat, y)
                logs[name] = metric
            else:
                logs[name] = metric(y_hat, y)

        output = dict(loss=loss, logs=logs)
        return output

    def setup_model(self, args: DictConfig = None):
        """
        Builds up the meta architecture. You can also additionally pass in args to configure
        from a config other than the orignal one.
        """
        conf = ifnone(args, self._cfg)
        meta_arch = build_model(conf)
        self._model = meta_arch

    @property
    def param_dicts(self):
        """Returns the paramters for model optimization"""
        return (
            self._model.build_param_dicts()
            if self._model is not noop
            else trainable_params(self)
        )

    def setup_training_data(self, name: str = None, dls_conf: DictConfig = None):
        """
        Builds the training dataset from name and the dataloader from `dls_conf`, if
        None then parsers the values from the passed config while creating the instance
        """
        name = ifnone(name, self._cfg.datasets.train)
        conf = ifnone(dls_conf, self._cfg.dataloader.train)
        self._train_dl = build_classification_loader_from_config(name, conf)

    def setup_validation_data(
        self, name: Union[List, str] = None, dls_conf: DictConfig = None
    ):
        """Same as `setup_training_data` but sets up validation dataset and dataloaders"""
        name = ifnone(name, self._cfg.datasets.valid)
        conf = ifnone(dls_conf, self._cfg.dataloader.valid)

        # if name is still none , that means no validation data
        if name is None:
            self._validation_dl = None
        else:
            if isinstance(name, list) or isinstance(name, ListConfig):
                names = list(name)
                dls = [build_classification_loader_from_config(n, conf) for n in names]
            elif isinstance(name, str):
                dls = build_classification_loader_from_config(name, conf)
            else:
                logger.warning(
                    "Validation dataset name format not understood. Must either be str or List."
                )
                dls = None
            self._validation_dl = dls

    def setup_test_data(
        self, name: Union[List, str] = None, dls_conf: DictConfig = None
    ):
        """Same as `setup_training_data` but sets up test dataset and dataloaders"""
        name = ifnone(name, self._cfg.datasets.test)
        conf = ifnone(dls_conf, self._cfg.dataloader.test)

        # if name is still none , that means no validation data
        if name is None:
            self._test_dl = None
        else:
            if isinstance(name, list) or isinstance(name, ListConfig):
                names = list(name)
                dls = [build_classification_loader_from_config(n, conf) for n in names]
            elif isinstance(name, str):
                dls = build_classification_loader_from_config(name, conf)
            else:
                logger.warning(
                    "Test dataset name format not understood. Must either be str or List"
                )
                dls = None
            self._test_dl = dls

    def test_dataloader(self):
        self.setup_test_data()
        if self._test_dl is None or self._test_dl is noop:
            return None
        else:
            return self._test_dl

    @use_kwargs_dict(keep=True, n=8, nrows=2, ncols=4, figsize=None, imsize=3)
    def show_batch(self, prefix: str = "train", **kwargs):
        """Displays a batch from a dataloader defined by prefix"""
        if self._train_dl is noop:
            self.setup()

        if prefix == "train":
            loader = [self._train_dl]

        elif prefix == "validation":
            loader = self._validation_dl

        elif prefix == "test":
            loader = self._test_dl
        else:
            raise ValueError("Unkonwn stage must be train, validation or test")

        if loader is None:
            logger.warning(f"No Dataset and DataLoader provied for {prefix} stage")
            logger.info("Skipping show batch")
            return
        else:
            # DataLoaders can either be List or DataLoader
            try:
                loader = loader[0]
            except:
                pass

        inputs, classes = next(iter(loader))

        if prefix == "train":
            inputs, _ = self.mixup_fn(inputs, classes)

        if self._cfg.input.mean == "imagenet":
            mean, std = imagenet_stats
        elif self._cfg.input.mean == "cifar":
            mean, std = cifar_stats
        elif self._cfg.input.mean == "mnist":
            mean, std = mnist_stats
        else:
            mean, std = np.array(self._cfg.input.mean), np.array(self._cfg.input.std)

        mean = torch.tensor(np.array(mean)).float()
        std = torch.tensor(np.array(std)).float()

        batch = (inputs, classes)

        show_image_batch(batch, mean=mean, std=std, **kwargs)

    @property
    def model(self):
        return self._model

    @model.setter
    def model(self, m: GaleModule):
        assert isinstance(m, GaleModule)
        self._model = m