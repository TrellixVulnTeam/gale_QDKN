# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/06_classification.task.ipynb (unless otherwise specified).

__all__ = ['logger', 'get_callable_name', 'get_callable_dict', 'Mixup', 'ClassificationTask']

# Cell
import logging
from typing import *

import numpy as np
import pytorch_lightning as pl
import torch
import torch.nn.functional as F
import torchmetrics
from fastcore.all import *
from omegaconf import DictConfig, ListConfig, OmegaConf
from timm.data.mixup import Mixup, mixup_target
from torch import nn

from .data import (
    build_classification_loader_from_config,
    cifar_stats,
    imagenet_stats,
    mnist_stats,
    show_image_batch,
)
from .modelling import build_model
from ..core.classes import GaleTask
from ..core.nn.losses import build_loss

logger = logging.getLogger(__name__)

# Cell
def get_callable_name(fn_or_class: Union[Callable, object]) -> str:
    return getattr(fn_or_class, "__name__", fn_or_class.__class__.__name__).lower()


def get_callable_dict(fn: Union[Callable, Mapping, Sequence]) -> Union[Dict, Mapping]:
    if isinstance(fn, Mapping):
        return fn
    elif isinstance(fn, Sequence):
        return {get_callable_name(f): f for f in fn}
    elif callable(fn):
        return {get_callable_name(fn): fn}

# Cell
# hide
# fmt: off
class Mixup(Mixup):
    """
    CPU friendly Mixup from timm
    """
    def __call__(self, x, target):
        assert len(x) % 2 == 0, 'Batch size should be even when using this'
        if self.mode == 'elem':
            lam = self._mix_elem(x)
        elif self.mode == 'pair':
            lam = self._mix_pair(x)
        else:
            lam = self._mix_batch(x)
        target = mixup_target(target, self.num_classes, lam, self.label_smoothing, device=x.device)
        return x, target
# fmt: on

# Cell
class ClassificationTask(GaleTask):
    is_restored = True
    """
    A General PyTorch Lightning Task for Image Classification

    Arguments:
    1. cfg: gale default config.
    2. trainer (Optional): Pytorch Lightning Trainer instance
    2. metrics (Optional): A List of `torchmetrics` used during training.
    """

    def __init__(
        self,
        cfg: DictConfig,
        trainer: pl.Trainer = None,
        metrics: Union[torchmetrics.Metric, Mapping, Sequence, None] = None,
    ):
        super(ClassificationTask, self).__init__(
            cfg=cfg, trainer=trainer, metrics=metrics
        )
        if trainer is not None and not isinstance(trainer, pl.Trainer):
            msg = f"trainer constructor argument must be either None or pytroch_lightning.Trainer. But got {type(trainer)} instead."
            raise ValueError(msg)

        self._cfg = OmegaConf.structured(cfg)
        self.save_hyperparameters(self._cfg)

        self._train_dl = noop
        self._validation_dl = noop
        self._test_dl = noop
        self._optimizer = noop
        self._scheduler = noop
        self._trainer = trainer
        self.metrics = nn.ModuleDict(
            {} if metrics is None else get_callable_dict(metrics)
        )

        self.setup_model()

        # if trained is nor passed them the Model is being restored
        if self._trainer is not None:
            self.is_restored = False

        if not self._is_model_being_restored:
            self.setup_training_data()
            self.setup_validation_data()
            self.setup_test_data()

        mixup_args = self._cfg.training.mixup.init_args
        self.mixup_off_epoch = self._cfg.training.mixup.off_epoch
        self.mixup_fn = Mixup(**mixup_args)

        # sets up loss_fn
        self.train_loss_fn = build_loss(self._cfg.training.train_loss_fn)
        self.eval_loss_fn = build_loss(self._cfg.training.eval_loss_fn)

    @use_kwargs_dict(keep=True, n=8, nrows=2, ncols=4, figsize=None, imsize=3)
    def show_batch(self, prefix: str = "train", **kwargs):
        """
        Displays a batch of Image from the given dataloaders. If there are
        multiple test and validation dataloaders will display images from
        the 1st dataloader.
        """
        if prefix == "train":
            loader = self._train_dl

        elif prefix == "valid" or prefix == "validation":
            if self._validation_dl is None:
                raise ValueError("No validation dataloader(s) provided")
            else:
                if isinstance(self._validation_dl, list):
                    loader = self._validation_dl[0]

                else:
                    loader = self._validation_dl

        elif prefix == "test":
            if self._test_dl is None:
                raise ValueError("No test dataloader(s) provided")
            else:
                if isinstance(self._test_dl, list):
                    loader = self._test_dl[0]

                else:
                    loader = self._test_dl

        samples = next(iter(loader))

        inputs, classes = samples

        if prefix == "train":
            inputs, mix_classes = self.mixup_fn(inputs, classes)

        if self._cfg.input.mean == "imagenet":
            mean, std = imagenet_stats
        elif self._cfg.input.mean == "cifar":
            mean, std = cifar_stats
        elif self._cfg.input.mean == "mnist":
            mean, std = mnist_stats
        else:
            mean, std = np.array(self._cfg.input.mean), np.array(self._cfg.input.std)

        mean = torch.tensor(np.array(mean)).float()
        std = torch.tensor(np.array(std)).float()

        batch = (inputs, classes)

        show_image_batch(batch, mean=mean, std=std, **kwargs)

    @property
    def _is_model_being_restored(self):
        return self.is_restored

    @_is_model_being_restored.setter
    def _is_model_being_restored(self, x: bool):
        self.is_restored = x

    def setup_model(self, args: DictConfig = None):
        """
        Sets up the Image Classification MetaArch.
        If `args` is passed then the model is built from `args`.
        """
        conf = ifnone(args, self._cfg)
        self._model = build_model(conf)

    def forward(self, xb: torch.Tensor):
        """
        Returns logits for given image batch `xb`
        """
        return self._model(xb)

    # fmt:off
    def setup_training_data(self, name: str = None, args: DictConfig = None):
        """
        Setups the training DataLoader using `config`.
        You also optionally setup training data by manually `ClassificationTask.setup_training_data`
        and giving in name of the registerd dataset and args for the DataLoader. If `name`
        and `args` are passed training dataloader is instantiate from them.
        """
        name = ifnone(name, self._cfg.datasets.train)
        conf = ifnone(args, self._cfg.dataloader.train)
        self._train_dl = build_classification_loader_from_config(name, conf)

    def setup_validation_data(self, name: Union[List, str] = None, args: DictConfig = None):
        """
        Sets up the Validation DataLoader (s) using `config`. If `name`
        and `args` are passed then dataloader(s) is instantiate from them.
        """
        name = ifnone(name, self._cfg.datasets.valid)
        conf = ifnone(args, self._cfg.dataloader.valid)

        if name is None:
            self._validation_dl = None

        elif isinstance(name, str):
            self._validation_dl = build_classification_loader_from_config(name, conf)

        elif isinstance(name, List) or isinstance(name, ListConfig):
            name = list(name)
            self._validation_dl = [build_classification_loader_from_config(n, conf) for n in name]

        else:
            logger.warning("Dataset name format not understood. Must either be str or list. Defaulting to None")
            self._validation_dl = None
    # fmt:on

    @property
    def param_dicts(self) -> Union[Iterator, List[Dict]]:
        """
        Parameter groups for optimizer for the optimizer to optimizer. This
        method calls in `build_param_dicts()` method of the meta_arch and filters
        the empty paramters groups
        """
        pgs = self._model.build_param_dicts()
        pgs_filterd = []

        for group in pgs:
            if group["params"] == []:
                pass
            else:
                pgs_filterd += [group]
        return pgs_filterd

# Cell
# fmt: off
@patch
def setup_test_data(self: ClassificationTask, name: Union[List, str] = None, args: DictConfig = None):
    """
    Sets up the Test DataLoader (s) using `config`. If `name`
    and `args` are passed then dataloader(s) is instantiate from them.
    """
    name = ifnone(name, self._cfg.datasets.test)
    conf = ifnone(args, self._cfg.dataloader.test)

    if name is None:
        self._test_dl = None

    elif isinstance(name, str):
        self._test_dl = build_classification_loader_from_config(name, conf)

    elif isinstance(name, List) or isinstance(name, ListConfig):
        name = list(name)
        self._test_dl = [build_classification_loader_from_config(n, conf) for n in name]

    else:
        logger.warning("Dataset name format not understood. Must either be str or list. Defaulting to None")
        self._test_dl = None
# fmt:on

# Cell
@patch
def shared_step(self: ClassificationTask, batch: Any, batch_idx: int, stage: str):
    """
    The common training/validation/test step. Override for custom behavior. This step
    is shared between training/validation/test step. For training/validation/test steps
    `stage` is train/val/test respectively.

    This step does the following:
    1. Also applies `mixup` to the training data.
    2. Computes the loss and metric values for each metrics provided.
    3. Returns the loss to optimize in training step.
    """

    if self.mixup_off_epoch and self.current_epoch >= self.mixup_off_epoch:
        if self.mixup_fn is not None:
            self.mixup_fn.mixup_enabled = False

    x, y = batch

    if stage == "train":
        y_mix = None
        if self.mixup_fn is not None:
            x, y_mix = self.mixup_fn(x, y)

    y_hat = self(x)

    if stage == "train":
        if y_mix is not None:
            loss = self.train_loss_fn(y_hat, y_mix)
        else:
            loss = self.train_loss_fn(y_hat, y)
    else:
        loss = self.eval_loss_fn(y_hat, y)

    y_hat = F.softmax(y_hat)

    logs = {}

    for name, metric in self.metrics.items():
        if isinstance(metric, torchmetrics.metric.Metric):
            metric(y_hat, y)
            logs[name] = metric  # log the metric itself if it is of type Metric
        else:
            logs[name] = metric(y_hat, y)

        self.log_dict({f"{stage}_{k}": v for k, v in logs.items()})

    # Return loss for the training step
    if stage == "train":
        return loss