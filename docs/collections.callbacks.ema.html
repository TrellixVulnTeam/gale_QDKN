---

title: Model Exponential Moving Average Callback for PyTorch Lightning


keywords: fastai
sidebar: home_sidebar

summary: "This is intended to allow functionality like https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage"
description: "This is intended to allow functionality like https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage"
nb_path: "nbs/07b_collections.callbacks.ema.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/07b_collections.callbacks.ema.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="cd0231aa-a152-4f0f-9f89-c29b1a55d9cb"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#cd0231aa-a152-4f0f-9f89-c29b1a55d9cb');

            setTimeout(function() {
                var nbb_cell_id = 5;
                var nbb_unformatted_code = "# export\nclass EMACallback(Callback):\n    \"\"\"\n    Model Exponential Moving Average. Empirically it has been found that using the moving average \n    of the trained parameters of a deep network is better than using its trained parameters directly.\n    \n    If `use_ema_weights`, then the ema parameters of the network is set after training end.\n    \"\"\"\n    def __init__(self, decay=0.9999, use_ema_weights: bool = True):\n        self.decay = decay\n        self.ema = None\n        self.use_ema_weights = use_ema_weights\n\n    def on_fit_start(self, trainer, pl_module):\n        \"Initialize `ModelEmaV2` from timm to keep a copy of the moving average of the weights\"\n        self.ema = ModelEmaV2(pl_module, decay=self.decay, device=None)\n\n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n        \"Update the stored parameters using a moving average\"\n        # Update currently maintained parameters.\n        self.ema.update(pl_module)\n\n    def on_validation_epoch_start(self, trainer, pl_module):\n        \"do validation using the stored parameters\"\n        # save original parameters before replacing with EMA version\n        self.store(pl_module.parameters())\n\n        # update the LightningModule with the EMA weights\n        # ~ Copy EMA parameters to LightningModule\n        self.copy_to(self.ema.module.parameters(), pl_module.parameters())\n\n    def on_validation_end(self, trainer, pl_module):\n        \"Restore original parameters to resume training later\"\n        self.restore(pl_module.parameters())\n\n    def on_train_end(self, trainer, pl_module):\n        # update the LightningModule with the EMA weights\n        if self.use_ema_weights:\n            self.copy_to(self.ema.module.parameters(), pl_module.parameters())\n            msg = \"Model weights replaced with the EMA version.\"\n            log_main_process(_logger, logging.INFO, msg)\n            \n    def on_save_checkpoint(self, trainer, pl_module, checkpoint):\n        if self.ema is not None:\n            return {\"state_dict_ema\": get_state_dict(self.ema, unwrap_model)}\n    \n    def on_load_checkpoint(self, callback_state):\n        if self.ema is not None:\n            self.ema.module.load_state_dict(callback_state[\"state_dict_ema\"])\n\n    def store(self, parameters):\n        \"Save the current parameters for restoring later.\"\n        self.collected_params = [param.clone() for param in parameters]       \n\n    def restore(self, parameters):\n        \"\"\"\n        Restore the parameters stored with the `store` method.\n        Useful to validate the model with EMA parameters without affecting the\n        original optimization process.\n        \"\"\"\n        for c_param, param in zip(self.collected_params, parameters):\n            param.data.copy_(c_param.data)\n\n    def copy_to(self, shadow_parameters, parameters):\n        \"Copy current parameters into given collection of parameters.\"\n        for s_param, param in zip(shadow_parameters, parameters):\n            if param.requires_grad:\n                param.data.copy_(s_param.data)";
                var nbb_formatted_code = "# export\nclass EMACallback(Callback):\n    \"\"\"\n    Model Exponential Moving Average. Empirically it has been found that using the moving average\n    of the trained parameters of a deep network is better than using its trained parameters directly.\n\n    If `use_ema_weights`, then the ema parameters of the network is set after training end.\n    \"\"\"\n\n    def __init__(self, decay=0.9999, use_ema_weights: bool = True):\n        self.decay = decay\n        self.ema = None\n        self.use_ema_weights = use_ema_weights\n\n    def on_fit_start(self, trainer, pl_module):\n        \"Initialize `ModelEmaV2` from timm to keep a copy of the moving average of the weights\"\n        self.ema = ModelEmaV2(pl_module, decay=self.decay, device=None)\n\n    def on_train_batch_end(\n        self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx\n    ):\n        \"Update the stored parameters using a moving average\"\n        # Update currently maintained parameters.\n        self.ema.update(pl_module)\n\n    def on_validation_epoch_start(self, trainer, pl_module):\n        \"do validation using the stored parameters\"\n        # save original parameters before replacing with EMA version\n        self.store(pl_module.parameters())\n\n        # update the LightningModule with the EMA weights\n        # ~ Copy EMA parameters to LightningModule\n        self.copy_to(self.ema.module.parameters(), pl_module.parameters())\n\n    def on_validation_end(self, trainer, pl_module):\n        \"Restore original parameters to resume training later\"\n        self.restore(pl_module.parameters())\n\n    def on_train_end(self, trainer, pl_module):\n        # update the LightningModule with the EMA weights\n        if self.use_ema_weights:\n            self.copy_to(self.ema.module.parameters(), pl_module.parameters())\n            msg = \"Model weights replaced with the EMA version.\"\n            log_main_process(_logger, logging.INFO, msg)\n\n    def on_save_checkpoint(self, trainer, pl_module, checkpoint):\n        if self.ema is not None:\n            return {\"state_dict_ema\": get_state_dict(self.ema, unwrap_model)}\n\n    def on_load_checkpoint(self, callback_state):\n        if self.ema is not None:\n            self.ema.module.load_state_dict(callback_state[\"state_dict_ema\"])\n\n    def store(self, parameters):\n        \"Save the current parameters for restoring later.\"\n        self.collected_params = [param.clone() for param in parameters]\n\n    def restore(self, parameters):\n        \"\"\"\n        Restore the parameters stored with the `store` method.\n        Useful to validate the model with EMA parameters without affecting the\n        original optimization process.\n        \"\"\"\n        for c_param, param in zip(self.collected_params, parameters):\n            param.data.copy_(c_param.data)\n\n    def copy_to(self, shadow_parameters, parameters):\n        \"Copy current parameters into given collection of parameters.\"\n        for s_param, param in zip(shadow_parameters, parameters):\n            if param.requires_grad:\n                param.data.copy_(s_param.data)";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="EMACallback" class="doc_header"><code>class</code> <code>EMACallback</code><a href="https://github.com/benihime91/gale/tree/master/gale/collections/callbacks/ema.py#L20" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>EMACallback</code>(<strong><code>decay</code></strong>=<em><code>0.9999</code></em>, <strong><code>use_ema_weights</code></strong>:<code>bool</code>=<em><code>True</code></em>) :: <code>Callback</code></p>
</blockquote>
<p>Model Exponential Moving Average. Empirically it has been found that using the moving average
of the trained parameters of a deep network is better than using its trained parameters directly.</p>
<p>If <code>use_ema_weights</code>, then the ema parameters of the network is set after training end.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

