---

title: Classes


keywords: fastai
sidebar: home_sidebar

summary: "Interfaces common to all `Modules` and `Models` in Gale."
description: "Interfaces common to all `Modules` and `Models` in Gale."
nb_path: "nbs/03_core.classes.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/03_core.classes.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Configurable" class="doc_header"><code>class</code> <code>Configurable</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L28" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Configurable</code>() :: <code>ABC</code></p>
</blockquote>
<p>Helper Class to instantiate obj from config</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="08fda52c-41b2-413d-9afa-db1907cb034e"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#08fda52c-41b2-413d-9afa-db1907cb034e');

            setTimeout(function() {
                var nbb_cell_id = 5;
                var nbb_unformatted_code = "# export\nclass Configurable(ABC):\n    \"\"\"\n    Helper Class to instantiate obj from config\n    \"\"\"\n\n    @classmethod\n    def from_config_dict(cls, config: DictConfig, **kwargs):\n        \"\"\"\n        Instantiates object using `DictConfig-based` configuration. You can optionally\n        pass in extra `kwargs`\n        \"\"\"\n        # Resolve the config dict\n        if isinstance(config, DictConfig):\n            config = OmegaConf.to_container(config, resolve=True)\n            config = OmegaConf.create(config)\n\n        if \"_target_\" in config:\n            # regular hydra-based instantiation\n            instance = hydra.utils.instantiate(config=config, **kwargs)\n        else:\n            # instantiate directly using kwargs\n            try:\n                instance = cls(cfg=config, **kwargs)\n            except:\n                cfg = OmegaConf.to_container(config, resolve=True)\n                instance = cls(**config, **kwargs)\n\n        if not hasattr(instance, \"_cfg\"):\n            instance._cfg = config\n        return instance\n\n    def to_config_dict(self) -> DictConfig:\n        # fmt: off\n        \"\"\"Returns object's configuration to config dictionary\"\"\"\n        if hasattr(self, \"_cfg\") and self._cfg is not None and isinstance(self._cfg, DictConfig):\n            # Resolve the config dict\n            config = OmegaConf.to_container(self._cfg, resolve=True)\n            config = OmegaConf.create(config)\n            OmegaConf.set_struct(config, True)\n            self._cfg = config\n\n            return self._cfg\n        else:\n            raise NotImplementedError(\"to_config_dict() can currently only return object._cfg but current object does not have it.\")\n        # fmt: on";
                var nbb_formatted_code = "# export\nclass Configurable(ABC):\n    \"\"\"\n    Helper Class to instantiate obj from config\n    \"\"\"\n\n    @classmethod\n    def from_config_dict(cls, config: DictConfig, **kwargs):\n        \"\"\"\n        Instantiates object using `DictConfig-based` configuration. You can optionally\n        pass in extra `kwargs`\n        \"\"\"\n        # Resolve the config dict\n        if isinstance(config, DictConfig):\n            config = OmegaConf.to_container(config, resolve=True)\n            config = OmegaConf.create(config)\n\n        if \"_target_\" in config:\n            # regular hydra-based instantiation\n            instance = hydra.utils.instantiate(config=config, **kwargs)\n        else:\n            # instantiate directly using kwargs\n            try:\n                instance = cls(cfg=config, **kwargs)\n            except:\n                cfg = OmegaConf.to_container(config, resolve=True)\n                instance = cls(**config, **kwargs)\n\n        if not hasattr(instance, \"_cfg\"):\n            instance._cfg = config\n        return instance\n\n    def to_config_dict(self) -> DictConfig:\n        # fmt: off\n        \"\"\"Returns object's configuration to config dictionary\"\"\"\n        if hasattr(self, \"_cfg\") and self._cfg is not None and isinstance(self._cfg, DictConfig):\n            # Resolve the config dict\n            config = OmegaConf.to_container(self._cfg, resolve=True)\n            config = OmegaConf.create(config)\n            OmegaConf.set_struct(config, True)\n            self._cfg = config\n\n            return self._cfg\n        else:\n            raise NotImplementedError(\"to_config_dict() can currently only return object._cfg but current object does not have it.\")\n        # fmt: on";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This class provides a common interface for modules so that, they can be easy loaded from a Hydra Config file. This class also supports instantiating via hydra.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Configurable.from_config_dict" class="doc_header"><code>Configurable.from_config_dict</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L33" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Configurable.from_config_dict</code>(<strong><code>config</code></strong>:<code>DictConfig</code>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Instantiates object using <code>DictConfig-based</code> configuration. You can optionally
pass in extra <code>kwargs</code></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Configurable.to_config_dict" class="doc_header"><code>Configurable.to_config_dict</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L59" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Configurable.to_config_dict</code>()</p>
</blockquote>
<p>Returns object's configuration to config dictionary</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="GaleModule" class="doc_header"><code>class</code> <code>GaleModule</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L75" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>GaleModule</code>() :: <code>Module</code></p>
</blockquote>
<p>Abstract class offering interface which should be implemented by all <code>Backbones</code>,
<code>Heads</code> and <code>Meta Archs</code> in gale.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="be322927-dedd-44a2-87da-87129e803b16"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#be322927-dedd-44a2-87da-87129e803b16');

            setTimeout(function() {
                var nbb_cell_id = 8;
                var nbb_unformatted_code = "# export\nclass GaleModule(Module, Configurable, metaclass=ABCMeta):\n    \"\"\"\n    Abstract class offering interface which should be implemented by all `Backbones`,\n    `Heads` and `Meta Archs` in gale.\n    \"\"\"\n\n    @abstractmethod\n    def forward(self) -> Any:\n        \"\"\"\n        The main logic for the model lives here. Can return either features, logits\n        or loss.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def build_param_dicts(self) -> Union[Iterable, List[Dict], Dict, List]:\n        \"\"\"\n        Should return the iterable of parameters to optimize or dicts defining parameter groups\n        for the Module.\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def param_lists(self):\n        \"Returns the list of paramters in the module\"\n        return [p for p in self.parameters()]\n\n    def all_params(self, n=slice(None), with_grad=False):\n        \"List of `param_groups` upto n\"\n        res = L(p for p in self.param_lists[n])\n        # fmt: off\n        return L(o for o in res if hasattr(o, \"grad\") and o.grad is not None) if with_grad else res\n        # fmt: on\n\n    def _set_require_grad(self, rg, p):\n        p.requires_grad_(rg)\n\n    def unfreeze(self) -> None:\n        \"\"\"\n        Unfreeze all parameters for training.\n        \"\"\"\n        for param in self.parameters():\n            param.requires_grad = True\n\n        self.train()\n\n    def freeze(self) -> None:\n        \"\"\"\n        Freeze all params for inference & set model to eval\n        \"\"\"\n        for param in self.parameters():\n            param.requires_grad = False\n        self.eval()\n\n    def freeze_to(self, n) -> None:\n        \"Freeze parameter groups up to `n`\"\n        self.frozen_idx = n if n >= 0 else len(self.param_lists) + n\n        if self.frozen_idx >= len(self.param_lists):\n            # fmt: off\n            _logger.warning(f\"Freezing {self.frozen_idx} groups; model has {len(self.param_lists)}; whole model is frozen.\")\n            # fmt: on\n\n        for o in self.all_params(slice(n, None)):\n            self._set_require_grad(True, o)\n\n        for o in self.all_params(slice(None, n)):\n            self._set_require_grad(False, o)\n\n    @contextmanager\n    def as_frozen(self):\n        \"\"\"\n        Context manager which temporarily freezes a module, yields control\n        and finally unfreezes the module.\n        \"\"\"\n        self.freeze()\n\n        try:\n            yield\n        finally:\n            self.unfreeze()";
                var nbb_formatted_code = "# export\nclass GaleModule(Module, Configurable, metaclass=ABCMeta):\n    \"\"\"\n    Abstract class offering interface which should be implemented by all `Backbones`,\n    `Heads` and `Meta Archs` in gale.\n    \"\"\"\n\n    @abstractmethod\n    def forward(self) -> Any:\n        \"\"\"\n        The main logic for the model lives here. Can return either features, logits\n        or loss.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def build_param_dicts(self) -> Union[Iterable, List[Dict], Dict, List]:\n        \"\"\"\n        Should return the iterable of parameters to optimize or dicts defining parameter groups\n        for the Module.\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def param_lists(self):\n        \"Returns the list of paramters in the module\"\n        return [p for p in self.parameters()]\n\n    def all_params(self, n=slice(None), with_grad=False):\n        \"List of `param_groups` upto n\"\n        res = L(p for p in self.param_lists[n])\n        # fmt: off\n        return L(o for o in res if hasattr(o, \"grad\") and o.grad is not None) if with_grad else res\n        # fmt: on\n\n    def _set_require_grad(self, rg, p):\n        p.requires_grad_(rg)\n\n    def unfreeze(self) -> None:\n        \"\"\"\n        Unfreeze all parameters for training.\n        \"\"\"\n        for param in self.parameters():\n            param.requires_grad = True\n\n        self.train()\n\n    def freeze(self) -> None:\n        \"\"\"\n        Freeze all params for inference & set model to eval\n        \"\"\"\n        for param in self.parameters():\n            param.requires_grad = False\n        self.eval()\n\n    def freeze_to(self, n) -> None:\n        \"Freeze parameter groups up to `n`\"\n        self.frozen_idx = n if n >= 0 else len(self.param_lists) + n\n        if self.frozen_idx >= len(self.param_lists):\n            # fmt: off\n            _logger.warning(f\"Freezing {self.frozen_idx} groups; model has {len(self.param_lists)}; whole model is frozen.\")\n            # fmt: on\n\n        for o in self.all_params(slice(n, None)):\n            self._set_require_grad(True, o)\n\n        for o in self.all_params(slice(None, n)):\n            self._set_require_grad(False, o)\n\n    @contextmanager\n    def as_frozen(self):\n        \"\"\"\n        Context manager which temporarily freezes a module, yields control\n        and finally unfreezes the module.\n        \"\"\"\n        self.freeze()\n\n        try:\n            yield\n        finally:\n            self.unfreeze()";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Any Module that is Registerd in Gale should inherit from this class or its subclass.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaleModule.forward" class="doc_header"><code>GaleModule.forward</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L81" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaleModule.forward</code>()</p>
</blockquote>
<p>The main logic for the model lives here. Can return either features, logits
or loss.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaleModule.build_param_dicts" class="doc_header"><code>GaleModule.build_param_dicts</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L89" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaleModule.build_param_dicts</code>()</p>
</blockquote>
<p>Should return the iterable of parameters to optimize or dicts defining parameter groups
for the Module.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Configurable.from_config_dict" class="doc_header"><code>Configurable.from_config_dict</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L33" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Configurable.from_config_dict</code>(<strong><code>config</code></strong>:<code>DictConfig</code>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Instantiates object using <code>DictConfig-based</code> configuration. You can optionally
pass in extra <code>kwargs</code></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaleModule.param_lists" class="doc_header"><code>GaleModule.param_lists</code><a href="" class="source_link" style="float:right">[source]</a></h4><p>Returns the list of paramters in the module</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaleModule.all_params" class="doc_header"><code>GaleModule.all_params</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L102" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaleModule.all_params</code>(<strong><code>n</code></strong>=<em><code>slice(None, None, None)</code></em>, <strong><code>with_grad</code></strong>=<em><code>False</code></em>)</p>
</blockquote>
<p>List of <code>param_groups</code> upto n</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaleModule.freeze" class="doc_header"><code>GaleModule.freeze</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L121" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaleModule.freeze</code>()</p>
</blockquote>
<p>Freeze all params for inference &amp; set model to eval</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaleModule.freeze_to" class="doc_header"><code>GaleModule.freeze_to</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L129" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaleModule.freeze_to</code>(<strong><code>n</code></strong>)</p>
</blockquote>
<p>Freeze parameter groups up to <code>n</code></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaleModule.unfreeze" class="doc_header"><code>GaleModule.unfreeze</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L112" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaleModule.unfreeze</code>()</p>
</blockquote>
<p>Unfreeze all parameters for training.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaleModule.as_frozen" class="doc_header"><code>GaleModule.as_frozen</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L143" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaleModule.as_frozen</code>()</p>
</blockquote>
<p>Context manager which temporarily freezes a module, yields control
and finally unfreezes the module.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="get_callable_name" class="doc_header"><code>get_callable_name</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L157" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>get_callable_name</code>(<strong><code>fn_or_class</code></strong>:<code>object</code>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="get_callable_dict" class="doc_header"><code>get_callable_dict</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L161" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>get_callable_dict</code>(<strong><code>fn</code></strong>:<code>Union</code>[<code>Callable</code>, <code>Mapping</code>, <code>Sequence</code>])</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="setup_metrics" class="doc_header"><code>setup_metrics</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L170" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>setup_metrics</code>(<strong><code>metrics</code></strong>:<code>Union</code>[<code>Metric</code>, <code>Mapping</code>, <code>Sequence</code>, <code>NoneType</code>])</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="12cf2bda-2b75-4b4c-8768-3723a2e6537e"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#12cf2bda-2b75-4b4c-8768-3723a2e6537e');

            setTimeout(function() {
                var nbb_cell_id = 18;
                var nbb_unformatted_code = "# export\ndef get_callable_name(fn_or_class: Union[Callable, object]) -> str:\n    return getattr(fn_or_class, \"__name__\", fn_or_class.__class__.__name__).lower()\n\n\ndef get_callable_dict(fn: Union[Callable, Mapping, Sequence]) -> Union[Dict, Mapping]:\n    if isinstance(fn, Mapping):\n        return fn\n    elif isinstance(fn, Sequence):\n        return {get_callable_name(f): f for f in fn}\n    elif callable(fn):\n        return {get_callable_name(fn): fn}\n\n\ndef setup_metrics(\n    metrics: Union[torchmetrics.Metric, Mapping, Sequence, None]\n) -> torch.nn.ModuleDict:\n    m = {} if metrics is None else get_callable_dict(metrics)\n    return torch.nn.ModuleDict(m)";
                var nbb_formatted_code = "# export\ndef get_callable_name(fn_or_class: Union[Callable, object]) -> str:\n    return getattr(fn_or_class, \"__name__\", fn_or_class.__class__.__name__).lower()\n\n\ndef get_callable_dict(fn: Union[Callable, Mapping, Sequence]) -> Union[Dict, Mapping]:\n    if isinstance(fn, Mapping):\n        return fn\n    elif isinstance(fn, Sequence):\n        return {get_callable_name(f): f for f in fn}\n    elif callable(fn):\n        return {get_callable_name(fn): fn}\n\n\ndef setup_metrics(\n    metrics: Union[torchmetrics.Metric, Mapping, Sequence, None]\n) -> torch.nn.ModuleDict:\n    m = {} if metrics is None else get_callable_dict(metrics)\n    return torch.nn.ModuleDict(m)";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="GaleTask" class="doc_header"><code>class</code> <code>GaleTask</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L178" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>GaleTask</code>(<strong><code>cfg</code></strong>:<code>DictConfig</code>, <strong><code>trainer</code></strong>:<code>Optional</code>[<code>Trainer</code>]=<em><code>None</code></em>, <strong><code>metrics</code></strong>:<code>Union</code>[<code>Metric</code>, <code>Mapping</code>, <code>Sequence</code>, <code>NoneType</code>]=<em><code>None</code></em>) :: <code>LightningModule</code></p>
</blockquote>
<p>Interface for Pytorch-lightning based Gale modules</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="6b8b4248-31c5-4635-ab2d-bd03fffcb258"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#6b8b4248-31c5-4635-ab2d-bd03fffcb258');

            setTimeout(function() {
                var nbb_cell_id = 19;
                var nbb_unformatted_code = "# export\n# fmt: off\nclass GaleTask(pl.LightningModule):\n    \"\"\"\n    Interface for Pytorch-lightning based Gale modules\n    \"\"\"\n    is_restored = True\n    \n    def __init__(\n        self,\n        cfg: DictConfig,\n        trainer: Optional[pl.Trainer] = None,\n        metrics: Union[torchmetrics.Metric, Mapping, Sequence, None] = None,\n    ):\n        \"\"\"\n        Base class from which all PyTorch Lightning Tasks in Gale should inherit.\n        Provides a few helper functions primarily for optimization.\n\n        Arguments:\n        1. `cfg` `(DictConfig)`:  configuration object. cfg object should be inherited from `BaseGaleConfig`.\n        2. `trainer` `(Optional, pl.Trainer)`: Pytorch Lightning Trainer instance\n        3. `metrics` `(Optional)`: Metrics to compute for training and evaluation.\n        \"\"\"\n        super().__init__()\n        self._cfg = OmegaConf.create(cfg)\n        self._cfg = OmegaConf.structured(cfg)\n        \n        if trainer is not None and not isinstance(trainer, pl.Trainer):\n            msg = f\"Trainer constructor argument must be either None or pl.Trainer.But got {type(trainer)} instead.\"\n            raise ValueError(msg) \n        \n        self._train_dl = noop\n        self._validation_dl = noop\n        self._test_dl = noop\n        \n        self._optimizer = noop\n        self._scheduler = noop\n        \n        self._trainer = ifnone(trainer, noop)\n        self._metrics = setup_metrics(metrics)\n        self._model = noop \n        \n        self.save_hyperparameters(self._cfg)\n        \n        # if trained is not passed them the Model is being restored\n        if self._trainer is not None:\n            self.is_restored = False\n        else:\n            self.is_restored = True\n               \n        \n    @abstractmethod\n    def forward(self, x: torch.Tensor) -> Any:\n        \"\"\"\n        The Forward method for LightningModule, users should modify this method.\n        \"\"\"\n        raise NotImplementedError\n        \n        \n    def shared_step(self, batch: Any, batch_idx: int, stage: str) -> Dict:\n        \"\"\"\n        The common training/validation/test step. Override for custom behavior. This step\n        is shared between training/validation/test step. For training/validation/test steps\n        `stage` is train/val/test respectively. You training logic should go here avoid directly overriding\n        training/validation/test step methods. This step needs to return a dictionary contatining\n        the loss to optimize and values to log.\n        \"\"\"\n        raise NotImplementedError\n        \n    def training_step(self, batch: Any, batch_idx: int) -> Any:\n        \"\"\"\n        The training step of the LightningModule. For common use cases you need\n        not need to override this method. See `GaleTask.shared_step()`\n        \"\"\"\n        output = self.shared_step(batch, batch_idx, stage=\"train\")\n        self.log_dict({f\"train/{k}\": v for k, v in output[\"logs\"].items()})\n        return output[\"loss\"]\n\n    def validation_step(self, batch: Any, batch_idx: int) -> None:\n        \"\"\"\n        The validation step of the LightningModule. For common use cases you need\n        not need to override this method. See `GaleTask.shared_step()`\n        \"\"\"\n        output = self.shared_step(batch, batch_idx, stage=\"validation\")\n        self.log_dict({f\"val/{k}\": v for k, v in output[\"logs\"].items()})\n\n    def test_step(self, batch: Any, batch_idx: int) -> None:\n        \"\"\"\n        The test step of the LightningModule. For common use cases you need\n        not need to override this method. See `GaleTask.shared_step()`\n        \"\"\"\n        output = self.shared_step(batch, batch_idx, stage=\"test\")\n        self.log_dict({f\"test/{k}\": v for k, v in output[\"logs\"].items()})\n    \n    def configure_optimizers(self) -> Any:\n        \"\"\"\n        Choose what optimizers and learning-rate schedulers to use in your optimization.\n        See https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html\n        \"\"\"\n        # if self.setup_optimization() has been called manually no\n        # need to call again\n        if self._optimizer is noop and self._scheduler is noop:\n            self.setup_optimization()\n\n        if self._scheduler is None:\n            return self._optimizer\n        else:\n            return [self._optimizer], [self._scheduler]\n        \n    def train_dataloader(self) -> torch.utils.data.DataLoader:\n        \"Returns the Dataloader used for Training\"\n        if self._train_dl is not None and self._train_dl is not noop:\n            return self._train_dl\n\n    def val_dataloader(self) -> Any:\n        \"Returns the List of Dataloaders or Dataloader used for Validation\"\n        if self._validation_dl is not None and self._validation_dl is not noop:\n            return self._validation_dl\n\n    def test_dataloader(self) -> Any:\n        \"Returns the List of Dataloaders or Dataloader used for Testing\"\n        if self._test_dl is not None and self._test_dl is not noop:\n            return self._test_dl\n    \n    def process_optim_config(self, opt_conf: DictConfig) -> DictConfig:\n        \"\"\"\n        Prepares an optimizer from a string name and its optional config parameters.\n        Preprocess the optimization config and adds some infered values like max_steps, max_epochs, etc.\n        This method also fills in the values for `max_iters` & `epochs`, `steps_per_epoch` if \n        the values are `-1`\n        \"\"\"\n        # some optimizers/schedulers need parameters only known dynamically\n        # allow users to override the getter to instantiate them lazily\n        \n        opt_conf = copy.deepcopy(opt_conf)\n        \n        # Force into DictConfig structure\n        opt_conf = OmegaConf.create(opt_conf)\n        \n        if self._trainer.max_epochs is None and self._trainer.max_steps is None:\n            raise ValueError(\"Either one of max_epochs or max_epochs must be provided in Trainer\")\n        else:\n            max_steps, steps = self.num_training_steps()\n            max_epochs = ifnone(self._trainer.max_epochs, max_steps // steps)\n            \n        vals = dict(steps_per_epoch=steps, max_steps=max_steps, max_epochs=max_epochs)\n        \n        # Force into native dictionary\n        opt_conf = OmegaConf.to_container(opt_conf, resolve=True)\n        \n        for key,value in vals.items():\n            if opt_conf[key] < 1:\n                opt_conf[key] = value\n                \n        # populate values in learning rate schedulers initialization arguments\n        opt_conf = OmegaConf.create(opt_conf)\n        sched_config = OmegaConf.to_container(opt_conf.scheduler.init_args, resolve=True)\n        \n        # Force into DictConfig structure\n        opt_conf = OmegaConf.create(opt_conf)\n        \n        # @TODO: Find a better way to do this\n        if \"max_iters\" in sched_config:\n            if sched_config[\"max_iters\"] == -1:\n                OmegaConf.update(opt_conf, \"scheduler.init_args.max_iters\", max_steps)\n                msg = f\"Set the value of 'max_iters' to be {max_steps}.\"\n                log_main_process(_logger, logging.DEBUG, msg)\n\n        if \"epochs\" in sched_config:\n            if sched_config[\"epochs\"] == -1:\n                OmegaConf.update(opt_conf, \"scheduler.init_args.epochs\", max_epochs)\n                msg = f\"Set the value of 'epochs' to be {max_epochs}.\"\n                log_main_process(_logger, logging.DEBUG, msg)\n\n        if \"steps_per_epoch\" in sched_config:\n            if sched_config[\"steps_per_epoch\"] == -1:\n                OmegaConf.update(opt_conf, \"scheduler.init_args.steps_per_epoch\", steps)\n                msg = f\"Set the value of 'steps_per_epoch' to be {steps}.\"\n                log_main_process(_logger, logging.DEBUG, msg)\n                \n        if \"max_steps\" in sched_config:\n            if sched_config[\"max_steps\"] == -1:\n                OmegaConf.update(opt_conf, \"scheduler.init_args.max_steps\", max_steps)\n                msg = f\"Set the value of 'max_steps' to be {max_steps}.\"\n                log_main_process(_logger, logging.DEBUG, msg)\n                \n        return opt_conf\n    \n    def setup_optimization(self, conf: DictConfig = None):\n        \"\"\"\n        Prepares an optimizer from a string name and its optional config parameters.\n        You can also manually call this method with a valid optimization config\n        to setup the optimizers and lr_schedulers.\n        \"\"\"\n        if conf is None:\n            # See if internal config has `optimization` namespace\n            if self._cfg is not None and hasattr(self._cfg, 'optimization'):\n                conf = self._cfg.optimization\n        \n        opt_conf = conf\n        \n        # If config is still None, or internal config has no Optim, return without instantiation\n        if opt_conf is None:\n            log_main_process(_logger, logging.WARNING, \"No optimization config found,therefore no optimizer was created\")\n            self._optimizer, self._scheduler = None,None\n        \n        else:\n            opt_conf = self.process_optim_config(opt_conf)\n            self._optimizer = self.build_optimizer(opt_conf, params=self.param_dicts)\n            self._scheduler = self.build_lr_scheduler(opt_conf, optimizer=self._optimizer)\n            \n    def build_optimizer(self, opt_conf: DictConfig, params: Any) -> Any:\n        \"\"\"\n        Builds a single optimizer from `opt_conf`. `params` are the parameter\n        dict with the weights for the optimizer to optimizer.\n        \"\"\"\n        if opt_conf.optimizer.name is None:\n            msg = \"Optimizer is None, so no optimizer will be created.\"\n            log_main_process(_logger, logging.WARNING, msg)\n            return None\n        else:\n            opt = opt_conf.optimizer\n            opt = OPTIM_REGISTRY.get(opt.name)(params=params, **opt.init_args)\n            msg = \"Created optimizer: {}\".format(opt.__class__.__name__)\n            log_main_process(_logger, logging.DEBUG, msg)\n            return opt\n    \n    def build_lr_scheduler(self, opt_conf: DictConfig, optimizer: torch.optim.Optimizer) -> Any:\n        \"\"\"\n        Build the Learning Rate scheduler for current task and optimizer.\n        \"\"\"\n        # model must have a max_lrs property\n        # so that this value can be inferred to torch One Cycle Schedulers\n        max_lrs = self._model.get_lrs()\n        \n        if opt_conf.scheduler.name is None:\n            msg = \"scheduler is None, so no scheduler will be created.\"\n            log_main_process(_logger, logging.DEBUG, msg)\n            return None\n        \n        else:\n            args = opt_conf.scheduler.init_args\n            d_args = OmegaConf.to_container(args, resolve=True)\n            kwds = {}\n            \n            # if a key value is ListConfig then we convert it to simple list\n            # also dynamically compute the value of max_lrs\n            for key, value in d_args.items():\n                if isinstance(value, list):\n                    kwds[key] = list(value)     \n                elif key == \"max_lr\":\n                    kwds[\"max_lr\"] = max_lrs                \n                else:\n                    kwds[key] = value\n            instance = SCHEDULER_REGISTRY.get(opt_conf.scheduler.name)\n            sch = instance(optimizer=optimizer, **kwds)\n            \n            # convert the lr_scheduler to pytorch-lightning LRScheduler dictionary format\n            msg = \"Created lr_scheduler : {}.\".format(sch.__class__.__name__)\n            log_main_process(_logger, logging.DEBUG, msg)\n             \n            sch = {\n                \"scheduler\": sch, \n                \"interval\": opt_conf.scheduler.interval, \n                \"monitor\": opt_conf.scheduler.monitor\n            }\n            return sch        \n\n    def setup_training_data(self, *args, **kwargs) -> None:\n        \"\"\"\n        Setups data loader to be used in training\n        \"\"\"\n        pass\n\n    def setup_validation_data(self, *args, **kwargs) -> None:\n        \"\"\"\n        Setups data loader (s) to be used in validation\n        \"\"\"\n        pass\n\n    def setup_test_data(self, *args, **kwargs) -> None:\n        \"\"\"\n        (Optionally) Setups data loader to be used in test\n        \"\"\"\n        pass       \n        \n    @property\n    def _is_model_being_restored(self):\n        \"\"\"\n        Wether the model is being used for inference of training.\n        For training it is mandatory to pass in the Training while initializing\n        the class\n        \"\"\"\n        return self.is_restored\n\n    @_is_model_being_restored.setter\n    def _is_model_being_restored(self, x: bool):\n        self.is_restored = x\n        \n    @property\n    def metrics(self):\n        \"\"\"\n        Property that returns the metrics for the current Lightning Task\n        \"\"\"\n        return self._metrics\n    \n    @metrics.setter\n    def metrics(self, metrics: Union[torchmetrics.Metric, Mapping, Sequence, None] = None):\n        self._metrics = setup_metrics(metrics)\n        \n    def num_training_steps(self) -> int:\n        \"\"\"\n        Total training steps inferred from train dataloader and devices.\n        \"\"\"\n        if isinstance(self._trainer.limit_train_batches, int) and self._trainer.limit_train_batches != 0:\n            dataset_size = self._trainer.limit_train_batches\n        elif isinstance(self._trainer.limit_train_batches, float):\n            dataset_size = len(self._train_dl)\n            dataset_size = int(dataset_size * self._trainer.limit_train_batches)\n        else:\n            dataset_size = len(self._train_dl)\n        \n        num_devices = max(1, self._trainer.num_gpus, self._trainer.num_processes)\n        \n        if self._trainer.tpu_cores:\n            num_devices = max(num_devices, self._trainer.tpu_cores)\n        \n        effective_batch_size = self._trainer.accumulate_grad_batches * num_devices\n        max_estimated_steps = (dataset_size // effective_batch_size) * self._trainer.max_epochs\n        \n        if self._trainer.max_steps and self._trainer.max_steps < max_estimated_steps:\n            return self._trainer.max_steps\n        return max_estimated_steps, dataset_size\n    \n    @property\n    def param_dicts(self) -> Union[Iterator, List[Dict]]:\n        \"\"\"\n        Property that returns the param dicts for optimization.\n        Override for custom training behaviour. Currently returns all the trainable paramters.\n        \"\"\"\n        return trainable_params(self)";
                var nbb_formatted_code = "# export\n# fmt: off\nclass GaleTask(pl.LightningModule):\n    \"\"\"\n    Interface for Pytorch-lightning based Gale modules\n    \"\"\"\n    is_restored = True\n    \n    def __init__(\n        self,\n        cfg: DictConfig,\n        trainer: Optional[pl.Trainer] = None,\n        metrics: Union[torchmetrics.Metric, Mapping, Sequence, None] = None,\n    ):\n        \"\"\"\n        Base class from which all PyTorch Lightning Tasks in Gale should inherit.\n        Provides a few helper functions primarily for optimization.\n\n        Arguments:\n        1. `cfg` `(DictConfig)`:  configuration object. cfg object should be inherited from `BaseGaleConfig`.\n        2. `trainer` `(Optional, pl.Trainer)`: Pytorch Lightning Trainer instance\n        3. `metrics` `(Optional)`: Metrics to compute for training and evaluation.\n        \"\"\"\n        super().__init__()\n        self._cfg = OmegaConf.create(cfg)\n        self._cfg = OmegaConf.structured(cfg)\n        \n        if trainer is not None and not isinstance(trainer, pl.Trainer):\n            msg = f\"Trainer constructor argument must be either None or pl.Trainer.But got {type(trainer)} instead.\"\n            raise ValueError(msg) \n        \n        self._train_dl = noop\n        self._validation_dl = noop\n        self._test_dl = noop\n        \n        self._optimizer = noop\n        self._scheduler = noop\n        \n        self._trainer = ifnone(trainer, noop)\n        self._metrics = setup_metrics(metrics)\n        self._model = noop \n        \n        self.save_hyperparameters(self._cfg)\n        \n        # if trained is not passed them the Model is being restored\n        if self._trainer is not None:\n            self.is_restored = False\n        else:\n            self.is_restored = True\n               \n        \n    @abstractmethod\n    def forward(self, x: torch.Tensor) -> Any:\n        \"\"\"\n        The Forward method for LightningModule, users should modify this method.\n        \"\"\"\n        raise NotImplementedError\n        \n        \n    def shared_step(self, batch: Any, batch_idx: int, stage: str) -> Dict:\n        \"\"\"\n        The common training/validation/test step. Override for custom behavior. This step\n        is shared between training/validation/test step. For training/validation/test steps\n        `stage` is train/val/test respectively. You training logic should go here avoid directly overriding\n        training/validation/test step methods. This step needs to return a dictionary contatining\n        the loss to optimize and values to log.\n        \"\"\"\n        raise NotImplementedError\n        \n    def training_step(self, batch: Any, batch_idx: int) -> Any:\n        \"\"\"\n        The training step of the LightningModule. For common use cases you need\n        not need to override this method. See `GaleTask.shared_step()`\n        \"\"\"\n        output = self.shared_step(batch, batch_idx, stage=\"train\")\n        self.log_dict({f\"train/{k}\": v for k, v in output[\"logs\"].items()})\n        return output[\"loss\"]\n\n    def validation_step(self, batch: Any, batch_idx: int) -> None:\n        \"\"\"\n        The validation step of the LightningModule. For common use cases you need\n        not need to override this method. See `GaleTask.shared_step()`\n        \"\"\"\n        output = self.shared_step(batch, batch_idx, stage=\"validation\")\n        self.log_dict({f\"val/{k}\": v for k, v in output[\"logs\"].items()})\n\n    def test_step(self, batch: Any, batch_idx: int) -> None:\n        \"\"\"\n        The test step of the LightningModule. For common use cases you need\n        not need to override this method. See `GaleTask.shared_step()`\n        \"\"\"\n        output = self.shared_step(batch, batch_idx, stage=\"test\")\n        self.log_dict({f\"test/{k}\": v for k, v in output[\"logs\"].items()})\n    \n    def configure_optimizers(self) -> Any:\n        \"\"\"\n        Choose what optimizers and learning-rate schedulers to use in your optimization.\n        See https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html\n        \"\"\"\n        # if self.setup_optimization() has been called manually no\n        # need to call again\n        if self._optimizer is noop and self._scheduler is noop:\n            self.setup_optimization()\n\n        if self._scheduler is None:\n            return self._optimizer\n        else:\n            return [self._optimizer], [self._scheduler]\n        \n    def train_dataloader(self) -> torch.utils.data.DataLoader:\n        \"Returns the Dataloader used for Training\"\n        if self._train_dl is not None and self._train_dl is not noop:\n            return self._train_dl\n\n    def val_dataloader(self) -> Any:\n        \"Returns the List of Dataloaders or Dataloader used for Validation\"\n        if self._validation_dl is not None and self._validation_dl is not noop:\n            return self._validation_dl\n\n    def test_dataloader(self) -> Any:\n        \"Returns the List of Dataloaders or Dataloader used for Testing\"\n        if self._test_dl is not None and self._test_dl is not noop:\n            return self._test_dl\n    \n    def process_optim_config(self, opt_conf: DictConfig) -> DictConfig:\n        \"\"\"\n        Prepares an optimizer from a string name and its optional config parameters.\n        Preprocess the optimization config and adds some infered values like max_steps, max_epochs, etc.\n        This method also fills in the values for `max_iters` & `epochs`, `steps_per_epoch` if \n        the values are `-1`\n        \"\"\"\n        # some optimizers/schedulers need parameters only known dynamically\n        # allow users to override the getter to instantiate them lazily\n        \n        opt_conf = copy.deepcopy(opt_conf)\n        \n        # Force into DictConfig structure\n        opt_conf = OmegaConf.create(opt_conf)\n        \n        if self._trainer.max_epochs is None and self._trainer.max_steps is None:\n            raise ValueError(\"Either one of max_epochs or max_epochs must be provided in Trainer\")\n        else:\n            max_steps, steps = self.num_training_steps()\n            max_epochs = ifnone(self._trainer.max_epochs, max_steps // steps)\n            \n        vals = dict(steps_per_epoch=steps, max_steps=max_steps, max_epochs=max_epochs)\n        \n        # Force into native dictionary\n        opt_conf = OmegaConf.to_container(opt_conf, resolve=True)\n        \n        for key,value in vals.items():\n            if opt_conf[key] < 1:\n                opt_conf[key] = value\n                \n        # populate values in learning rate schedulers initialization arguments\n        opt_conf = OmegaConf.create(opt_conf)\n        sched_config = OmegaConf.to_container(opt_conf.scheduler.init_args, resolve=True)\n        \n        # Force into DictConfig structure\n        opt_conf = OmegaConf.create(opt_conf)\n        \n        # @TODO: Find a better way to do this\n        if \"max_iters\" in sched_config:\n            if sched_config[\"max_iters\"] == -1:\n                OmegaConf.update(opt_conf, \"scheduler.init_args.max_iters\", max_steps)\n                msg = f\"Set the value of 'max_iters' to be {max_steps}.\"\n                log_main_process(_logger, logging.DEBUG, msg)\n\n        if \"epochs\" in sched_config:\n            if sched_config[\"epochs\"] == -1:\n                OmegaConf.update(opt_conf, \"scheduler.init_args.epochs\", max_epochs)\n                msg = f\"Set the value of 'epochs' to be {max_epochs}.\"\n                log_main_process(_logger, logging.DEBUG, msg)\n\n        if \"steps_per_epoch\" in sched_config:\n            if sched_config[\"steps_per_epoch\"] == -1:\n                OmegaConf.update(opt_conf, \"scheduler.init_args.steps_per_epoch\", steps)\n                msg = f\"Set the value of 'steps_per_epoch' to be {steps}.\"\n                log_main_process(_logger, logging.DEBUG, msg)\n                \n        if \"max_steps\" in sched_config:\n            if sched_config[\"max_steps\"] == -1:\n                OmegaConf.update(opt_conf, \"scheduler.init_args.max_steps\", max_steps)\n                msg = f\"Set the value of 'max_steps' to be {max_steps}.\"\n                log_main_process(_logger, logging.DEBUG, msg)\n                \n        return opt_conf\n    \n    def setup_optimization(self, conf: DictConfig = None):\n        \"\"\"\n        Prepares an optimizer from a string name and its optional config parameters.\n        You can also manually call this method with a valid optimization config\n        to setup the optimizers and lr_schedulers.\n        \"\"\"\n        if conf is None:\n            # See if internal config has `optimization` namespace\n            if self._cfg is not None and hasattr(self._cfg, 'optimization'):\n                conf = self._cfg.optimization\n        \n        opt_conf = conf\n        \n        # If config is still None, or internal config has no Optim, return without instantiation\n        if opt_conf is None:\n            log_main_process(_logger, logging.WARNING, \"No optimization config found,therefore no optimizer was created\")\n            self._optimizer, self._scheduler = None,None\n        \n        else:\n            opt_conf = self.process_optim_config(opt_conf)\n            self._optimizer = self.build_optimizer(opt_conf, params=self.param_dicts)\n            self._scheduler = self.build_lr_scheduler(opt_conf, optimizer=self._optimizer)\n            \n    def build_optimizer(self, opt_conf: DictConfig, params: Any) -> Any:\n        \"\"\"\n        Builds a single optimizer from `opt_conf`. `params` are the parameter\n        dict with the weights for the optimizer to optimizer.\n        \"\"\"\n        if opt_conf.optimizer.name is None:\n            msg = \"Optimizer is None, so no optimizer will be created.\"\n            log_main_process(_logger, logging.WARNING, msg)\n            return None\n        else:\n            opt = opt_conf.optimizer\n            opt = OPTIM_REGISTRY.get(opt.name)(params=params, **opt.init_args)\n            msg = \"Created optimizer: {}\".format(opt.__class__.__name__)\n            log_main_process(_logger, logging.DEBUG, msg)\n            return opt\n    \n    def build_lr_scheduler(self, opt_conf: DictConfig, optimizer: torch.optim.Optimizer) -> Any:\n        \"\"\"\n        Build the Learning Rate scheduler for current task and optimizer.\n        \"\"\"\n        # model must have a max_lrs property\n        # so that this value can be inferred to torch One Cycle Schedulers\n        max_lrs = self._model.get_lrs()\n        \n        if opt_conf.scheduler.name is None:\n            msg = \"scheduler is None, so no scheduler will be created.\"\n            log_main_process(_logger, logging.DEBUG, msg)\n            return None\n        \n        else:\n            args = opt_conf.scheduler.init_args\n            d_args = OmegaConf.to_container(args, resolve=True)\n            kwds = {}\n            \n            # if a key value is ListConfig then we convert it to simple list\n            # also dynamically compute the value of max_lrs\n            for key, value in d_args.items():\n                if isinstance(value, list):\n                    kwds[key] = list(value)     \n                elif key == \"max_lr\":\n                    kwds[\"max_lr\"] = max_lrs                \n                else:\n                    kwds[key] = value\n            instance = SCHEDULER_REGISTRY.get(opt_conf.scheduler.name)\n            sch = instance(optimizer=optimizer, **kwds)\n            \n            # convert the lr_scheduler to pytorch-lightning LRScheduler dictionary format\n            msg = \"Created lr_scheduler : {}.\".format(sch.__class__.__name__)\n            log_main_process(_logger, logging.DEBUG, msg)\n             \n            sch = {\n                \"scheduler\": sch, \n                \"interval\": opt_conf.scheduler.interval, \n                \"monitor\": opt_conf.scheduler.monitor\n            }\n            return sch        \n\n    def setup_training_data(self, *args, **kwargs) -> None:\n        \"\"\"\n        Setups data loader to be used in training\n        \"\"\"\n        pass\n\n    def setup_validation_data(self, *args, **kwargs) -> None:\n        \"\"\"\n        Setups data loader (s) to be used in validation\n        \"\"\"\n        pass\n\n    def setup_test_data(self, *args, **kwargs) -> None:\n        \"\"\"\n        (Optionally) Setups data loader to be used in test\n        \"\"\"\n        pass       \n        \n    @property\n    def _is_model_being_restored(self):\n        \"\"\"\n        Wether the model is being used for inference of training.\n        For training it is mandatory to pass in the Training while initializing\n        the class\n        \"\"\"\n        return self.is_restored\n\n    @_is_model_being_restored.setter\n    def _is_model_being_restored(self, x: bool):\n        self.is_restored = x\n        \n    @property\n    def metrics(self):\n        \"\"\"\n        Property that returns the metrics for the current Lightning Task\n        \"\"\"\n        return self._metrics\n    \n    @metrics.setter\n    def metrics(self, metrics: Union[torchmetrics.Metric, Mapping, Sequence, None] = None):\n        self._metrics = setup_metrics(metrics)\n        \n    def num_training_steps(self) -> int:\n        \"\"\"\n        Total training steps inferred from train dataloader and devices.\n        \"\"\"\n        if isinstance(self._trainer.limit_train_batches, int) and self._trainer.limit_train_batches != 0:\n            dataset_size = self._trainer.limit_train_batches\n        elif isinstance(self._trainer.limit_train_batches, float):\n            dataset_size = len(self._train_dl)\n            dataset_size = int(dataset_size * self._trainer.limit_train_batches)\n        else:\n            dataset_size = len(self._train_dl)\n        \n        num_devices = max(1, self._trainer.num_gpus, self._trainer.num_processes)\n        \n        if self._trainer.tpu_cores:\n            num_devices = max(num_devices, self._trainer.tpu_cores)\n        \n        effective_batch_size = self._trainer.accumulate_grad_batches * num_devices\n        max_estimated_steps = (dataset_size // effective_batch_size) * self._trainer.max_epochs\n        \n        if self._trainer.max_steps and self._trainer.max_steps < max_estimated_steps:\n            return self._trainer.max_steps\n        return max_estimated_steps, dataset_size\n    \n    @property\n    def param_dicts(self) -> Union[Iterator, List[Dict]]:\n        \"\"\"\n        Property that returns the param dicts for optimization.\n        Override for custom training behaviour. Currently returns all the trainable paramters.\n        \"\"\"\n        return trainable_params(self)";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaleTask.metrics" class="doc_header"><code>GaleTask.metrics</code><a href="" class="source_link" style="float:right">[source]</a></h4><p>Property that returns the metrics for the current Lightning Task</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaleTask.param_dicts" class="doc_header"><code>GaleTask.param_dicts</code><a href="" class="source_link" style="float:right">[source]</a></h4><p>Property that returns the param dicts for optimization.
Override for custom training behaviour. Currently returns all the trainable paramters.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaleTask._is_model_being_restored" class="doc_header"><code>GaleTask._is_model_being_restored</code><a href="" class="source_link" style="float:right">[source]</a></h4><p>Wether the model is being used for inference of training.
For training it is mandatory to pass in the Training while initializing
the class</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaleTask.forward" class="doc_header"><code>GaleTask.forward</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L227" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaleTask.forward</code>(<strong><code>x</code></strong>:<code>Tensor</code>)</p>
</blockquote>
<p>The Forward method for LightningModule, users should modify this method.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaleTask.num_training_steps" class="doc_header"><code>GaleTask.num_training_steps</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L486" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaleTask.num_training_steps</code>()</p>
</blockquote>
<p>Total training steps inferred from train dataloader and devices.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaleTask.shared_step" class="doc_header"><code>GaleTask.shared_step</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L235" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaleTask.shared_step</code>(<strong><code>batch</code></strong>:<code>Any</code>, <strong><code>batch_idx</code></strong>:<code>int</code>, <strong><code>stage</code></strong>:<code>str</code>)</p>
</blockquote>
<p>The common training/validation/test step. Override for custom behavior. This step
is shared between training/validation/test step. For training/validation/test steps
<code>stage</code> is train/val/test respectively. You training logic should go here avoid directly overriding
training/validation/test step methods. This step needs to return a dictionary contatining
the loss to optimize and values to log.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaleTask.training_step" class="doc_header"><code>GaleTask.training_step</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L245" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaleTask.training_step</code>(<strong><code>batch</code></strong>:<code>Any</code>, <strong><code>batch_idx</code></strong>:<code>int</code>)</p>
</blockquote>
<p>The training step of the LightningModule. For common use cases you need
not need to override this method. See <a href="/gale/core.classes.html#GaleTask.shared_step("><code>GaleTask.shared_step()</code></a>)</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaleTask.validation_step" class="doc_header"><code>GaleTask.validation_step</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L254" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaleTask.validation_step</code>(<strong><code>batch</code></strong>:<code>Any</code>, <strong><code>batch_idx</code></strong>:<code>int</code>)</p>
</blockquote>
<p>The validation step of the LightningModule. For common use cases you need
not need to override this method. See <a href="/gale/core.classes.html#GaleTask.shared_step("><code>GaleTask.shared_step()</code></a>)</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaleTask.test_step" class="doc_header"><code>GaleTask.test_step</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L262" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaleTask.test_step</code>(<strong><code>batch</code></strong>:<code>Any</code>, <strong><code>batch_idx</code></strong>:<code>int</code>)</p>
</blockquote>
<p>The test step of the LightningModule. For common use cases you need
not need to override this method. See <a href="/gale/core.classes.html#GaleTask.shared_step("><code>GaleTask.shared_step()</code></a>)</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaleTask.configure_optimizers" class="doc_header"><code>GaleTask.configure_optimizers</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L270" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaleTask.configure_optimizers</code>()</p>
</blockquote>
<p>Choose what optimizers and learning-rate schedulers to use in your optimization.
See <a href="https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html">https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html</a></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaleTask.process_optim_config" class="doc_header"><code>GaleTask.process_optim_config</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L300" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaleTask.process_optim_config</code>(<strong><code>opt_conf</code></strong>:<code>DictConfig</code>)</p>
</blockquote>
<p>Prepares an optimizer from a string name and its optional config parameters.
Preprocess the optimization config and adds some infered values like max_steps, max_epochs, etc.
This method also fills in the values for <code>max_iters</code> &amp; <code>epochs</code>, <code>steps_per_epoch</code> if
the values are <code>-1</code></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaleTask.setup_optimization" class="doc_header"><code>GaleTask.setup_optimization</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L364" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaleTask.setup_optimization</code>(<strong><code>conf</code></strong>:<code>DictConfig</code>=<em><code>None</code></em>)</p>
</blockquote>
<p>Prepares an optimizer from a string name and its optional config parameters.
You can also manually call this method with a valid optimization config
to setup the optimizers and lr_schedulers.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaleTask.build_optimizer" class="doc_header"><code>GaleTask.build_optimizer</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L387" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaleTask.build_optimizer</code>(<strong><code>opt_conf</code></strong>:<code>DictConfig</code>, <strong><code>params</code></strong>:<code>Any</code>)</p>
</blockquote>
<p>Builds a single optimizer from <code>opt_conf</code>. <code>params</code> are the parameter
dict with the weights for the optimizer to optimizer.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaleTask.build_lr_scheduler" class="doc_header"><code>GaleTask.build_lr_scheduler</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L403" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaleTask.build_lr_scheduler</code>(<strong><code>opt_conf</code></strong>:<code>DictConfig</code>, <strong><code>optimizer</code></strong>:<code>Optimizer</code>)</p>
</blockquote>
<p>Build the Learning Rate scheduler for current task and optimizer.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaleTask.train_dataloader" class="doc_header"><code>GaleTask.train_dataloader</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L285" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaleTask.train_dataloader</code>()</p>
</blockquote>
<p>Returns the Dataloader used for Training</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaleTask.val_dataloader" class="doc_header"><code>GaleTask.val_dataloader</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L290" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaleTask.val_dataloader</code>()</p>
</blockquote>
<p>Returns the List of Dataloaders or Dataloader used for Validation</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaleTask.test_dataloader" class="doc_header"><code>GaleTask.test_dataloader</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L295" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaleTask.test_dataloader</code>()</p>
</blockquote>
<p>Returns the List of Dataloaders or Dataloader used for Testing</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaleTask.setup_training_data" class="doc_header"><code>GaleTask.setup_training_data</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L444" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaleTask.setup_training_data</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Setups data loader to be used in training</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaleTask.setup_validation_data" class="doc_header"><code>GaleTask.setup_validation_data</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L450" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaleTask.setup_validation_data</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Setups data loader (s) to be used in validation</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaleTask.setup_test_data" class="doc_header"><code>GaleTask.setup_test_data</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/classes.py#L456" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaleTask.setup_test_data</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>(Optionally) Setups data loader to be used in test</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

