---

title: Optimizers


keywords: fastai
sidebar: home_sidebar

summary: "Collection of usefull `Optimizers` their variants."
description: "Collection of usefull `Optimizers` their variants."
nb_path: "nbs/02_core.nn.optim.optimizers.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/02_core.nn.optim.optimizers.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="3e80bdc1-6be5-40b3-b31c-a88818dde372"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#3e80bdc1-6be5-40b3-b31c-a88818dde372');

            setTimeout(function() {
                var nbb_cell_id = 5;
                var nbb_unformatted_code = "# export\n_all_ = [\"RAdam\", \"RMSpropTF\", \"SGD\", \"Adam\", \"AdamW\", \"RMSprop\"]";
                var nbb_formatted_code = "# export\n_all_ = [\"RAdam\", \"RMSpropTF\", \"SGD\", \"Adam\", \"AdamW\", \"RMSprop\"]";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Ranger" class="doc_header"><code>Ranger</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/nn/optim/optimizers.py#L21" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Ranger</code>(<strong><code>params</code></strong>:<code>Iterable</code>, <strong><code>betas</code></strong>:<code>Tuple</code>[<code>float</code>, <code>float</code>]=<em><code>(0.95, 0.999)</code></em>, <strong><code>eps</code></strong>:<code>float</code>=<em><code>1e-05</code></em>, <strong><code>k</code></strong>:<code>int</code>=<em><code>6</code></em>, <strong><code>alpha</code></strong>:<code>float</code>=<em><code>0.5</code></em>, <strong><code>lr</code></strong>=<em><code>0.001</code></em>, <strong><code>weight_decay</code></strong>=<em><code>0</code></em>)</p>
</blockquote>
<p>Convenience method for <code>Lookahead</code> with <code>RAdam</code></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="bae8e293-10a4-4482-a23b-544663e5e9b9"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#bae8e293-10a4-4482-a23b-544663e5e9b9');

            setTimeout(function() {
                var nbb_cell_id = 6;
                var nbb_unformatted_code = "# export\n@delegates(RAdam)\ndef Ranger(\n    params: Iterable,\n    betas: Tuple[float, float] = (0.95, 0.999),\n    eps: float = 1e-5,\n    k: int = 6,\n    alpha: float = 0.5,\n    **kwargs\n):\n    \"Convenience method for `Lookahead` with `RAdam`\"\n    return Lookahead(RAdam(params, betas=betas, eps=eps, **kwargs), alpha=alpha, k=k)";
                var nbb_formatted_code = "# export\n@delegates(RAdam)\ndef Ranger(\n    params: Iterable,\n    betas: Tuple[float, float] = (0.95, 0.999),\n    eps: float = 1e-5,\n    k: int = 6,\n    alpha: float = 0.5,\n    **kwargs\n):\n    \"Convenience method for `Lookahead` with `RAdam`\"\n    return Lookahead(RAdam(params, betas=betas, eps=eps, **kwargs), alpha=alpha, k=k)";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="RangerGC" class="doc_header"><code>class</code> <code>RangerGC</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/nn/optim/optimizers.py#L34" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>RangerGC</code>(<strong><code>params</code></strong>:<code>Iterable</code>, <strong><code>lr</code></strong>:<code>float</code>=<em><code>0.001</code></em>, <strong><code>alpha</code></strong>:<code>float</code>=<em><code>0.5</code></em>, <strong><code>k</code></strong>:<code>int</code>=<em><code>6</code></em>, <strong><code>N_sma_threshhold</code></strong>:<code>int</code>=<em><code>5</code></em>, <strong><code>betas</code></strong>:<code>Tuple</code>[<code>float</code>, <code>float</code>]=<em><code>(0.95, 0.999)</code></em>, <strong><code>eps</code></strong>:<code>float</code>=<em><code>1e-05</code></em>, <strong><code>weight_decay</code></strong>:<code>Union</code>[<code>float</code>, <code>int</code>]=<em><code>0</code></em>, <strong><code>use_gc</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>gc_conv_only</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <code>Optimizer</code></p>
</blockquote>
<p>Ranger deep learning optimizer - RAdam + Lookahead + Gradient Centralization, combined into one optimizer.</p>
<p>Source - <a href="https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer/blob/master/ranger/ranger.py">https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer/blob/master/ranger/ranger.py</a></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="6a7c6231-58fe-43d1-b953-c84d5aa74be1"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#6a7c6231-58fe-43d1-b953-c84d5aa74be1');

            setTimeout(function() {
                var nbb_cell_id = 7;
                var nbb_unformatted_code = "# export\nclass RangerGC(Optimizer):\n    \"\"\"\n    Ranger deep learning optimizer - RAdam + Lookahead + Gradient Centralization, combined into one optimizer.\n\n    Source - https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer/blob/master/ranger/ranger.py\n    \"\"\"\n\n    def __init__(\n        self,\n        params: Iterable,\n        lr: float = 1e-3,\n        alpha: float = 0.5,\n        k: int = 6,\n        N_sma_threshhold: int = 5,\n        betas: Tuple[float, float] = (0.95, 0.999),\n        eps: float = 1e-5,\n        weight_decay: Union[float, int] = 0,\n        use_gc: bool = True,\n        gc_conv_only: bool = False,\n    ):\n\n        # parameter checks\n        if not 0.0 <= alpha <= 1.0:\n            raise ValueError(f\"Invalid slow update rate: {alpha}\")\n        if not 1 <= k:\n            raise ValueError(f\"Invalid lookahead steps: {k}\")\n        if not lr > 0:\n            raise ValueError(f\"Invalid Learning Rate: {lr}\")\n        if not eps > 0:\n            raise ValueError(f\"Invalid eps: {eps}\")\n\n        # prep defaults and init torch.optim base\n        defaults = dict(\n            lr=lr,\n            alpha=alpha,\n            k=k,\n            step_counter=0,\n            betas=betas,\n            N_sma_threshhold=N_sma_threshhold,\n            eps=eps,\n            weight_decay=weight_decay,\n        )\n        super().__init__(params, defaults)\n\n        # adjustable threshold\n        self.N_sma_threshhold = N_sma_threshhold\n\n        # look ahead params\n\n        self.alpha = alpha\n        self.k = k\n\n        # radam buffer for state\n        self.radam_buffer = [[None, None, None] for ind in range(10)]\n\n        # gc on or off\n        self.use_gc = use_gc\n\n        # level of gradient centralization\n        self.gc_gradient_threshold = 3 if gc_conv_only else 1\n\n    def __setstate__(self, state):\n        super(Ranger, self).__setstate__(state)\n\n    def step(self, closure=None):\n        loss = None\n\n        if closure is not None:\n            loss = closure()\n\n        # Evaluate averages and grad, update param tensors\n        for group in self.param_groups:\n\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        \"Ranger optimizer does not support sparse gradients\"\n                    )\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state[\"step\"] = 0\n                    state[\"exp_avg\"] = torch.zeros_like(p_data_fp32)\n                    state[\"exp_avg_sq\"] = torch.zeros_like(p_data_fp32)\n\n                    # look ahead weight storage now in state dict\n                    state[\"slow_buffer\"] = torch.empty_like(p.data)\n                    state[\"slow_buffer\"].copy_(p.data)\n\n                else:\n                    state[\"exp_avg\"] = state[\"exp_avg\"].type_as(p_data_fp32)\n                    state[\"exp_avg_sq\"] = state[\"exp_avg_sq\"].type_as(p_data_fp32)\n\n                # begin computations\n                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n                beta1, beta2 = group[\"betas\"]\n\n                # GC operation for Conv layers and FC layers\n                if grad.dim() > self.gc_gradient_threshold:\n                    grad.add_(-grad.mean(dim=tuple(range(1, grad.dim())), keepdim=True))\n\n                state[\"step\"] += 1\n\n                # compute variance mov avg\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                # compute mean moving avg\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                buffered = self.radam_buffer[int(state[\"step\"] % 10)]\n\n                if state[\"step\"] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state[\"step\"]\n                    beta2_t = beta2 ** state[\"step\"]\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state[\"step\"] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n                    if N_sma > self.N_sma_threshhold:\n                        step_size = math.sqrt(\n                            (1 - beta2_t)\n                            * (N_sma - 4)\n                            / (N_sma_max - 4)\n                            * (N_sma - 2)\n                            / N_sma\n                            * N_sma_max\n                            / (N_sma_max - 2)\n                        ) / (1 - beta1 ** state[\"step\"])\n                    else:\n                        step_size = 1.0 / (1 - beta1 ** state[\"step\"])\n                    buffered[2] = step_size\n\n                if group[\"weight_decay\"] != 0:\n                    p_data_fp32.add_(-group[\"weight_decay\"] * group[\"lr\"], p_data_fp32)\n\n                # apply lr\n                if N_sma > self.N_sma_threshhold:\n                    denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n                    p_data_fp32.addcdiv_(-step_size * group[\"lr\"], exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-step_size * group[\"lr\"], exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n                # integrated look ahead...\n                # we do it at the param level instead of group level\n                if state[\"step\"] % group[\"k\"] == 0:\n                    # get access to slow param tensor\n                    slow_p = state[\"slow_buffer\"]\n                    # (fast weights - slow weights) * alpha\n                    slow_p.add_(self.alpha, p.data - slow_p)\n                    # copy interpolated weights to RAdam param tensor\n                    p.data.copy_(slow_p)\n\n        return loss";
                var nbb_formatted_code = "# export\nclass RangerGC(Optimizer):\n    \"\"\"\n    Ranger deep learning optimizer - RAdam + Lookahead + Gradient Centralization, combined into one optimizer.\n\n    Source - https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer/blob/master/ranger/ranger.py\n    \"\"\"\n\n    def __init__(\n        self,\n        params: Iterable,\n        lr: float = 1e-3,\n        alpha: float = 0.5,\n        k: int = 6,\n        N_sma_threshhold: int = 5,\n        betas: Tuple[float, float] = (0.95, 0.999),\n        eps: float = 1e-5,\n        weight_decay: Union[float, int] = 0,\n        use_gc: bool = True,\n        gc_conv_only: bool = False,\n    ):\n\n        # parameter checks\n        if not 0.0 <= alpha <= 1.0:\n            raise ValueError(f\"Invalid slow update rate: {alpha}\")\n        if not 1 <= k:\n            raise ValueError(f\"Invalid lookahead steps: {k}\")\n        if not lr > 0:\n            raise ValueError(f\"Invalid Learning Rate: {lr}\")\n        if not eps > 0:\n            raise ValueError(f\"Invalid eps: {eps}\")\n\n        # prep defaults and init torch.optim base\n        defaults = dict(\n            lr=lr,\n            alpha=alpha,\n            k=k,\n            step_counter=0,\n            betas=betas,\n            N_sma_threshhold=N_sma_threshhold,\n            eps=eps,\n            weight_decay=weight_decay,\n        )\n        super().__init__(params, defaults)\n\n        # adjustable threshold\n        self.N_sma_threshhold = N_sma_threshhold\n\n        # look ahead params\n\n        self.alpha = alpha\n        self.k = k\n\n        # radam buffer for state\n        self.radam_buffer = [[None, None, None] for ind in range(10)]\n\n        # gc on or off\n        self.use_gc = use_gc\n\n        # level of gradient centralization\n        self.gc_gradient_threshold = 3 if gc_conv_only else 1\n\n    def __setstate__(self, state):\n        super(Ranger, self).__setstate__(state)\n\n    def step(self, closure=None):\n        loss = None\n\n        if closure is not None:\n            loss = closure()\n\n        # Evaluate averages and grad, update param tensors\n        for group in self.param_groups:\n\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        \"Ranger optimizer does not support sparse gradients\"\n                    )\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state[\"step\"] = 0\n                    state[\"exp_avg\"] = torch.zeros_like(p_data_fp32)\n                    state[\"exp_avg_sq\"] = torch.zeros_like(p_data_fp32)\n\n                    # look ahead weight storage now in state dict\n                    state[\"slow_buffer\"] = torch.empty_like(p.data)\n                    state[\"slow_buffer\"].copy_(p.data)\n\n                else:\n                    state[\"exp_avg\"] = state[\"exp_avg\"].type_as(p_data_fp32)\n                    state[\"exp_avg_sq\"] = state[\"exp_avg_sq\"].type_as(p_data_fp32)\n\n                # begin computations\n                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n                beta1, beta2 = group[\"betas\"]\n\n                # GC operation for Conv layers and FC layers\n                if grad.dim() > self.gc_gradient_threshold:\n                    grad.add_(-grad.mean(dim=tuple(range(1, grad.dim())), keepdim=True))\n\n                state[\"step\"] += 1\n\n                # compute variance mov avg\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                # compute mean moving avg\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                buffered = self.radam_buffer[int(state[\"step\"] % 10)]\n\n                if state[\"step\"] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state[\"step\"]\n                    beta2_t = beta2 ** state[\"step\"]\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state[\"step\"] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n                    if N_sma > self.N_sma_threshhold:\n                        step_size = math.sqrt(\n                            (1 - beta2_t)\n                            * (N_sma - 4)\n                            / (N_sma_max - 4)\n                            * (N_sma - 2)\n                            / N_sma\n                            * N_sma_max\n                            / (N_sma_max - 2)\n                        ) / (1 - beta1 ** state[\"step\"])\n                    else:\n                        step_size = 1.0 / (1 - beta1 ** state[\"step\"])\n                    buffered[2] = step_size\n\n                if group[\"weight_decay\"] != 0:\n                    p_data_fp32.add_(-group[\"weight_decay\"] * group[\"lr\"], p_data_fp32)\n\n                # apply lr\n                if N_sma > self.N_sma_threshhold:\n                    denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n                    p_data_fp32.addcdiv_(-step_size * group[\"lr\"], exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-step_size * group[\"lr\"], exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n                # integrated look ahead...\n                # we do it at the param level instead of group level\n                if state[\"step\"] % group[\"k\"] == 0:\n                    # get access to slow param tensor\n                    slow_p = state[\"slow_buffer\"]\n                    # (fast weights - slow weights) * alpha\n                    slow_p.add_(self.alpha, p.data - slow_p)\n                    # copy interpolated weights to RAdam param tensor\n                    p.data.copy_(slow_p)\n\n        return loss";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SGDP" class="doc_header"><code>class</code> <code>SGDP</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/nn/optim/optimizers.py#L198" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SGDP</code>(<strong><code>params</code></strong>:<code>Iterable</code>, <strong><code>lr</code></strong>=<em><code>&lt;required parameter&gt;</code></em>, <strong><code>momentum</code></strong>:<code>Union</code>[<code>float</code>, <code>int</code>]=<em><code>0</code></em>, <strong><code>dampening</code></strong>:<code>Union</code>[<code>float</code>, <code>int</code>]=<em><code>0</code></em>, <strong><code>weight_decay</code></strong>:<code>Union</code>[<code>float</code>, <code>int</code>]=<em><code>0</code></em>, <strong><code>nesterov</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>eps</code></strong>:<code>float</code>=<em><code>1e-08</code></em>, <strong><code>delta</code></strong>:<code>float</code>=<em><code>0.1</code></em>, <strong><code>wd_ratio</code></strong>:<code>Union</code>[<code>float</code>, <code>int</code>]=<em><code>0.1</code></em>) :: <code>Optimizer</code></p>
</blockquote>
<p>SGDP Optimizer Implementation copied from <a href="https://github.com/clovaai/AdamP/blob/master/adamp/sgdp.py">https://github.com/clovaai/AdamP/blob/master/adamp/sgdp.py</a></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="99fb9e5f-06d2-4c10-9f2f-263b53f348a4"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#99fb9e5f-06d2-4c10-9f2f-263b53f348a4');

            setTimeout(function() {
                var nbb_cell_id = 8;
                var nbb_unformatted_code = "# export\nclass SGDP(Optimizer):\n    \"SGDP Optimizer Implementation copied from https://github.com/clovaai/AdamP/blob/master/adamp/sgdp.py\"\n\n    def __init__(\n        self,\n        params: Iterable,\n        lr=required,\n        momentum: Union[float, int] = 0,\n        dampening: Union[float, int] = 0,\n        weight_decay: Union[float, int] = 0,\n        nesterov: bool = False,\n        eps: float = 1e-8,\n        delta: float = 0.1,\n        wd_ratio: Union[float, int] = 0.1,\n    ):\n\n        defaults = dict(\n            lr=lr,\n            momentum=momentum,\n            dampening=dampening,\n            weight_decay=weight_decay,\n            nesterov=nesterov,\n            eps=eps,\n            delta=delta,\n            wd_ratio=wd_ratio,\n        )\n        super(SGDP, self).__init__(params, defaults)\n\n    def _channel_view(self, x):\n        return x.view(x.size(0), -1)\n\n    def _layer_view(self, x):\n        return x.view(1, -1)\n\n    def _cosine_similarity(self, x, y, eps, view_func):\n        x = view_func(x)\n        y = view_func(y)\n\n        return F.cosine_similarity(x, y, dim=1, eps=eps).abs_()\n\n    def _projection(self, p, grad, perturb, delta, wd_ratio, eps):\n        wd = 1\n        expand_size = [-1] + [1] * (len(p.shape) - 1)\n        for view_func in [self._channel_view, self._layer_view]:\n\n            cosine_sim = self._cosine_similarity(grad, p.data, eps, view_func)\n\n            if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\n                p_n = p.data / view_func(p.data).norm(dim=1).view(expand_size).add_(eps)\n                perturb -= p_n * view_func(p_n * perturb).sum(dim=1).view(expand_size)\n                wd = wd_ratio\n\n                return perturb, wd\n\n        return perturb, wd\n\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            momentum = group[\"momentum\"]\n            dampening = group[\"dampening\"]\n            nesterov = group[\"nesterov\"]\n\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\"momentum\"] = torch.zeros_like(p.data)\n\n                # SGD\n                buf = state[\"momentum\"]\n                buf.mul_(momentum).add_(grad, alpha=1 - dampening)\n                if nesterov:\n                    d_p = grad + momentum * buf\n                else:\n                    d_p = buf\n\n                # Projection\n                wd_ratio = 1\n                if len(p.shape) > 1:\n                    d_p, wd_ratio = self._projection(\n                        p, grad, d_p, group[\"delta\"], group[\"wd_ratio\"], group[\"eps\"]\n                    )\n\n                # Weight decay\n                if group[\"weight_decay\"] > 0:\n                    p.data.mul_(\n                        1\n                        - group[\"lr\"]\n                        * group[\"weight_decay\"]\n                        * wd_ratio\n                        / (1 - momentum)\n                    )\n\n                # Step\n                p.data.add_(d_p, alpha=-group[\"lr\"])\n\n        return loss";
                var nbb_formatted_code = "# export\nclass SGDP(Optimizer):\n    \"SGDP Optimizer Implementation copied from https://github.com/clovaai/AdamP/blob/master/adamp/sgdp.py\"\n\n    def __init__(\n        self,\n        params: Iterable,\n        lr=required,\n        momentum: Union[float, int] = 0,\n        dampening: Union[float, int] = 0,\n        weight_decay: Union[float, int] = 0,\n        nesterov: bool = False,\n        eps: float = 1e-8,\n        delta: float = 0.1,\n        wd_ratio: Union[float, int] = 0.1,\n    ):\n\n        defaults = dict(\n            lr=lr,\n            momentum=momentum,\n            dampening=dampening,\n            weight_decay=weight_decay,\n            nesterov=nesterov,\n            eps=eps,\n            delta=delta,\n            wd_ratio=wd_ratio,\n        )\n        super(SGDP, self).__init__(params, defaults)\n\n    def _channel_view(self, x):\n        return x.view(x.size(0), -1)\n\n    def _layer_view(self, x):\n        return x.view(1, -1)\n\n    def _cosine_similarity(self, x, y, eps, view_func):\n        x = view_func(x)\n        y = view_func(y)\n\n        return F.cosine_similarity(x, y, dim=1, eps=eps).abs_()\n\n    def _projection(self, p, grad, perturb, delta, wd_ratio, eps):\n        wd = 1\n        expand_size = [-1] + [1] * (len(p.shape) - 1)\n        for view_func in [self._channel_view, self._layer_view]:\n\n            cosine_sim = self._cosine_similarity(grad, p.data, eps, view_func)\n\n            if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\n                p_n = p.data / view_func(p.data).norm(dim=1).view(expand_size).add_(eps)\n                perturb -= p_n * view_func(p_n * perturb).sum(dim=1).view(expand_size)\n                wd = wd_ratio\n\n                return perturb, wd\n\n        return perturb, wd\n\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            momentum = group[\"momentum\"]\n            dampening = group[\"dampening\"]\n            nesterov = group[\"nesterov\"]\n\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\"momentum\"] = torch.zeros_like(p.data)\n\n                # SGD\n                buf = state[\"momentum\"]\n                buf.mul_(momentum).add_(grad, alpha=1 - dampening)\n                if nesterov:\n                    d_p = grad + momentum * buf\n                else:\n                    d_p = buf\n\n                # Projection\n                wd_ratio = 1\n                if len(p.shape) > 1:\n                    d_p, wd_ratio = self._projection(\n                        p, grad, d_p, group[\"delta\"], group[\"wd_ratio\"], group[\"eps\"]\n                    )\n\n                # Weight decay\n                if group[\"weight_decay\"] > 0:\n                    p.data.mul_(\n                        1\n                        - group[\"lr\"]\n                        * group[\"weight_decay\"]\n                        * wd_ratio\n                        / (1 - momentum)\n                    )\n\n                # Step\n                p.data.add_(d_p, alpha=-group[\"lr\"])\n\n        return loss";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="AdamP" class="doc_header"><code>class</code> <code>AdamP</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/nn/optim/optimizers.py#L305" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>AdamP</code>(<strong><code>params</code></strong>:<code>Iterable</code>, <strong><code>lr</code></strong>:<code>Union</code>[<code>float</code>, <code>int</code>]=<em><code>0.001</code></em>, <strong><code>betas</code></strong>:<code>Tuple</code>[<code>float</code>, <code>float</code>]=<em><code>(0.9, 0.999)</code></em>, <strong><code>eps</code></strong>:<code>float</code>=<em><code>1e-08</code></em>, <strong><code>weight_decay</code></strong>:<code>Union</code>[<code>float</code>, <code>int</code>]=<em><code>0</code></em>, <strong><code>delta</code></strong>:<code>float</code>=<em><code>0.1</code></em>, <strong><code>wd_ratio</code></strong>:<code>float</code>=<em><code>0.1</code></em>, <strong><code>nesterov</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <code>Optimizer</code></p>
</blockquote>
<p>AdamP Optimizer Implementation copied from <a href="https://github.com/clovaai/AdamP/blob/master/adamp/adamp.py">https://github.com/clovaai/AdamP/blob/master/adamp/adamp.py</a></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="83e9d468-8ebf-42f8-a357-2a3f56f0a271"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#83e9d468-8ebf-42f8-a357-2a3f56f0a271');

            setTimeout(function() {
                var nbb_cell_id = 9;
                var nbb_unformatted_code = "# export\nclass AdamP(Optimizer):\n    \"AdamP Optimizer Implementation copied from https://github.com/clovaai/AdamP/blob/master/adamp/adamp.py\"\n\n    def __init__(\n        self,\n        params: Iterable,\n        lr: Union[float, int] = 1e-3,\n        betas: Tuple[float, float] = (0.9, 0.999),\n        eps: float = 1e-8,\n        weight_decay: Union[float, int] = 0,\n        delta: float = 0.1,\n        wd_ratio: float = 0.1,\n        nesterov: bool = False,\n    ):\n\n        defaults = dict(\n            lr=lr,\n            betas=betas,\n            eps=eps,\n            weight_decay=weight_decay,\n            delta=delta,\n            wd_ratio=wd_ratio,\n            nesterov=nesterov,\n        )\n\n        super(AdamP, self).__init__(params, defaults)\n\n    def _channel_view(self, x):\n        return x.view(x.size(0), -1)\n\n    def _layer_view(self, x):\n        return x.view(1, -1)\n\n    def _cosine_similarity(self, x, y, eps, view_func):\n        x = view_func(x)\n        y = view_func(y)\n\n        x_norm = x.norm(dim=1).add_(eps)\n        y_norm = y.norm(dim=1).add_(eps)\n        dot = (x * y).sum(dim=1)\n\n        return dot.abs() / x_norm / y_norm\n\n    def _projection(self, p, grad, perturb, delta, wd_ratio, eps):\n        wd = 1\n        expand_size = [-1] + [1] * (len(p.shape) - 1)\n        for view_func in [self._channel_view, self._layer_view]:\n\n            cosine_sim = self._cosine_similarity(grad, p.data, eps, view_func)\n\n            if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\n                p_n = p.data / view_func(p.data).norm(dim=1).view(expand_size).add_(eps)\n                perturb -= p_n * view_func(p_n * perturb).sum(dim=1).view(expand_size)\n                wd = wd_ratio\n\n                return perturb, wd\n\n        return perturb, wd\n\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n\n                grad = p.grad.data\n                beta1, beta2 = group[\"betas\"]\n                nesterov = group[\"nesterov\"]\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\"step\"] = 0\n                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n\n                # Adam\n                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n\n                state[\"step\"] += 1\n                bias_correction1 = 1 - beta1 ** state[\"step\"]\n                bias_correction2 = 1 - beta2 ** state[\"step\"]\n\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n\n                denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(\n                    group[\"eps\"]\n                )\n                step_size = group[\"lr\"] / bias_correction1\n\n                if nesterov:\n                    perturb = (beta1 * exp_avg + (1 - beta1) * grad) / denom\n                else:\n                    perturb = exp_avg / denom\n\n                # Projection\n                wd_ratio = 1\n                if len(p.shape) > 1:\n                    perturb, wd_ratio = self._projection(\n                        p,\n                        grad,\n                        perturb,\n                        group[\"delta\"],\n                        group[\"wd_ratio\"],\n                        group[\"eps\"],\n                    )\n\n                # Weight decay\n                if group[\"weight_decay\"] > 0:\n                    p.data.mul_(1 - group[\"lr\"] * group[\"weight_decay\"] * wd_ratio)\n\n                # Step\n                p.data.add_(-step_size, perturb)\n\n        return loss";
                var nbb_formatted_code = "# export\nclass AdamP(Optimizer):\n    \"AdamP Optimizer Implementation copied from https://github.com/clovaai/AdamP/blob/master/adamp/adamp.py\"\n\n    def __init__(\n        self,\n        params: Iterable,\n        lr: Union[float, int] = 1e-3,\n        betas: Tuple[float, float] = (0.9, 0.999),\n        eps: float = 1e-8,\n        weight_decay: Union[float, int] = 0,\n        delta: float = 0.1,\n        wd_ratio: float = 0.1,\n        nesterov: bool = False,\n    ):\n\n        defaults = dict(\n            lr=lr,\n            betas=betas,\n            eps=eps,\n            weight_decay=weight_decay,\n            delta=delta,\n            wd_ratio=wd_ratio,\n            nesterov=nesterov,\n        )\n\n        super(AdamP, self).__init__(params, defaults)\n\n    def _channel_view(self, x):\n        return x.view(x.size(0), -1)\n\n    def _layer_view(self, x):\n        return x.view(1, -1)\n\n    def _cosine_similarity(self, x, y, eps, view_func):\n        x = view_func(x)\n        y = view_func(y)\n\n        x_norm = x.norm(dim=1).add_(eps)\n        y_norm = y.norm(dim=1).add_(eps)\n        dot = (x * y).sum(dim=1)\n\n        return dot.abs() / x_norm / y_norm\n\n    def _projection(self, p, grad, perturb, delta, wd_ratio, eps):\n        wd = 1\n        expand_size = [-1] + [1] * (len(p.shape) - 1)\n        for view_func in [self._channel_view, self._layer_view]:\n\n            cosine_sim = self._cosine_similarity(grad, p.data, eps, view_func)\n\n            if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\n                p_n = p.data / view_func(p.data).norm(dim=1).view(expand_size).add_(eps)\n                perturb -= p_n * view_func(p_n * perturb).sum(dim=1).view(expand_size)\n                wd = wd_ratio\n\n                return perturb, wd\n\n        return perturb, wd\n\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n\n                grad = p.grad.data\n                beta1, beta2 = group[\"betas\"]\n                nesterov = group[\"nesterov\"]\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\"step\"] = 0\n                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n\n                # Adam\n                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n\n                state[\"step\"] += 1\n                bias_correction1 = 1 - beta1 ** state[\"step\"]\n                bias_correction2 = 1 - beta2 ** state[\"step\"]\n\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n\n                denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(\n                    group[\"eps\"]\n                )\n                step_size = group[\"lr\"] / bias_correction1\n\n                if nesterov:\n                    perturb = (beta1 * exp_avg + (1 - beta1) * grad) / denom\n                else:\n                    perturb = exp_avg / denom\n\n                # Projection\n                wd_ratio = 1\n                if len(p.shape) > 1:\n                    perturb, wd_ratio = self._projection(\n                        p,\n                        grad,\n                        perturb,\n                        group[\"delta\"],\n                        group[\"wd_ratio\"],\n                        group[\"eps\"],\n                    )\n\n                # Weight decay\n                if group[\"weight_decay\"] > 0:\n                    p.data.mul_(1 - group[\"lr\"] * group[\"weight_decay\"] * wd_ratio)\n\n                # Step\n                p.data.add_(-step_size, perturb)\n\n        return loss";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

