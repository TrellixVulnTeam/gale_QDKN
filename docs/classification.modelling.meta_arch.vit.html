---

title: Meta Architectures : Vision Transformer (ViT) 


keywords: fastai
sidebar: home_sidebar

summary: "Pretrained Vision Transformers modified for use in gale from timm"
description: "Pretrained Vision Transformers modified for use in gale from timm"
nb_path: "nbs/04b_classification.modelling.meta_arch.vit.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/04b_classification.modelling.meta_arch.vit.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="e97bb961-a672-4a09-a56f-8fd09759f789"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#e97bb961-a672-4a09-a56f-8fd09759f789');

            setTimeout(function() {
                var nbb_cell_id = 5;
                var nbb_unformatted_code = "# export\n# @TODO: Add support for Discriminative Lr's\nclass ViT(GaleModule):\n    \"\"\"\n    A interface to create a Vision Transformer from timm. For available model check :\n    https://github.com/rwightman/pytorch-image-models/timm/models/vision_transformer.py\n    \"\"\"\n\n    @use_kwargs_dict(\n        keep=True,\n        num_classes=1000,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n    )\n    def __init__(\n        self,\n        model_name: str,\n        input_shape: ShapeSpec,\n        lr: float = 1e-03,\n        wd: float = 1e-05,\n        pretrained: bool = True,\n        freeze_to: Optional[int] = None,\n        finetune: Optional[bool] = None,\n        act: Optional[str] = None,\n        reset_classifier: bool = True,\n        filter_wd: bool = True,\n        **kwargs,\n    ):\n        \"\"\"\n        Arguments:\n        1. `input_shape` (ShapeSpec): input image shape. For ViT `height=width` and check the above link for avilable model shapes.\n        2. `model_name` (str): name of the ViT model, check the above link for avilable models.\n        3. `pretrained` (bool): load weights pretrained on imagenet.\n        4. `act` (str): name of the activation layer. Must be registerd in `ACTIVATION_REGISTRY`\n        5. `num_classes` (int): num output classes.\n        6. `drop_rate` (float): dropout rate.\n        7. `attn_drop_rate` (float): attention dropout rate.\n        8. `drop_path_rate` (float): stochastic depth rate.\n        9. `reset_classifier` (bool): resets the weights of the classifier.\n        10. `freeze_to` (int): Freeze the param meter groups of the model upto n.\n        11. `finetune` (bool): Freeze all the layers and keep only the `classifier` trainable.\n        \"\"\"\n        super(ViT, self).__init__()\n        # create model from timm\n        assert input_shape.height == input_shape.width\n        in_chans = input_shape.channels\n\n        if act is not None:\n            act = ACTIVATION_REGISTRY.get(act)\n        # fmt: off\n        self.model: VisionTransformer = create_model(model_name, pretrained, in_chans=in_chans, act=act, **kwargs)\n        # fmt: on\n        assert isinstance(self.model, VisionTransformer)\n\n        if reset_classifier:\n            num_cls = kwargs.pop(\"num_classes\")\n            self.model.reset_classifier(num_cls)\n\n        if freeze_to is not None:\n            self.freeze_to(freeze_to)\n\n        if finetune:\n            if freeze_to is not None and isinstance(freeze_to, int):\n                msg = \"You have sprecified freeze_to along with finetune\"\n                _logger.warning(msg)\n            _logger.info(\"Freezing all the model parameters except for the classifier\")\n            self.freeze()\n\n            classifier = [\"head\", \"head_dist\"]\n\n            for name, module in self.model.named_children():\n                if name in classifier:\n                    for p in module.parameters():\n                        p.requires_grad_(True)\n\n        store_attr(\"wd, lr, filter_wd\")\n\n    def forward(self, batched_inputs: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Runs the batched_inputs through the created model.\n        \"\"\"\n        out = self.model(batched_inputs)\n        return out\n\n    @classmethod\n    def from_config_dict(cls, cfg: DictConfig):\n        \"\"\"\n        Instantiate the Meta Architecture from gale config\n        \"\"\"\n        # fmt: off\n        input_shape = ShapeSpec(cfg.input.channels, cfg.input.height, cfg.input.width)\n        _logger.debug(f\"Inputs: {input_shape}\")\n        instance = super().from_config_dict(cfg.model.meta_architecture.init_args, input_shape=input_shape)\n        param_count = get_human_readable_count(sum([m.numel() for m in instance.parameters()]))\n        _logger.debug('{} created, param count: {}.'.format(cfg.model.meta_architecture.init_args.model_name, param_count))\n        # fmt: on\n        return instance\n\n    def build_param_dicts(self):\n        \"\"\"\n        Builds up the Paramters dicts for optimization.\n        \"\"\"\n        if self.filter_wd:\n            param_lists = add_weight_decay(\n                self.model,\n                weight_decay=self.wd,\n                skip_list=self.model.no_weight_decay(),\n            )\n            param_lists[0][\"lr\"] = self.lr\n            param_lists[1][\"lr\"] = self.lr\n        else:\n            ps = trainable_params(self.model)\n            param_lists = dict(params=ps, lr=self.lr, wd=self.wd)\n        return param_lists\n\n    def get_lrs(self) -> List:\n        \"\"\"\n        Returns a List containining the Lrs' for\n        each parameter group. This is required to build schedulers\n        like `torch.optim.lr_scheduler.OneCycleScheduler` which needs\n        the max lrs' for all the Param Groups.\n        \"\"\"\n        lrs = []\n\n        for p in self.build_param_dicts():\n            lrs.append(p[\"lr\"])\n        return lrs";
                var nbb_formatted_code = "# export\n# @TODO: Add support for Discriminative Lr's\nclass ViT(GaleModule):\n    \"\"\"\n    A interface to create a Vision Transformer from timm. For available model check :\n    https://github.com/rwightman/pytorch-image-models/timm/models/vision_transformer.py\n    \"\"\"\n\n    @use_kwargs_dict(\n        keep=True,\n        num_classes=1000,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n    )\n    def __init__(\n        self,\n        model_name: str,\n        input_shape: ShapeSpec,\n        lr: float = 1e-03,\n        wd: float = 1e-05,\n        pretrained: bool = True,\n        freeze_to: Optional[int] = None,\n        finetune: Optional[bool] = None,\n        act: Optional[str] = None,\n        reset_classifier: bool = True,\n        filter_wd: bool = True,\n        **kwargs,\n    ):\n        \"\"\"\n        Arguments:\n        1. `input_shape` (ShapeSpec): input image shape. For ViT `height=width` and check the above link for avilable model shapes.\n        2. `model_name` (str): name of the ViT model, check the above link for avilable models.\n        3. `pretrained` (bool): load weights pretrained on imagenet.\n        4. `act` (str): name of the activation layer. Must be registerd in `ACTIVATION_REGISTRY`\n        5. `num_classes` (int): num output classes.\n        6. `drop_rate` (float): dropout rate.\n        7. `attn_drop_rate` (float): attention dropout rate.\n        8. `drop_path_rate` (float): stochastic depth rate.\n        9. `reset_classifier` (bool): resets the weights of the classifier.\n        10. `freeze_to` (int): Freeze the param meter groups of the model upto n.\n        11. `finetune` (bool): Freeze all the layers and keep only the `classifier` trainable.\n        \"\"\"\n        super(ViT, self).__init__()\n        # create model from timm\n        assert input_shape.height == input_shape.width\n        in_chans = input_shape.channels\n\n        if act is not None:\n            act = ACTIVATION_REGISTRY.get(act)\n        # fmt: off\n        self.model: VisionTransformer = create_model(model_name, pretrained, in_chans=in_chans, act=act, **kwargs)\n        # fmt: on\n        assert isinstance(self.model, VisionTransformer)\n\n        if reset_classifier:\n            num_cls = kwargs.pop(\"num_classes\")\n            self.model.reset_classifier(num_cls)\n\n        if freeze_to is not None:\n            self.freeze_to(freeze_to)\n\n        if finetune:\n            if freeze_to is not None and isinstance(freeze_to, int):\n                msg = \"You have sprecified freeze_to along with finetune\"\n                _logger.warning(msg)\n            _logger.info(\"Freezing all the model parameters except for the classifier\")\n            self.freeze()\n\n            classifier = [\"head\", \"head_dist\"]\n\n            for name, module in self.model.named_children():\n                if name in classifier:\n                    for p in module.parameters():\n                        p.requires_grad_(True)\n\n        store_attr(\"wd, lr, filter_wd\")\n\n    def forward(self, batched_inputs: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Runs the batched_inputs through the created model.\n        \"\"\"\n        out = self.model(batched_inputs)\n        return out\n\n    @classmethod\n    def from_config_dict(cls, cfg: DictConfig):\n        \"\"\"\n        Instantiate the Meta Architecture from gale config\n        \"\"\"\n        # fmt: off\n        input_shape = ShapeSpec(cfg.input.channels, cfg.input.height, cfg.input.width)\n        _logger.debug(f\"Inputs: {input_shape}\")\n        instance = super().from_config_dict(cfg.model.meta_architecture.init_args, input_shape=input_shape)\n        param_count = get_human_readable_count(sum([m.numel() for m in instance.parameters()]))\n        _logger.debug('{} created, param count: {}.'.format(cfg.model.meta_architecture.init_args.model_name, param_count))\n        # fmt: on\n        return instance\n\n    def build_param_dicts(self):\n        \"\"\"\n        Builds up the Paramters dicts for optimization.\n        \"\"\"\n        if self.filter_wd:\n            param_lists = add_weight_decay(\n                self.model,\n                weight_decay=self.wd,\n                skip_list=self.model.no_weight_decay(),\n            )\n            param_lists[0][\"lr\"] = self.lr\n            param_lists[1][\"lr\"] = self.lr\n        else:\n            ps = trainable_params(self.model)\n            param_lists = dict(params=ps, lr=self.lr, wd=self.wd)\n        return param_lists\n\n    def get_lrs(self) -> List:\n        \"\"\"\n        Returns a List containining the Lrs' for\n        each parameter group. This is required to build schedulers\n        like `torch.optim.lr_scheduler.OneCycleScheduler` which needs\n        the max lrs' for all the Param Groups.\n        \"\"\"\n        lrs = []\n\n        for p in self.build_param_dicts():\n            lrs.append(p[\"lr\"])\n        return lrs";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ViT" class="doc_header"><code>class</code> <code>ViT</code><a href="https://github.com/benihime91/gale/tree/master/gale/classification/modelling/meta_arch/vit.py#L26" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ViT</code>(<strong><code>model_name</code></strong>:<code>str</code>, <strong><code>input_shape</code></strong>:<code>ShapeSpec</code>, <strong><code>lr</code></strong>:<code>float</code>=<em><code>0.001</code></em>, <strong><code>wd</code></strong>:<code>float</code>=<em><code>1e-05</code></em>, <strong><code>pretrained</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>freeze_to</code></strong>:<code>Optional</code>[<code>int</code>]=<em><code>None</code></em>, <strong><code>finetune</code></strong>:<code>Optional</code>[<code>bool</code>]=<em><code>None</code></em>, <strong><code>act</code></strong>:<code>Optional</code>[<code>str</code>]=<em><code>None</code></em>, <strong><code>reset_classifier</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>filter_wd</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>num_classes</code></strong>=<em><code>1000</code></em>, <strong><code>drop_rate</code></strong>=<em><code>0.0</code></em>, <strong><code>attn_drop_rate</code></strong>=<em><code>0.0</code></em>, <strong><code>drop_path_rate</code></strong>=<em><code>0.0</code></em>, <strong>**<code>kwargs</code></strong>) :: <a href="/gale/core.classes.html#GaleModule"><code>GaleModule</code></a></p>
</blockquote>
<p>A interface to create a Vision Transformer from timm. For available model check :
<a href="https://github.com/rwightman/pytorch-image-models/timm/models/vision_transformer.py">https://github.com/rwightman/pytorch-image-models/timm/models/vision_transformer.py</a></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Arguments :</strong></p>
<ol>
<li><code>input_shape</code> (ShapeSpec): input image shape. For ViT <code>height=width</code> and check the above link for avilable model shapes.</li>
<li><code>model_name</code> (str): name of the ViT model, check the above link for avilable models.</li>
<li><code>pretrained</code> (bool): load weights pretrained on imagenet.</li>
<li><code>act</code> (str): name of the activation layer. Must be registerd in <a href="/gale/core.utils.structures.html#ACTIVATION_REGISTRY"><code>ACTIVATION_REGISTRY</code></a></li>
<li><code>num_classes</code> (int): num output classes.</li>
<li><code>drop_rate</code> (float): dropout rate.</li>
<li><code>attn_drop_rate</code> (float): attention dropout rate.</li>
<li><code>drop_path_rate</code> (float): stochastic depth rate.</li>
<li><code>reset_classifier</code> (bool): resets the weights of the classifier.</li>
<li><code>freeze_to</code> (int): Freeze the param meter groups of the model upto n.</li>
<li><code>finetune</code> (bool): Freeze all the layers and keep only the <code>classifier</code> trainable.</li>
</ol>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">inp</span> <span class="o">=</span> <span class="n">ShapeSpec</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">ViT</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;vit_small_patch16_224&quot;</span><span class="p">,</span>
    <span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">input_shape</span><span class="o">=</span><span class="n">inp</span><span class="p">,</span>
    <span class="n">finetune</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">reset_classifier</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="e9805686-015c-44bf-9c0e-455f1ef5122e"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#e9805686-015c-44bf-9c0e-455f1ef5122e');

            setTimeout(function() {
                var nbb_cell_id = 7;
                var nbb_unformatted_code = "inp = ShapeSpec(3, 224, 224)\n\nm = ViT(\n    model_name=\"vit_small_patch16_224\",\n    pretrained=False,\n    input_shape=inp,\n    finetune=True,\n    reset_classifier=True,\n    num_classes=10,\n)";
                var nbb_formatted_code = "inp = ShapeSpec(3, 224, 224)\n\nm = ViT(\n    model_name=\"vit_small_patch16_224\",\n    pretrained=False,\n    input_shape=inp,\n    finetune=True,\n    reset_classifier=True,\n    num_classes=10,\n)";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">channels</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">height</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">width</span><span class="p">)</span>
<span class="n">o</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="cfbb1005-61e8-4a50-8717-90677b1373ad"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#cfbb1005-61e8-4a50-8717-90677b1373ad');

            setTimeout(function() {
                var nbb_cell_id = 8;
                var nbb_unformatted_code = "i = torch.randn(2, inp.channels, inp.height, inp.width)\no = m(i)";
                var nbb_formatted_code = "i = torch.randn(2, inp.channels, inp.height, inp.width)\no = m(i)";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Similar to <a href="/gale/classification.modelling.meta_arch.common.html#GeneralizedImageClassifier"><code>GeneralizedImageClassifier</code></a> we can also instantiate <a href="/gale/classification.modelling.meta_arch.vit.html#ViT"><code>ViT</code></a> from a config. <a href="/gale/classification.modelling.meta_arch.vit.html#ViT"><code>ViT</code></a> does not require neither a <code>backbone</code> nor a <code>head</code> configuration. We just need the particular initialization arguments for the vit model defined in <code>model_name</code>.
{% include note.html content='You input shape must match the dimensions that the Vision Transformer model supports. Unlike <a href="/gale/classification.modelling.meta_arch.common.html#GeneralizedImageClassifier"><code>GeneralizedImageClassifier</code></a>, <a href="/gale/classification.modelling.meta_arch.vit.html#ViT"><code>ViT</code></a> is dependent on the shape.' %}</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">field</span>
<span class="kn">from</span> <span class="nn">omegaconf</span> <span class="kn">import</span> <span class="n">MISSING</span><span class="p">,</span> <span class="n">OmegaConf</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="8363cd2f-6c50-436b-b406-915ad29a1962"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#8363cd2f-6c50-436b-b406-915ad29a1962');

            setTimeout(function() {
                var nbb_cell_id = 9;
                var nbb_unformatted_code = "from dataclasses import dataclass, field\nfrom omegaconf import MISSING, OmegaConf";
                var nbb_formatted_code = "from dataclasses import dataclass, field\nfrom omegaconf import MISSING, OmegaConf";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">ModelConf</span><span class="p">:</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;vit_small_patch16_224&quot;</span>
    <span class="n">pretrained</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">finetune</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">reset_classifier</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span>


<span class="n">inp</span> <span class="o">=</span> <span class="n">ShapeSpec</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>

<span class="n">meta_args</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">structured</span><span class="p">(</span><span class="n">ModelConf</span><span class="p">())</span>

<span class="n">meta</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">()</span>
<span class="n">meta</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;ViT&quot;</span>
<span class="n">meta</span><span class="o">.</span><span class="n">init_args</span> <span class="o">=</span> <span class="n">meta_args</span>

<span class="n">i</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">()</span>
<span class="n">i</span><span class="o">.</span><span class="n">channels</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">channels</span>
<span class="n">i</span><span class="o">.</span><span class="n">height</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">height</span>
<span class="n">i</span><span class="o">.</span><span class="n">width</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">width</span>

<span class="n">C</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">()</span>
<span class="n">C</span><span class="o">.</span><span class="n">input</span> <span class="o">=</span> <span class="n">i</span>
<span class="n">C</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">()</span>
<span class="n">C</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">meta_architecture</span> <span class="o">=</span> <span class="n">meta</span>

<span class="c1"># print(OmegaConf.to_yaml(C, resolve=True))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="4a81b407-226f-4759-84ff-e1ad61fca84e"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#4a81b407-226f-4759-84ff-e1ad61fca84e');

            setTimeout(function() {
                var nbb_cell_id = 10;
                var nbb_unformatted_code = "@dataclass\nclass ModelConf:\n    model_name: str = \"vit_small_patch16_224\"\n    pretrained: bool = False\n    finetune: bool = True\n    reset_classifier: bool = True\n    num_classes: int = 10\n\n\ninp = ShapeSpec(3, 224, 224)\n\nmeta_args = OmegaConf.structured(ModelConf())\n\nmeta = OmegaConf.create()\nmeta.name = \"ViT\"\nmeta.init_args = meta_args\n\ni = OmegaConf.create()\ni.channels = inp.channels\ni.height = inp.height\ni.width = inp.width\n\nC = OmegaConf.create()\nC.input = i\nC.model = OmegaConf.create()\nC.model.meta_architecture = meta\n\n# print(OmegaConf.to_yaml(C, resolve=True))";
                var nbb_formatted_code = "@dataclass\nclass ModelConf:\n    model_name: str = \"vit_small_patch16_224\"\n    pretrained: bool = False\n    finetune: bool = True\n    reset_classifier: bool = True\n    num_classes: int = 10\n\n\ninp = ShapeSpec(3, 224, 224)\n\nmeta_args = OmegaConf.structured(ModelConf())\n\nmeta = OmegaConf.create()\nmeta.name = \"ViT\"\nmeta.init_args = meta_args\n\ni = OmegaConf.create()\ni.channels = inp.channels\ni.height = inp.height\ni.width = inp.width\n\nC = OmegaConf.create()\nC.input = i\nC.model = OmegaConf.create()\nC.model.meta_architecture = meta\n\n# print(OmegaConf.to_yaml(C, resolve=True))";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">ViT</span><span class="o">.</span><span class="n">from_config_dict</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>

<span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">ViT</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">VisionTransformer</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="eea9835a-8e5c-4477-882f-02b949624e3e"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#eea9835a-8e5c-4477-882f-02b949624e3e');

            setTimeout(function() {
                var nbb_cell_id = 11;
                var nbb_unformatted_code = "m = ViT.from_config_dict(C)\n\nassert isinstance(m, ViT)\nassert isinstance(m.model, VisionTransformer)";
                var nbb_formatted_code = "m = ViT.from_config_dict(C)\n\nassert isinstance(m, ViT)\nassert isinstance(m.model, VisionTransformer)";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>
<details class="description">
      <summary data-open="Hide Output" data-close="Show Output"></summary>
        <summary></summary>
        
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>ViT(
  (model): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=2304, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=2304, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=2304, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=2304, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=2304, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=2304, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=2304, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=2304, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=2304, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=2304, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=2304, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=2304, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=2304, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=2304, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=2304, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=2304, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pre_logits): Identity()
    (head): Linear(in_features=768, out_features=10, bias=True)
  )
)
</pre>
</div>
</div>

<div class="output_area">




<div id="2ede6691-9a11-4062-9d75-5e57d3cb9557"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#2ede6691-9a11-4062-9d75-5e57d3cb9557');

            setTimeout(function() {
                var nbb_cell_id = 13;
                var nbb_unformatted_code = "# collapse-output\nprint(m)";
                var nbb_formatted_code = "# collapse-output\nprint(m)";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

    </details>
</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">channels</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">height</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">width</span><span class="p">)</span>
<span class="n">o</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="b37df4f5-83e5-41f7-85f3-eda9201dc50d"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#b37df4f5-83e5-41f7-85f3-eda9201dc50d');

            setTimeout(function() {
                var nbb_cell_id = 12;
                var nbb_unformatted_code = "i = torch.randn(2, inp.channels, inp.height, inp.width)\no = m(i)";
                var nbb_formatted_code = "i = torch.randn(2, inp.channels, inp.height, inp.width)\no = m(i)";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

