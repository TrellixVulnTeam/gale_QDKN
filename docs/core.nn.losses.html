---

title: Loss Functions


keywords: fastai
sidebar: home_sidebar

summary: "Custom loss functions in `Gale`"
description: "Custom loss functions in `Gale`"
nb_path: "nbs/01a_core.nn.losses.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/01a_core.nn.losses.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">fastcore.test</span> <span class="kn">import</span> <span class="o">*</span>

<span class="kn">from</span> <span class="nn">gale.core.utils.logger</span> <span class="kn">import</span> <span class="n">setup_logger</span>

<span class="n">setup_logger</span><span class="p">()</span>
<span class="n">_logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;gale.core.nn.losses&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="c9d96eba-09b9-47f4-b279-8be0b354d41c"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#c9d96eba-09b9-47f4-b279-8be0b354d41c');

            setTimeout(function() {
                var nbb_cell_id = 5;
                var nbb_unformatted_code = "from fastcore.test import *\n\nfrom gale.core.utils.logger import setup_logger\n\nsetup_logger()\n_logger = logging.getLogger(\"gale.core.nn.losses\")";
                var nbb_formatted_code = "from fastcore.test import *\n\nfrom gale.core.utils.logger import setup_logger\n\nsetup_logger()\n_logger = logging.getLogger(\"gale.core.nn.losses\")";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="cb824106-0214-439f-b1af-b126961b9bca"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#cb824106-0214-439f-b1af-b126961b9bca');

            setTimeout(function() {
                var nbb_cell_id = 6;
                var nbb_unformatted_code = "# export\nLOSS_REGISTRY = Registry(\"Loss Registry\")\nLOSS_REGISTRY.__doc__ = \"\"\"\nRegistry of Loss Functions\n\"\"\"\n\nLOSS_REGISTRY.register(SoftTargetCrossEntropy)";
                var nbb_formatted_code = "# export\nLOSS_REGISTRY = Registry(\"Loss Registry\")\nLOSS_REGISTRY.__doc__ = \"\"\"\nRegistry of Loss Functions\n\"\"\"\n\nLOSS_REGISTRY.register(SoftTargetCrossEntropy)";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="6250aeeb-a474-4a94-a0f7-19af794b5d78"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#6250aeeb-a474-4a94-a0f7-19af794b5d78');

            setTimeout(function() {
                var nbb_cell_id = 7;
                var nbb_unformatted_code = "# export\n_all_ = [\"SoftTargetCrossEntropy\", \"LOSS_REGISTRY\"]";
                var nbb_formatted_code = "# export\n_all_ = [\"SoftTargetCrossEntropy\", \"LOSS_REGISTRY\"]";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LabelSmoothingCrossEntropy" class="doc_header"><code>class</code> <code>LabelSmoothingCrossEntropy</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/nn/losses.py#L37" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LabelSmoothingCrossEntropy</code>(<strong><code>eps</code></strong>:<code>float</code>=<em><code>0.1</code></em>, <strong><code>reduction</code></strong>:<code>str</code>=<em><code>'mean'</code></em>, <strong><code>weight</code></strong>:<code>Optional</code>[<code>Tensor</code>]=<em><code>None</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Cross Entropy Loss with Label Smoothing</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="98ea66c3-7433-416d-823e-26a4b1b8e26a"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#98ea66c3-7433-416d-823e-26a4b1b8e26a');

            setTimeout(function() {
                var nbb_cell_id = 8;
                var nbb_unformatted_code = "# export\n@LOSS_REGISTRY.register()\nclass LabelSmoothingCrossEntropy(nn.Module):\n    \"Cross Entropy Loss with Label Smoothing\"\n\n    def __init__(\n        self,\n        eps: float = 0.1,\n        reduction: str = \"mean\",\n        weight: Optional[Tensor] = None,\n    ):\n        super(LabelSmoothingCrossEntropy, self).__init__()\n        store_attr(\"eps, reduction, weight\")\n\n    def forward(self, input: Tensor, target: Tensor):\n        \"\"\"\n        Shape:\n        - Input  : $(N,C)$ where $N$ is the mini-batch size and $C$ is the total number of classes\n        - Target : $(N)$ where each value is $0 \\leq {targets}[i] \\leq C-10\u2264targets[i]\u2264C\u22121$\n        - Output: scalar. If `reduction` is `none`, then $(N, *)$ , same shape as input.\n        \"\"\"\n        c = input.size()[1]\n        log_preds = F.log_softmax(input, dim=1)\n        if self.reduction == \"sum\":\n            loss = -log_preds.sum()\n        else:\n            loss = -log_preds.sum(dim=1)\n            if self.reduction == \"mean\":\n                loss = loss.mean()\n        # fmt: off\n        loss = loss * self.eps / c + (1 - self.eps) * F.nll_loss(log_preds, target.long(), weight=self.weight, reduction=self.reduction)\n        # fmt: on\n        return loss";
                var nbb_formatted_code = "# export\n@LOSS_REGISTRY.register()\nclass LabelSmoothingCrossEntropy(nn.Module):\n    \"Cross Entropy Loss with Label Smoothing\"\n\n    def __init__(\n        self,\n        eps: float = 0.1,\n        reduction: str = \"mean\",\n        weight: Optional[Tensor] = None,\n    ):\n        super(LabelSmoothingCrossEntropy, self).__init__()\n        store_attr(\"eps, reduction, weight\")\n\n    def forward(self, input: Tensor, target: Tensor):\n        \"\"\"\n        Shape:\n        - Input  : $(N,C)$ where $N$ is the mini-batch size and $C$ is the total number of classes\n        - Target : $(N)$ where each value is $0 \\leq {targets}[i] \\leq C-10\u2264targets[i]\u2264C\u22121$\n        - Output: scalar. If `reduction` is `none`, then $(N, *)$ , same shape as input.\n        \"\"\"\n        c = input.size()[1]\n        log_preds = F.log_softmax(input, dim=1)\n        if self.reduction == \"sum\":\n            loss = -log_preds.sum()\n        else:\n            loss = -log_preds.sum(dim=1)\n            if self.reduction == \"mean\":\n                loss = loss.mean()\n        # fmt: off\n        loss = loss * self.eps / c + (1 - self.eps) * F.nll_loss(log_preds, target.long(), weight=self.weight, reduction=self.reduction)\n        # fmt: on\n        return loss";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">LabelSmoothingCrossEntropy</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="22101b53-0633-426e-9093-924d2e287aee"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#22101b53-0633-426e-9093-924d2e287aee');

            setTimeout(function() {
                var nbb_cell_id = 9;
                var nbb_unformatted_code = "criterion = LabelSmoothingCrossEntropy(reduction=\"mean\")\n\noutput = torch.randn(32, 5, requires_grad=True)\ntarget = torch.empty(32, dtype=torch.long).random_(5)\n\nloss = criterion(output, target)";
                var nbb_formatted_code = "criterion = LabelSmoothingCrossEntropy(reduction=\"mean\")\n\noutput = torch.randn(32, 5, requires_grad=True)\ntarget = torch.empty(32, dtype=torch.long).random_(5)\n\nloss = criterion(output, target)";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="BinarySigmoidFocalLoss" class="doc_header"><code>class</code> <code>BinarySigmoidFocalLoss</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/nn/losses.py#L71" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>BinarySigmoidFocalLoss</code>(<strong><code>alpha</code></strong>:<code>float</code>=<em><code>-1</code></em>, <strong><code>gamma</code></strong>:<code>float</code>=<em><code>2</code></em>, <strong><code>reduction</code></strong>:<code>str</code>=<em><code>'mean'</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Creates a criterion that computes the focal loss between binary <code>input</code> and <code>target</code>.
Focal Loss used in RetinaNet for dense detection: <a href="https://arxiv.org/abs/1708.02002">https://arxiv.org/abs/1708.02002</a>.</p>
<p>Source: <a href="https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/focal_loss.py">https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/focal_loss.py</a></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="9a88f1be-5b24-4e2a-aede-0402e664561c"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#9a88f1be-5b24-4e2a-aede-0402e664561c');

            setTimeout(function() {
                var nbb_cell_id = 10;
                var nbb_unformatted_code = "# export\n@LOSS_REGISTRY.register()\nclass BinarySigmoidFocalLoss(nn.Module):\n    \"\"\"\n    Creates a criterion that computes the focal loss between binary `input` and `target`.\n    Focal Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n\n    Source: https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/focal_loss.py\n    \"\"\"\n\n    def __init__(\n        self,\n        alpha: float = -1,\n        gamma: float = 2,\n        reduction: str = \"mean\",\n    ):\n        super(BinarySigmoidFocalLoss, self).__init__()\n        store_attr(\"alpha, gamma, reduction\")\n\n    def forward(self, input: Tensor, target: Tensor):\n        \"\"\"\n        Shape:\n        - Input: : $(N, *)$ where $*$ means, any number of additional dimensions.\n        - Target: : $(N, *)$, same shape as the input.\n        - Output: scalar. If `reduction` is 'none', then $(N, *)$ , same shape as input.\n        \"\"\"\n        loss = sigmoid_focal_loss(input, target, self.gamma, self.alpha, self.reduction)\n        return loss";
                var nbb_formatted_code = "# export\n@LOSS_REGISTRY.register()\nclass BinarySigmoidFocalLoss(nn.Module):\n    \"\"\"\n    Creates a criterion that computes the focal loss between binary `input` and `target`.\n    Focal Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n\n    Source: https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/focal_loss.py\n    \"\"\"\n\n    def __init__(\n        self,\n        alpha: float = -1,\n        gamma: float = 2,\n        reduction: str = \"mean\",\n    ):\n        super(BinarySigmoidFocalLoss, self).__init__()\n        store_attr(\"alpha, gamma, reduction\")\n\n    def forward(self, input: Tensor, target: Tensor):\n        \"\"\"\n        Shape:\n        - Input: : $(N, *)$ where $*$ means, any number of additional dimensions.\n        - Target: : $(N, *)$, same shape as the input.\n        - Output: scalar. If `reduction` is 'none', then $(N, *)$ , same shape as input.\n        \"\"\"\n        loss = sigmoid_focal_loss(input, target, self.gamma, self.alpha, self.reduction)\n        return loss";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://arxiv.org/pdf/1708.02002.pdf">Focal Loss</a> is the same as cross entropy except easy-to-classify observations are down-weighted in the loss calculation. The strength of down-weighting is proportional to the size of the gamma parameter. Put another way, the larger gamma the less the easy-to-classify observations contribute to the loss.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">BinarySigmoidFocalLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>

<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">64</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">64</span><span class="p">],</span> <span class="mf">1.5</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="50414944-2a7c-4d6d-93e3-e1743fc149cc"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#50414944-2a7c-4d6d-93e3-e1743fc149cc');

            setTimeout(function() {
                var nbb_cell_id = 11;
                var nbb_unformatted_code = "criterion = BinarySigmoidFocalLoss(reduction=\"mean\")\n\ntarget = torch.ones([10, 64], dtype=torch.float32)\noutput = torch.full([10, 64], 1.5)\n\nloss = criterion(output, target)";
                var nbb_formatted_code = "criterion = BinarySigmoidFocalLoss(reduction=\"mean\")\n\ntarget = torch.ones([10, 64], dtype=torch.float32)\noutput = torch.full([10, 64], 1.5)\n\nloss = criterion(output, target)";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="FocalLoss" class="doc_header"><code>class</code> <code>FocalLoss</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/nn/losses.py#L100" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>FocalLoss</code>(<strong><code>alpha</code></strong>:<code>float</code>=<em><code>1</code></em>, <strong><code>gamma</code></strong>:<code>float</code>=<em><code>2</code></em>, <strong><code>reduction</code></strong>:<code>str</code>=<em><code>'mean'</code></em>, <strong><code>eps</code></strong>:<code>float</code>=<em><code>1e-08</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Same as <code>nn.CrossEntropyLoss</code> but with focal paramter, <code>gamma</code>.
Focal Loss used in RetinaNet for dense detection: <a href="https://arxiv.org/abs/1708.02002">https://arxiv.org/abs/1708.02002</a>.
Focal loss is computed as follows :
${FL}(p_t)$ = $lpha(1 - p_t)^{\gamma}{log}(p_t)$</p>
<p>Source: <a href="https://kornia.readthedocs.io/en/latest/_modules/kornia/losses/focal.html">https://kornia.readthedocs.io/en/latest/_modules/kornia/losses/focal.html</a></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="659f5fa7-9898-4ddf-a56f-59c8c58722fb"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#659f5fa7-9898-4ddf-a56f-59c8c58722fb');

            setTimeout(function() {
                var nbb_cell_id = 12;
                var nbb_unformatted_code = "# export\n@LOSS_REGISTRY.register()\nclass FocalLoss(nn.Module):\n    \"\"\"\n    Same as `nn.CrossEntropyLoss` but with focal paramter, `gamma`.\n    Focal Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n    Focal loss is computed as follows :\n    ${FL}(p_t)$ = $\\alpha(1 - p_t)^{\\gamma}{log}(p_t)$\n\n    Source: https://kornia.readthedocs.io/en/latest/_modules/kornia/losses/focal.html\n    \"\"\"\n\n    def __init__(\n        self,\n        alpha: float = 1,\n        gamma: float = 2,\n        reduction: str = \"mean\",\n        eps: float = 1e-8,\n    ):\n\n        super(FocalLoss, self).__init__()\n        store_attr(\"alpha, gamma, reduction, eps\")\n\n    def forward(self, input: Tensor, target: Tensor):\n        \"\"\"\n        Shape:\n        - Input  : $(N,C)$ where $N$ is the mini-batch size and $C$ is the total number of classes\n        - Target : $(N)$ where each value is $0 \\leq {targets}[i] \\leq C-10\u2264targets[i]\u2264C\u22121$\n        \"\"\"\n        if not len(input.shape) >= 2:\n            raise ValueError(\n                \"Invalid input shape, we expect BxCx*. Got: {}\".format(input.shape)\n            )\n\n        if input.size(0) != target.size(0):\n            raise ValueError(\n                \"Expected input batch_size ({}) to match target batch_size ({}).\".format(\n                    input.size(0), target.size(0)\n                )\n            )\n\n        n = input.size(0)\n\n        # compute softmax over the classes axis\n        softmax_inputs: Tensor = F.softmax(input, dim=1) + self.eps\n\n        # create the labels one hot tensor\n        one_hot_targs: Tensor = maybe_convert_to_onehot(target, softmax_inputs)\n\n        # compute the actual focal loss\n        focal_weight = torch.pow(-softmax_inputs + 1.0, self.gamma)\n\n        focal_factor = -self.alpha * focal_weight * torch.log(softmax_inputs)\n\n        loss = torch.sum(one_hot_targs * focal_factor, dim=1)\n\n        if self.reduction == \"none\":\n            loss = loss\n        elif self.reduction == \"mean\":\n            loss = torch.mean(loss)\n        elif self.reduction == \"sum\":\n            loss = torch.sum(loss)\n        else:\n            raise NotImplementedError(\n                \"Invalid reduction mode: {}\".format(self.reduction)\n            )\n        return loss";
                var nbb_formatted_code = "# export\n@LOSS_REGISTRY.register()\nclass FocalLoss(nn.Module):\n    \"\"\"\n    Same as `nn.CrossEntropyLoss` but with focal paramter, `gamma`.\n    Focal Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n    Focal loss is computed as follows :\n    ${FL}(p_t)$ = $\\alpha(1 - p_t)^{\\gamma}{log}(p_t)$\n\n    Source: https://kornia.readthedocs.io/en/latest/_modules/kornia/losses/focal.html\n    \"\"\"\n\n    def __init__(\n        self,\n        alpha: float = 1,\n        gamma: float = 2,\n        reduction: str = \"mean\",\n        eps: float = 1e-8,\n    ):\n\n        super(FocalLoss, self).__init__()\n        store_attr(\"alpha, gamma, reduction, eps\")\n\n    def forward(self, input: Tensor, target: Tensor):\n        \"\"\"\n        Shape:\n        - Input  : $(N,C)$ where $N$ is the mini-batch size and $C$ is the total number of classes\n        - Target : $(N)$ where each value is $0 \\leq {targets}[i] \\leq C-10\u2264targets[i]\u2264C\u22121$\n        \"\"\"\n        if not len(input.shape) >= 2:\n            raise ValueError(\n                \"Invalid input shape, we expect BxCx*. Got: {}\".format(input.shape)\n            )\n\n        if input.size(0) != target.size(0):\n            raise ValueError(\n                \"Expected input batch_size ({}) to match target batch_size ({}).\".format(\n                    input.size(0), target.size(0)\n                )\n            )\n\n        n = input.size(0)\n\n        # compute softmax over the classes axis\n        softmax_inputs: Tensor = F.softmax(input, dim=1) + self.eps\n\n        # create the labels one hot tensor\n        one_hot_targs: Tensor = maybe_convert_to_onehot(target, softmax_inputs)\n\n        # compute the actual focal loss\n        focal_weight = torch.pow(-softmax_inputs + 1.0, self.gamma)\n\n        focal_factor = -self.alpha * focal_weight * torch.log(softmax_inputs)\n\n        loss = torch.sum(one_hot_targs * focal_factor, dim=1)\n\n        if self.reduction == \"none\":\n            loss = loss\n        elif self.reduction == \"mean\":\n            loss = torch.mean(loss)\n        elif self.reduction == \"sum\":\n            loss = torch.sum(loss)\n        else:\n            raise NotImplementedError(\n                \"Invalid reduction mode: {}\".format(self.reduction)\n            )\n        return loss";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Arguments to <a href="/gale/core.nn.losses.html#FocalLoss"><code>FocalLoss</code></a>:</p>
<ul>
<li><code>alpha</code> (float): Weighting factor $\alpha$ in <code>[0, 1]</code>.</li>
<li><code>gamma</code> (float, optional): Focusing parameter $\gamma$ &gt;= 0. Default 2.</li>
<li><code>reduction</code> (str, optional): Specifies the reduction to apply to the</li>
<li><code>output</code>: <code>none</code> | <code>mean</code> | <code>sum</code>. <ul>
<li><code>none</code>: no reduction will be applied,</li>
<li><code>mean</code>: the sum of the output will be divided by the number of elements in the output </li>
<li><code>sum</code>: the output will be summed. </li>
<li>Default: <code>none</code>.</li>
</ul>
</li>
<li><code>eps</code> (float, optional): Scalar to enforce numerical stabiliy. Default: 1e-8.</li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">FocalLoss</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">5</span>  <span class="c1"># num_classes</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>


<span class="c1"># Compare focal loss with gamma = 0 ,cross entropy</span>
<span class="n">fl</span> <span class="o">=</span> <span class="n">FocalLoss</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>
<span class="n">ce</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">test_close</span><span class="p">(</span><span class="n">fl</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">),</span> <span class="n">ce</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">))</span>

<span class="c1"># Test focal loss with gamma &gt; 0 is different than cross entropy</span>
<span class="n">fl</span> <span class="o">=</span> <span class="n">FocalLoss</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">test_ne</span><span class="p">(</span><span class="n">fl</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">),</span> <span class="n">ce</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="f71e4761-2573-4748-b7ac-bb83eb333276"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#f71e4761-2573-4748-b7ac-bb83eb333276');

            setTimeout(function() {
                var nbb_cell_id = 13;
                var nbb_unformatted_code = "criterion = FocalLoss(alpha=0.5, gamma=2.0, reduction=\"mean\")\n\nN = 5  # num_classes\ninput = torch.randn(32, N, requires_grad=True)\ntarget = torch.empty(32, dtype=torch.long).random_(N)\nloss = criterion(input, target)\n\n\n# Compare focal loss with gamma = 0 ,cross entropy\nfl = FocalLoss(alpha=1, gamma=0, reduction=\"mean\")\nce = nn.CrossEntropyLoss(reduction=\"mean\")\noutput = torch.randn(32, N, requires_grad=True)\ntarget = torch.empty(32, dtype=torch.long).random_(N)\ntest_close(fl(output, target), ce(output, target))\n\n# Test focal loss with gamma > 0 is different than cross entropy\nfl = FocalLoss(gamma=2)\nwith torch.no_grad():\n    test_ne(fl(output, target), ce(output, target))";
                var nbb_formatted_code = "criterion = FocalLoss(alpha=0.5, gamma=2.0, reduction=\"mean\")\n\nN = 5  # num_classes\ninput = torch.randn(32, N, requires_grad=True)\ntarget = torch.empty(32, dtype=torch.long).random_(N)\nloss = criterion(input, target)\n\n\n# Compare focal loss with gamma = 0 ,cross entropy\nfl = FocalLoss(alpha=1, gamma=0, reduction=\"mean\")\nce = nn.CrossEntropyLoss(reduction=\"mean\")\noutput = torch.randn(32, N, requires_grad=True)\ntarget = torch.empty(32, dtype=torch.long).random_(N)\ntest_close(fl(output, target), ce(output, target))\n\n# Test focal loss with gamma > 0 is different than cross entropy\nfl = FocalLoss(gamma=2)\nwith torch.no_grad():\n    test_ne(fl(output, target), ce(output, target))";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To load a loss via gale config the loss must be present in either <a href="/gale/core.nn.losses.html#LOSS_REGISTRY"><code>LOSS_REGISTRY</code></a> or losses available in the <code>torch.nn.modules.loss</code><a href="https://pytorch.org/docs/stable/nn.html#loss-functions">_module</a></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="build_loss" class="doc_header"><code>build_loss</code><a href="https://github.com/benihime91/gale/tree/master/gale/core/nn/losses.py#L167" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>build_loss</code>(<strong><code>config</code></strong>:<code>DictConfig</code>)</p>
</blockquote>
<p>Builds a ClassyLoss from a config.
This assumes a 'name' key in the config which is used to determine what
model class to instantiate. For instance, a config <code>{"name": "my_loss",
"foo": "bar"}</code> will find a class that was registered as "my_loss". A custom
loss must first be registerd into <a href="/gale/core.nn.losses.html#LOSS_REGISTRY"><code>LOSS_REGISTRY</code></a>.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="5144993d-77bb-43f4-b677-f0b497fa3ae6"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#5144993d-77bb-43f4-b677-f0b497fa3ae6');

            setTimeout(function() {
                var nbb_cell_id = 14;
                var nbb_unformatted_code = "# export\ndef build_loss(config: DictConfig):\n    \"\"\"\n    Builds a ClassyLoss from a config.\n    This assumes a 'name' key in the config which is used to determine what\n    model class to instantiate. For instance, a config `{\"name\": \"my_loss\",\n    \"foo\": \"bar\"}` will find a class that was registered as \"my_loss\". A custom\n    loss must first be registerd into `LOSS_REGISTRY`.\n    \"\"\"\n\n    assert \"name\" in config, f\"name not provided for loss: {config}\"\n    config = OmegaConf.to_container(config, resolve=True)\n\n    name = config[\"name\"]\n    args = config[\"init_args\"]\n\n    # if we are passing weights, we need to change the weights from a list to a tensor\n    if args is not None:\n        if \"weight\" in args and args[\"weight\"] is not None:\n            args[\"weight\"] = torch.tensor(args[\"weight\"], dtype=torch.float)\n\n    if name in LOSS_REGISTRY:\n        instance = LOSS_REGISTRY.get(name)\n\n    # the name should be available in torch.nn.modules.loss\n    else:\n        assert hasattr(torch_losses, name), (\n            f\"{name} isn't a registered loss\"\n            \", nor is it available in torch.nn.modules.loss\"\n        )\n        instance = getattr(torch_losses, name)\n\n    if args is not None:\n        loss = instance(**args)\n    else:\n        loss = instance()\n    _logger.debug(\"Built loss function: {}\".format(loss.__class__.__name__))\n    return loss";
                var nbb_formatted_code = "# export\ndef build_loss(config: DictConfig):\n    \"\"\"\n    Builds a ClassyLoss from a config.\n    This assumes a 'name' key in the config which is used to determine what\n    model class to instantiate. For instance, a config `{\"name\": \"my_loss\",\n    \"foo\": \"bar\"}` will find a class that was registered as \"my_loss\". A custom\n    loss must first be registerd into `LOSS_REGISTRY`.\n    \"\"\"\n\n    assert \"name\" in config, f\"name not provided for loss: {config}\"\n    config = OmegaConf.to_container(config, resolve=True)\n\n    name = config[\"name\"]\n    args = config[\"init_args\"]\n\n    # if we are passing weights, we need to change the weights from a list to a tensor\n    if args is not None:\n        if \"weight\" in args and args[\"weight\"] is not None:\n            args[\"weight\"] = torch.tensor(args[\"weight\"], dtype=torch.float)\n\n    if name in LOSS_REGISTRY:\n        instance = LOSS_REGISTRY.get(name)\n\n    # the name should be available in torch.nn.modules.loss\n    else:\n        assert hasattr(torch_losses, name), (\n            f\"{name} isn't a registered loss\"\n            \", nor is it available in torch.nn.modules.loss\"\n        )\n        instance = getattr(torch_losses, name)\n\n    if args is not None:\n        loss = instance(**args)\n    else:\n        loss = instance()\n    _logger.debug(\"Built loss function: {}\".format(loss.__class__.__name__))\n    return loss";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">gale.config</span> <span class="kn">import</span> <span class="n">get_config</span>

<span class="n">cfg</span> <span class="o">=</span> <span class="n">get_config</span><span class="p">(</span>
    <span class="n">config_name</span><span class="o">=</span><span class="s2">&quot;classification&quot;</span><span class="p">,</span>
    <span class="n">overrides</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;loss@training.train_loss_fn=classification/ce_loss&quot;</span><span class="p">],</span>
<span class="p">)</span>
<span class="n">loss_cfg</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">training</span><span class="o">.</span><span class="n">train_loss_fn</span>
<span class="nb">print</span><span class="p">(</span><span class="n">OmegaConf</span><span class="o">.</span><span class="n">to_yaml</span><span class="p">(</span><span class="n">loss_cfg</span><span class="p">))</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">build_loss</span><span class="p">(</span><span class="n">loss_cfg</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>name: CrossEntropyLoss
init_args:
  weight: null
  size_average: null
  ignore_index: -100
  reduce: None
  reduction: mean

<span class="ansi-green-fg">[04/26 17:52:55 gale.core.nn.losses]: </span>Built loss function: CrossEntropyLoss
</pre>
</div>
</div>

<div class="output_area">




<div id="59db5699-917c-425c-b7ff-0543a5570fd8"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#59db5699-917c-425c-b7ff-0543a5570fd8');

            setTimeout(function() {
                var nbb_cell_id = 15;
                var nbb_unformatted_code = "from gale.config import get_config\n\ncfg = get_config(\n    config_name=\"classification\",\n    overrides=[\"loss@training.train_loss_fn=classification/ce_loss\"],\n)\nloss_cfg = cfg.training.train_loss_fn\nprint(OmegaConf.to_yaml(loss_cfg))\n\nloss = build_loss(loss_cfg)";
                var nbb_formatted_code = "from gale.config import get_config\n\ncfg = get_config(\n    config_name=\"classification\",\n    overrides=[\"loss@training.train_loss_fn=classification/ce_loss\"],\n)\nloss_cfg = cfg.training.train_loss_fn\nprint(OmegaConf.to_yaml(loss_cfg))\n\nloss = build_loss(loss_cfg)";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

