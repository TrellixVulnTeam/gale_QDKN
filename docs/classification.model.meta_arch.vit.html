---

title: Meta Architectures : Vision Transformer (ViT) 


keywords: fastai
sidebar: home_sidebar

summary: "Pretrained Vision Transformers modified for use in gale from timm"
description: "Pretrained Vision Transformers modified for use in gale from timm"
nb_path: "nbs/04b_classification.model.meta_arch.vit.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/04b_classification.model.meta_arch.vit.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="5b7f5e05-98ca-4728-8df5-d324cd523f62"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#5b7f5e05-98ca-4728-8df5-d324cd523f62');

            setTimeout(function() {
                var nbb_cell_id = 17;
                var nbb_unformatted_code = "# export\n# @TODO: Add support for Discriminative Lr's\nclass VisionTransformer(BasicModule):\n    _hypers = namedtuple(\"hypers\", field_names=[\"lr\", \"wd\"])\n    \"\"\"\n    A interface to create a Vision Transformer from timm. For available model check :\n    https://github.com/rwightman/pytorch-image-models/timm/models/vision_transformer.py\n    \"\"\"\n\n    @use_kwargs_dict(\n        keep=True,\n        num_classes=1000,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n    )\n    def __init__(\n        self,\n        model_name: str,\n        input_shape: ShapeSpec,\n        lr: float = 1e-03,\n        wd: float = 1e-05,\n        pretrained: bool = True,\n        freeze_to: Optional[int] = None,\n        finetune: Optional[bool] = None,\n        act: Optional[str] = None,\n        reset_classifier: bool = True,\n        filter_wd: bool = True,\n        **kwargs,\n    ):\n        \"\"\"\n        Arguments:\n        1. `input_shape` (ShapeSpec): input image shape. For ViT `height=width` and check the above link for avilable model shapes.\n        2. `model_name` (str): name of the ViT model, check the above link for avilable models.\n        3. `pretrained` (bool): load weights pretrained on imagenet.\n        4. `act` (str): name of the activation layer. Must be registerd in `ACTIVATION_REGISTRY`\n        5. `num_classes` (int): num output classes.\n        6. `drop_rate` (float): dropout rate.\n        7. `attn_drop_rate` (float): attention dropout rate.\n        8. `drop_path_rate` (float): stochastic depth rate.\n        9. `reset_classifier` (bool): resets the weights of the classifier.\n        10. `freeze_to` (int): Freeze the param meter groups of the model upto n.\n        11. `finetune` (bool): Freeze all the layers and keep only the `classifier` trainable.\n        \"\"\"\n        super(VisionTransformer, self).__init__()\n        # create model from timm\n        assert input_shape.height == input_shape.width\n        in_chans = input_shape.channels\n\n        if act is not None:\n            act = ACTIVATION_REGISTRY.get(act)\n\n        self.model = timm.create_model(\n            model_name, pretrained, in_chans=in_chans, act=act, **kwargs\n        )\n\n        if reset_classifier:\n            num_cls = kwargs.pop(\"num_classes\")\n            self.model.reset_classifier(num_cls)\n\n        if freeze_to is not None:\n            self.freeze_to(freeze_to)\n\n        if finetune:\n            if freeze_to is not None and isinstance(freeze_to, int):\n                msg = \"You have sprecified freeze_to along with finetune\"\n                _logger.warning(msg)\n            _logger.info(\"Freezing all the model parameters except for the classifier\")\n            self.freeze()\n\n            classifier = [\"head\", \"head_dist\"]\n\n            for name, module in self.model.named_children():\n                if name in classifier:\n                    for p in module.parameters():\n                        p.requires_grad_(True)\n\n        store_attr(\"wd, lr, filter_wd, input_shape\")\n\n    def forward(self, batched_inputs: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Runs the batched_inputs through the created model.\n        \"\"\"\n        out = self.model(batched_inputs)\n        return out\n\n    @classmethod\n    def from_config_dict(cls, cfg: DictConfig):\n        \"\"\"\n        Instantiate the Meta Architecture from gale config\n        \"\"\"\n        # fmt: off\n        input_shape = ShapeSpec(cfg.input.channels, cfg.input.height, cfg.input.width)\n        _logger.debug(f\"Inputs: {input_shape}\")\n        instance = super().from_config_dict(cfg.model.meta_architecture.init_args, input_shape=input_shape)\n        param_count = get_human_readable_count(sum([m.numel() for m in instance.parameters()]))\n        _logger.debug('{} created, param count: {}.'.format(cfg.model.meta_architecture.init_args.model_name, param_count))\n        # fmt: on\n        return instance\n\n    def build_param_dicts(self):\n        \"\"\"\n        Builds up the Paramters dicts for optimization.\n        \"\"\"\n        if self.filter_wd:\n            param_lists = add_weight_decay(\n                self.model,\n                weight_decay=self.wd,\n                skip_list=self.model.no_weight_decay(),\n            )\n            param_lists[0][\"lr\"] = self.lr\n            param_lists[1][\"lr\"] = self.lr\n        else:\n            ps = trainable_params(self.model)\n            param_lists = dict(params=ps, lr=self.lr, wd=self.wd)\n        return param_lists\n\n    @property\n    def hypers(self) -> Tuple:\n        \"\"\"\n        Returns list of parameters like `lr` and `wd`\n        for each param group\n        \"\"\"\n        lrs = []\n        wds = []\n\n        for p in self.build_param_dicts():\n            lrs.append(p[\"lr\"])\n            wds.append(p[\"weight_decay\"])\n        return self._hypers(lrs, wds)";
                var nbb_formatted_code = "# export\n# @TODO: Add support for Discriminative Lr's\nclass VisionTransformer(BasicModule):\n    _hypers = namedtuple(\"hypers\", field_names=[\"lr\", \"wd\"])\n    \"\"\"\n    A interface to create a Vision Transformer from timm. For available model check :\n    https://github.com/rwightman/pytorch-image-models/timm/models/vision_transformer.py\n    \"\"\"\n\n    @use_kwargs_dict(\n        keep=True,\n        num_classes=1000,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n    )\n    def __init__(\n        self,\n        model_name: str,\n        input_shape: ShapeSpec,\n        lr: float = 1e-03,\n        wd: float = 1e-05,\n        pretrained: bool = True,\n        freeze_to: Optional[int] = None,\n        finetune: Optional[bool] = None,\n        act: Optional[str] = None,\n        reset_classifier: bool = True,\n        filter_wd: bool = True,\n        **kwargs,\n    ):\n        \"\"\"\n        Arguments:\n        1. `input_shape` (ShapeSpec): input image shape. For ViT `height=width` and check the above link for avilable model shapes.\n        2. `model_name` (str): name of the ViT model, check the above link for avilable models.\n        3. `pretrained` (bool): load weights pretrained on imagenet.\n        4. `act` (str): name of the activation layer. Must be registerd in `ACTIVATION_REGISTRY`\n        5. `num_classes` (int): num output classes.\n        6. `drop_rate` (float): dropout rate.\n        7. `attn_drop_rate` (float): attention dropout rate.\n        8. `drop_path_rate` (float): stochastic depth rate.\n        9. `reset_classifier` (bool): resets the weights of the classifier.\n        10. `freeze_to` (int): Freeze the param meter groups of the model upto n.\n        11. `finetune` (bool): Freeze all the layers and keep only the `classifier` trainable.\n        \"\"\"\n        super(VisionTransformer, self).__init__()\n        # create model from timm\n        assert input_shape.height == input_shape.width\n        in_chans = input_shape.channels\n\n        if act is not None:\n            act = ACTIVATION_REGISTRY.get(act)\n\n        self.model = timm.create_model(\n            model_name, pretrained, in_chans=in_chans, act=act, **kwargs\n        )\n\n        if reset_classifier:\n            num_cls = kwargs.pop(\"num_classes\")\n            self.model.reset_classifier(num_cls)\n\n        if freeze_to is not None:\n            self.freeze_to(freeze_to)\n\n        if finetune:\n            if freeze_to is not None and isinstance(freeze_to, int):\n                msg = \"You have sprecified freeze_to along with finetune\"\n                _logger.warning(msg)\n            _logger.info(\"Freezing all the model parameters except for the classifier\")\n            self.freeze()\n\n            classifier = [\"head\", \"head_dist\"]\n\n            for name, module in self.model.named_children():\n                if name in classifier:\n                    for p in module.parameters():\n                        p.requires_grad_(True)\n\n        store_attr(\"wd, lr, filter_wd, input_shape\")\n\n    def forward(self, batched_inputs: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Runs the batched_inputs through the created model.\n        \"\"\"\n        out = self.model(batched_inputs)\n        return out\n\n    @classmethod\n    def from_config_dict(cls, cfg: DictConfig):\n        \"\"\"\n        Instantiate the Meta Architecture from gale config\n        \"\"\"\n        # fmt: off\n        input_shape = ShapeSpec(cfg.input.channels, cfg.input.height, cfg.input.width)\n        _logger.debug(f\"Inputs: {input_shape}\")\n        instance = super().from_config_dict(cfg.model.meta_architecture.init_args, input_shape=input_shape)\n        param_count = get_human_readable_count(sum([m.numel() for m in instance.parameters()]))\n        _logger.debug('{} created, param count: {}.'.format(cfg.model.meta_architecture.init_args.model_name, param_count))\n        # fmt: on\n        return instance\n\n    def build_param_dicts(self):\n        \"\"\"\n        Builds up the Paramters dicts for optimization.\n        \"\"\"\n        if self.filter_wd:\n            param_lists = add_weight_decay(\n                self.model,\n                weight_decay=self.wd,\n                skip_list=self.model.no_weight_decay(),\n            )\n            param_lists[0][\"lr\"] = self.lr\n            param_lists[1][\"lr\"] = self.lr\n        else:\n            ps = trainable_params(self.model)\n            param_lists = dict(params=ps, lr=self.lr, wd=self.wd)\n        return param_lists\n\n    @property\n    def hypers(self) -> Tuple:\n        \"\"\"\n        Returns list of parameters like `lr` and `wd`\n        for each param group\n        \"\"\"\n        lrs = []\n        wds = []\n\n        for p in self.build_param_dicts():\n            lrs.append(p[\"lr\"])\n            wds.append(p[\"weight_decay\"])\n        return self._hypers(lrs, wds)";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="VisionTransformer" class="doc_header"><code>class</code> <code>VisionTransformer</code><a href="https://github.com/benihime91/gale/tree/master/gale/classification/model/meta_arch/vision_transformer.py#L27" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>VisionTransformer</code>(<strong><code>model_name</code></strong>:<code>str</code>, <strong><code>input_shape</code></strong>:<code>ShapeSpec</code>, <strong><code>lr</code></strong>:<code>float</code>=<em><code>0.001</code></em>, <strong><code>wd</code></strong>:<code>float</code>=<em><code>1e-05</code></em>, <strong><code>pretrained</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>freeze_to</code></strong>:<code>Optional</code>[<code>int</code>]=<em><code>None</code></em>, <strong><code>finetune</code></strong>:<code>Optional</code>[<code>bool</code>]=<em><code>None</code></em>, <strong><code>act</code></strong>:<code>Optional</code>[<code>str</code>]=<em><code>None</code></em>, <strong><code>reset_classifier</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>filter_wd</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>num_classes</code></strong>=<em><code>1000</code></em>, <strong><code>drop_rate</code></strong>=<em><code>0.0</code></em>, <strong><code>attn_drop_rate</code></strong>=<em><code>0.0</code></em>, <strong><code>drop_path_rate</code></strong>=<em><code>0.0</code></em>, <strong>**<code>kwargs</code></strong>) :: <a href="/gale/core-classes.html#BasicModule"><code>BasicModule</code></a></p>
</blockquote>
<p>Abstract class offering interface which should be implemented by all <code>Backbones</code>,
<code>Heads</code> and <code>Meta Archs</code> in gale.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Arguments :</strong></p>
<ol>
<li><code>input_shape</code> (ShapeSpec): input image shape. For ViT <code>height=width</code> and check the above link for avilable model shapes.</li>
<li><code>model_name</code> (str): name of the ViT model, check the above link for avilable models.</li>
<li><code>pretrained</code> (bool): load weights pretrained on imagenet.</li>
<li><code>act</code> (str): name of the activation layer. Must be registerd in <a href="/gale/utils.structures.html#ACTIVATION_REGISTRY"><code>ACTIVATION_REGISTRY</code></a></li>
<li><code>num_classes</code> (int): num output classes.</li>
<li><code>drop_rate</code> (float): dropout rate.</li>
<li><code>attn_drop_rate</code> (float): attention dropout rate.</li>
<li><code>drop_path_rate</code> (float): stochastic depth rate.</li>
<li><code>reset_classifier</code> (bool): resets the weights of the classifier.</li>
<li><code>freeze_to</code> (int): Freeze the param meter groups of the model upto n.</li>
<li><code>finetune</code> (bool): Freeze all the layers and keep only the <code>classifier</code> trainable.</li>
</ol>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">inp</span> <span class="o">=</span> <span class="n">ShapeSpec</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">VisionTransformer</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;vit_small_patch16_224&quot;</span><span class="p">,</span>
    <span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">input_shape</span><span class="o">=</span><span class="n">inp</span><span class="p">,</span>
    <span class="n">finetune</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">reset_classifier</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Freezing all the model parameters except for the classifier
</pre>
</div>
</div>

<div class="output_area">




<div id="114f515e-6ca2-4e86-8372-d1199d4a3043"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#114f515e-6ca2-4e86-8372-d1199d4a3043');

            setTimeout(function() {
                var nbb_cell_id = 19;
                var nbb_unformatted_code = "inp = ShapeSpec(3, 224, 224)\n\nm = VisionTransformer(\n    model_name=\"vit_small_patch16_224\",\n    pretrained=False,\n    input_shape=inp,\n    finetune=True,\n    reset_classifier=True,\n    num_classes=10,\n)";
                var nbb_formatted_code = "inp = ShapeSpec(3, 224, 224)\n\nm = VisionTransformer(\n    model_name=\"vit_small_patch16_224\",\n    pretrained=False,\n    input_shape=inp,\n    finetune=True,\n    reset_classifier=True,\n    num_classes=10,\n)";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">channels</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">height</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">width</span><span class="p">)</span>
<span class="n">o</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="98d0dfb8-e748-42d0-b3f7-13041d183f14"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#98d0dfb8-e748-42d0-b3f7-13041d183f14');

            setTimeout(function() {
                var nbb_cell_id = 20;
                var nbb_unformatted_code = "i = torch.randn(2, inp.channels, inp.height, inp.width)\no = m(i)";
                var nbb_formatted_code = "i = torch.randn(2, inp.channels, inp.height, inp.width)\no = m(i)";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Similar to <a href="/gale/classification.model.meta_arch.common.html#GeneralizedImageClassifier"><code>GeneralizedImageClassifier</code></a> we can also instantiate <code>ViT</code> from a config. <code>ViT</code> does not require neither a <code>backbone</code> nor a <code>head</code> configuration. We just need the particular initialization arguments for the vit model defined in <code>model_name</code>.
{% include note.html content='You input shape must match the dimensions that the Vision Transformer model supports. Unlike <a href="/gale/classification.model.meta_arch.common.html#GeneralizedImageClassifier"><code>GeneralizedImageClassifier</code></a>, <code>ViT</code> is dependent on the shape.' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="DataClass">DataClass<a class="anchor-link" href="#DataClass"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">field</span>
<span class="kn">from</span> <span class="nn">omegaconf</span> <span class="kn">import</span> <span class="n">MISSING</span><span class="p">,</span> <span class="n">OmegaConf</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="f63e7158-b36b-463a-aea8-860cf8ca7de3"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#f63e7158-b36b-463a-aea8-860cf8ca7de3');

            setTimeout(function() {
                var nbb_cell_id = 21;
                var nbb_unformatted_code = "from dataclasses import dataclass, field\nfrom omegaconf import MISSING, OmegaConf";
                var nbb_formatted_code = "from dataclasses import dataclass, field\nfrom omegaconf import MISSING, OmegaConf";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="VisionTransformerDataClass" class="doc_header"><code>class</code> <code>VisionTransformerDataClass</code><a href="https://github.com/benihime91/gale/tree/master/gale/classification/model/meta_arch/vision_transformer.py#L158" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>VisionTransformerDataClass</code>(<strong><code>model_name</code></strong>:<code>str</code>=<em><code>'???'</code></em>, <strong><code>lr</code></strong>:<code>float</code>=<em><code>0.001</code></em>, <strong><code>wd</code></strong>:<code>float</code>=<em><code>1e-05</code></em>, <strong><code>pretrained</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>freeze_to</code></strong>:<code>Optional</code>[<code>int</code>]=<em><code>None</code></em>, <strong><code>finetune</code></strong>:<code>Optional</code>[<code>bool</code>]=<em><code>True</code></em>, <strong><code>reset_classifier</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>filter_wd</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>drop_rate</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>attn_drop_rate</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>drop_path_rate</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>num_classes</code></strong>:<code>int</code>=<em><code>'???'</code></em>)</p>
</blockquote>
<p>VisionTransformerDataClass(model_name:str='???', lr:float=0.001, wd:float=1e-05, pretrained:bool=False, freeze_to:Union[int, NoneType]=None, finetune:Union[bool, NoneType]=True, reset_classifier:bool=True, filter_wd:bool=True, drop_rate:float=0.0, attn_drop_rate:float=0.0, drop_path_rate:float=0.0, num_classes:int='???')</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">




<div id="210e6f49-7219-467b-99f4-28ea62fc4389"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#210e6f49-7219-467b-99f4-28ea62fc4389');

            setTimeout(function() {
                var nbb_cell_id = 22;
                var nbb_unformatted_code = "# export\n@dataclass\nclass VisionTransformerDataClass:\n    model_name: str = MISSING\n    lr: float = 1e-03\n    wd: float = 1e-05\n    pretrained: bool = False\n    freeze_to: Optional[int] = None\n    finetune: Optional[bool] = True\n    reset_classifier: bool = True\n    filter_wd: bool = True\n    drop_rate: float = 0.0\n    attn_drop_rate: float = 0.0\n    drop_path_rate: float = 0.0\n    num_classes: int = MISSING";
                var nbb_formatted_code = "# export\n@dataclass\nclass VisionTransformerDataClass:\n    model_name: str = MISSING\n    lr: float = 1e-03\n    wd: float = 1e-05\n    pretrained: bool = False\n    freeze_to: Optional[int] = None\n    finetune: Optional[bool] = True\n    reset_classifier: bool = True\n    filter_wd: bool = True\n    drop_rate: float = 0.0\n    attn_drop_rate: float = 0.0\n    drop_path_rate: float = 0.0\n    num_classes: int = MISSING";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here how a <a href="/gale/classification.model.meta_arch.vit.html#VisionTransformer"><code>VisionTransformer</code></a> can be instantiated via the config ...</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">inp</span> <span class="o">=</span> <span class="n">ShapeSpec</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>

<span class="n">meta_args</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">structured</span><span class="p">(</span>
    <span class="n">VisionTransformerDataClass</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;vit_small_patch16_224&quot;</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">meta</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">()</span>
<span class="n">meta</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;ViT&quot;</span>
<span class="n">meta</span><span class="o">.</span><span class="n">init_args</span> <span class="o">=</span> <span class="n">meta_args</span>

<span class="n">i</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">()</span>
<span class="n">i</span><span class="o">.</span><span class="n">channels</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">channels</span>
<span class="n">i</span><span class="o">.</span><span class="n">height</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">height</span>
<span class="n">i</span><span class="o">.</span><span class="n">width</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">width</span>

<span class="n">C</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">()</span>
<span class="n">C</span><span class="o">.</span><span class="n">input</span> <span class="o">=</span> <span class="n">i</span>
<span class="n">C</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">()</span>
<span class="n">C</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">meta_architecture</span> <span class="o">=</span> <span class="n">meta</span>

<span class="nb">print</span><span class="p">(</span><span class="n">OmegaConf</span><span class="o">.</span><span class="n">to_yaml</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">resolve</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>
<details class="description">
      <summary data-open="Hide Output" data-close="Show Output"></summary>
        <summary></summary>
        
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>input:
  channels: 3
  height: 224
  width: 224
model:
  meta_architecture:
    name: ViT
    init_args:
      model_name: vit_small_patch16_224
      lr: 0.001
      wd: 1.0e-05
      pretrained: false
      freeze_to: null
      finetune: true
      reset_classifier: true
      filter_wd: true
      drop_rate: 0.0
      attn_drop_rate: 0.0
      drop_path_rate: 0.0
      num_classes: 2

</pre>
</div>
</div>

<div class="output_area">




<div id="c178576a-4272-4152-b9cf-de1d91cb6197"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#c178576a-4272-4152-b9cf-de1d91cb6197');

            setTimeout(function() {
                var nbb_cell_id = 29;
                var nbb_unformatted_code = "# collapse-output\ninp = ShapeSpec(3, 224, 224)\n\nmeta_args = OmegaConf.structured(\n    VisionTransformerDataClass(model_name=\"vit_small_patch16_224\", num_classes=2)\n)\n\nmeta = OmegaConf.create()\nmeta.name = \"ViT\"\nmeta.init_args = meta_args\n\ni = OmegaConf.create()\ni.channels = inp.channels\ni.height = inp.height\ni.width = inp.width\n\nC = OmegaConf.create()\nC.input = i\nC.model = OmegaConf.create()\nC.model.meta_architecture = meta\n\nprint(OmegaConf.to_yaml(C, resolve=True))";
                var nbb_formatted_code = "# collapse-output\ninp = ShapeSpec(3, 224, 224)\n\nmeta_args = OmegaConf.structured(\n    VisionTransformerDataClass(model_name=\"vit_small_patch16_224\", num_classes=2)\n)\n\nmeta = OmegaConf.create()\nmeta.name = \"ViT\"\nmeta.init_args = meta_args\n\ni = OmegaConf.create()\ni.channels = inp.channels\ni.height = inp.height\ni.width = inp.width\n\nC = OmegaConf.create()\nC.input = i\nC.model = OmegaConf.create()\nC.model.meta_architecture = meta\n\nprint(OmegaConf.to_yaml(C, resolve=True))";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

    </details>
</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">VisionTransformer</span><span class="o">.</span><span class="n">from_config_dict</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
<span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">input_shape</span><span class="o">.</span><span class="n">channels</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">input_shape</span><span class="o">.</span><span class="n">height</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">input_shape</span><span class="o">.</span><span class="n">width</span><span class="p">)</span>
<span class="n">inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="o">*</span><span class="n">shape</span><span class="p">)</span>
<span class="n">o</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
<span class="n">o</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Freezing all the model parameters except for the classifier
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[-0.0844,  0.1789],
        [-0.0287, -0.1308]], grad_fn=&lt;AddmmBackward&gt;)</pre>
</div>

</div>

<div class="output_area">




<div id="d4b6e12f-4aec-41ab-afac-bda48e5d8a35"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#d4b6e12f-4aec-41ab-afac-bda48e5d8a35');

            setTimeout(function() {
                var nbb_cell_id = 30;
                var nbb_unformatted_code = "m = VisionTransformer.from_config_dict(C)\nshape = (m.input_shape.channels, m.input_shape.height, m.input_shape.width)\ninp = torch.randn(2, *shape)\no = m(inp)\no";
                var nbb_formatted_code = "m = VisionTransformer.from_config_dict(C)\nshape = (m.input_shape.channels, m.input_shape.height, m.input_shape.width)\ninp = torch.randn(2, *shape)\no = m(inp)\no";
                var nbb_cells = Jupyter.notebook.get_cells();
                for (var i = 0; i < nbb_cells.length; ++i) {
                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {
                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {
                             nbb_cells[i].set_text(nbb_formatted_code);
                        }
                        break;
                    }
                }
            }, 500);
            
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

